<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.5.3/css/bulma.min.css">
    <link rel="alternate" type="application/rss+xml" title="RSS" href="index.rss">
  </head>
  <body>
    <header class="section">
      <div class="container">
        <h1 class="title is-1">awesome-blogroll</h1>
        <p>An awesome list and aggregate feed of blogs from some of the best people, companies and communities in software.</p>
      </div>
    </header>
    <section class="section">
      <div class="container">
        <div class="feed-list tags">
          <span class="feed tag"><a href="https://8thlight.com/blog/feed/atom.xml">8th Light Blog</a></span>
          <span class="feed tag"><a href="https://feeds.feedburner.com/ponyfoo">Pony Foo</a></span>
          <span class="feed tag"><a href="https://joelonsoftware.com/feed">Joel On Software</a></span>
          <span class="feed tag"><a href="https://jvns.ca/atom.xml">Julia Evans</a></span>
          <span class="feed tag"><a href="http://www.evanmiller.org/news.xml">Evan Miller</a></span>
          <span class="feed tag"><a href="https://martinfowler.com/feed.atom">Martin Fowler</a></span>
          <span class="feed tag"><a href="http://multicians.org/rss.xml">Multicians - Changes</a></span>
          <span class="feed tag"><a href="https://rachelbythebay.com/w/atom.xml">rachelbythebay</a></span>
          <span class="feed tag"><a href="https://www.fourmilab.ch/fourmilog/atom_10.xml">Fourmilog - Fourmilab Change Log</a></span>
          <span class="feed tag"><a href="https://feeds.feedburner.com/TroyHunt">Troy Hunt</a></span>
          <span class="feed tag"><a href="http://loup-vaillant.fr/updates">Loup Vaillant</a></span>
          <span class="feed tag"><a href="https://codewithoutrules.com/atom.xml">Code Without Rules</a></span>
          <span class="feed tag"><a href="https://medium.com/feed/@copyconstruct">Cindy Sridharan</a></span>
          <span class="feed tag"><a href="https://www.sandimetz.com/blog?format=RSS">Sandi Metz</a></span>
          <span class="feed tag"><a href="https://rakyll.org/index.xml">Go, the unwritten parts</a></span>
          <span class="feed tag"><a href="http://250bpm.com/feed/pages/pagename/start/category/blog/t/250bpm-blogs/h/http%3A%2F%2Fwww.250bpm.com%2Fblog">250bpm</a></span>
          <span class="feed tag"><a href="https://www.scottaaronson.com/blog/?feed=rss2">Shtetl-Optimized</a></span>
          <span class="feed tag"><a href="http://apenwarr.ca/log/rss.php">apenwarr</a></span>
          <span class="feed tag"><a href="https://www.hillelwayne.com/post/index.xml">Hillel Wayne</a></span>
          <span class="feed tag"><a href="https://graydon2.dreamwidth.org/data/atom">frog hop</a></span>
          <span class="feed tag"><a href="http://feeds.feedburner.com/csswizardry">CSS Wizardry</a></span>
          <span class="feed tag"><a href="http://stratechery.com/feed/">Stratechery</a></span>
          <span class="feed tag"><a href="http://ronjeffries.com/feed.xml">Ron Jeffries</a></span>
          <span class="feed tag"><a href="https://danluu.com/atom.xml">Dan Luu</a></span>
          <span class="feed tag"><a href="https://jacquesmattheij.com/rss.xml">Jacques Mattheij</a></span>
          <span class="feed tag"><a href="http://paperswelove.org/feed.xml">Papers We Love</a></span>
          <span class="feed tag"><a href="https://moxie.org/blog/rss.xml">Moxie Marlinspike</a></span>
          <span class="feed tag"><a href="https://sdegutis.com/blog/atom.xml">Steven Degutis</a></span>
          <span class="feed tag"><a href="https://brandur.org/articles.atom">Brandur Leach</a></span>
          <span class="feed tag"><a href="http://blog.mecheye.net/feed/">Clean Rinse</a></span>
          <span class="feed tag"><a href="https://www.nngroup.com/feed/rss/">Nielsen Norman Group</a></span>
          <span class="feed tag"><a href="http://ourbit.norbertoherz.com/feed.xml">OurBit</a></span>
          <span class="feed tag"><a href="http://belkadan.com/blog/atom">-dealloc</a></span>
          <span class="feed tag"><a href="http://www.fewbutripe.com/feed.xml">Few, but ripe...</a></span>
          <span class="feed tag"><a href="http://beza1e1.tuxen.de/blog_en.atom">Andreas Zwinkau</a></span>
          <span class="feed tag"><a href="http://feeds.feedburner.com/brad-frosts-blog">Brad Frost</a></span>
          <span class="feed tag"><a href="https://prestonbyrne.com/feed/">Preston Byrne</a></span>
          <span class="feed tag"><a href="http://dustycloud.org/blog/index.xml">DustyCloud</a></span>
          <span class="feed tag"><a href="http://blog.ncase.me/rss/">Nicky Case</a></span>
          <span class="feed tag"><a href="https://technology.condenast.com/feed/rss">Condé Nast Technology</a></span>
          <span class="feed tag"><a href="https://developers.soundcloud.com/blog.rss">Soundcloud Developers Backstage Blog</a></span>
          <span class="feed tag"><a href="https://eklitzke.org/atom.xml?type=blog">Evan Klitzke</a></span>
        </div>
      </div>
    </section>
    <a href="#" class="return-to-top" style="position: fixed; bottom: 1rem; right: 1rem; width: 2rem; height: 2rem; border-radius: 100%; background: #ddd; opacity: 0.3; text-align: center; text-decoration: none; line-height: 2rem;">&uarr;</a>
    <main class="feed-items">
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://papers-we-love.github.io/2017/news/october-meetups/">October Meetups</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="http://paperswelove.org/feed.xml">Papers We Love</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">compsci</span>
              <span class="tag">community</span>
            </div>
            <span class="subtitle is-7">Sun Oct 01 2017 12:00:32 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>We have another great line-up of meet-ups scheduled for October across a number of our chapters:</p>

<p><strong>Vienna 10/2</strong>: <a href="https://www.meetup.com/Papers-We-Love-Vienna/events/242952984/">October: Bitcoin</a></p>

<p><strong>Chattanooga 10/3</strong>: <a href="https://www.meetup.com/Papers-We-Love-Chattanooga/events/243650192/">Jacob Kobernik on Blockchain: A Graph Primer</a></p>

<p><strong>Göteborg 10/5</strong>: <a href="https://www.meetup.com/Papers-We-Love-Gothenburg/events/243458363/">Dan Rosén on &quot;Algebraic Effects&quot; at TimeEdit</a></p>

<p><strong>Seattle 10/5</strong>: <a href="https://www.meetup.com/Papers-We-Love-Seattle/events/242431931/">PWL #36: Deep Speech</a></p>

<p><strong>San Diego 10/5</strong>: <a href="https://www.meetup.com/Papers-We-Love-San-Diego/events/241581472/">HyperLogLog in Practice: Algorithmic Engineering of a State of The Art…</a></p>

<p><strong>New York  10/9</strong>: <a href="https://www.meetup.com/papers-we-love/events/242066112/">Jessie Frazelle on SCONE: Secure Linux Containers with Intel SGX &amp; a PWLMini</a></p>

<p><strong>Columbus 10/11</strong>: <a href="https://www.meetup.com/Papers-We-Love-Columbus/events/243568135/">Papers We Love Book Club: The Dawn of Software Engineering</a></p>

<p><strong>Toronto 10/11</strong>: <a href="https://www.meetup.com/Papers-We-Love-Toronto/events/243723651/">#23 Yawar Amin on Lightweight Static Capabilities</a></p>

<p><strong>London 10/19</strong>: <a href="https://www.meetup.com/Papers-We-Love-London/events/243775403/">Simon Peyton Jones: &quot;Getting from A to B: fast route-finding on slow computers&quot;</a></p>

<p><strong>Columbus 10/25</strong>: <a href="https://www.meetup.com/Papers-We-Love-Columbus/events/tcnwxmywnbhc/">Papers We Love Book Club: The Dawn of Software Engineering</a></p>

<p><strong>Fairfax 10/25</strong>: <a href="https://www.meetup.com/Papers-We-Love-DC-NoVA/events/241227602/">Recursive Functions of Symbolic Expressions and Their Computation by Machine</a></p>

<p><strong>Denver 10/26</strong>: <a href="https://www.meetup.com/Papers-We-Love-Denver/events/fgbdrmywnbjc/">TBD, but probably an awesome paper :-)</a></p>

<p><strong>Denver 10/26</strong>: <a href="https://www.meetup.com/Papers-We-Love-Denver/events/239936931/">Harry Brumleve -&gt; A Universal Modular Actor Formalism for AI</a>
 </p>

<hr />

<p>The <strong>New York Chapter</strong> would like to give special thanks to our Platinum sponsor <a href="https://www.twosigma.com">TwoSigma</a>. Join us in St. Louis for <a href="http://pwlconf.org/">PWLConf 2017</a>!</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://papers-we-love.github.io/2017/news/september-meetups/">September Meetups</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="http://paperswelove.org/feed.xml">Papers We Love</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">compsci</span>
              <span class="tag">community</span>
            </div>
            <span class="subtitle is-7">Sun Oct 01 2017 00:00:25 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>We have another great line-up of meet-ups scheduled for September across a number of our chapters:</p>

<p><strong>Chattanooga 9/5</strong>: <a href="https://www.meetup.com/Papers-We-Love-Chattanooga/events/242993742/">Noel Weichbrodt on A History of Erlang</a></p>

<p><strong>Seattle 9/7</strong>: <a href="https://www.meetup.com/Papers-We-Love-Seattle/events/240921627/">PWL #35: Orleans</a></p>

<p><strong>Santa Monica 9/7</strong>: <a href="https://www.meetup.com/Papers-We-Love-LA/events/241748877/">Daniel Brice - Infinite sets that admit fast exhaustive search</a></p>

<p><strong>Bangalore 9/9</strong>: <a href="https://www.meetup.com/Papers-we-love-Bangalore/events/242590993/">An incremental approach to compiler construction</a></p>

<p><strong>Columbus 9/13</strong>: <a href="https://www.meetup.com/Papers-We-Love-Columbus/events/243105613/">Papers We Love Book Club: The Dawn of Software Engineering</a></p>

<p><strong>Zürich 9/14</strong>: <a href="https://www.meetup.com/Papers-we-love-Zurich/events/242544627/">Natallie Baikevich on Automatic Construction of Inlining Heuristics using ML</a></p>

<p><strong>Olivette 9/18</strong>: <a href="https://www.meetup.com/Papers-We-Love-in-saint-louis/events/242280231/">Why calculating is better than scheming</a></p>

<p><strong>Budapest 9/21</strong>: <a href="https://www.meetup.com/Papers-We-Love-Budapest/events/242791943/">Typed Self-Evaluation via Intensional Type Functions, Brown&amp;Palsberg -Érdi Gergő</a></p>

<p><strong>Montréal 9/21</strong>: <a href="https://www.meetup.com/Papers-We-Love-Montreal/events/243307433/">Simple Fast Algorithms for the Editing Distance between Trees &amp; Related Problems</a></p>

<p><strong>San Francisco 9/21</strong>: <a href="https://www.meetup.com/papers-we-love-too/events/241365481/">Dave Cohen on Hashgraph Consensus: Fair, Fast, Byzantine Fault Tolerance</a></p>

<p><strong>Arlington 9/27</strong>: <a href="https://www.meetup.com/Papers-We-Love-DC-NoVA/events/241228410/">Bitcoin: A Peer to Peer Cash System</a></p>

<p><strong>Columbus 9/28</strong>: <a href="https://www.meetup.com/Papers-We-Love-Columbus/events/243553132/">Papers We Love Book Club: The Dawn of Software Engineering</a>
 </p>

<hr />

<p>The <strong>New York Chapter</strong> would like to give special thanks to our Platinum sponsor <a href="https://www.twosigma.com">TwoSigma</a>. Join us in St. Louis for <a href="http://pwlconf.org/">PWLConf 2017</a>!</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://www.fourmilab.ch/fourmilog/archives/2017-09/001714.html">Floating Point Benchmark: Prolog Language Added</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://www.fourmilab.ch/fourmilog/atom_10.xml">Fourmilog - Fourmilab Change Log</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Fri Sep 29 2017 22:03:59 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            I have posted an update to my trigonometry-intense <a href="/fbench/" target="Fourmilog_Aux">floating point benchmark</a> which adds
<a href="https://en.wikipedia.org/wiki/Prolog" target="Fourmilog_Aux">Prolog</a> to the list of languages in which the benchmark is implemented.  A new release of the
<a href="/fbench/" target="Fourmilog_Aux">benchmark collection</a>
including Prolog is now available for downloading.

<p>

Prolog is a language designed for logic programming.  Working in
conjunction with a database of facts and rules, one can make
queries which are answered by applying the rules of formal
logic. Thus, in Prolog, one often describes the answer one seeks
and lets the language implementation find it rather than
prescribing the steps through which the answer is obtained as
one would in a conventional imperative programming language. 
Prolog is used in artificial intelligence and computational
linguistics research, and is well-suited to the analysis of
unstructured natural language.  Components of IBM's <a href="https://en.wikipedia.org/wiki/Watson_%28computer%29" target="Fourmilog_Aux">Watson</a>
question answering system are written in Prolog.  The first
Prolog system was developed in 1972, and the language was
standardised as ISO/IEC 13211-1 in 1995, with subsequent
corrigenda in 2007 and 2012.

</p><p>

Prolog is intended for problems which are nothing like that
performed by the floating point benchmark, which is a typical
scientific computing task involving floating point numbers and
trigonometric functions.  However, Prolog supports floating
point numbers and trigonometric functions, and as a
<a href="https://en.wikipedia.org/wiki/Turing_completeness" target="Fourmilog_Aux">Turing-complete language</a> is able to perform any computational
task which can be programmed in any other such language.  So,
this can be considered as a case of misusing Prolog for a task
for which it wasn't remotely intended, with the goal of seeing
how well it expresses the algorithms and performs.

</p><p>

The resulting program uses very little of Prolog's sophisticated
pattern-matching and inference engine: the task simply doesn't
require these facilities.  Instead, Prolog is used as a
<a href="https://en.wikipedia.org/wiki/Functional_programming" target="Fourmilog_Aux">functional programming</a> language, employing its variables to pass
arguments to and return values from functions coded as Prolog
rules.  The result looks somewhat odd to those accustomed to
other programming languages, but one you get used to the syntax,
the code is expressive and comprehensible.  Pattern matching
allows replacing all of the conditionals for the four cases of
the transit_surface operation (marginal or paraxial ray, flat or
curved surface) with four different rules selected by the
arguments passed to them, as done in functional languages such
as Haskell and Erlang.

</p><p>

I originally developed this benchmark on <a href=" http://gprolog.org/" target="Fourmilog_Aux">GNU Prolog</a>
version 1.4.4, but when I went to run the benchmark for an
archival run time of around five minutes, I ran into the problem
that GNU Prolog does not fully implement <a href="https://en.wikipedia.org/wiki/Tail_call" target="Fourmilog_Aux">tail call</a>
optimisation.  What does this mean?  Prolog, like many
functional languages, does not provide control structures for
iteration.  Instead, iteration is accomplished by recursion. 
For example, here's how you might write the factorial function
in C using iteration:

<pre>
    int factorial(int n) {
        int result = 1;
        int i;

        for (i = 1; i &lt;= n; i++) {
            result *= i;
        }
        return result;
    }
</pre>

But Prolog has no equivalent of C's <b>for</b> statement, so you define
the factorial function recursively as it's usually expressed in
mathematics:

<pre>
    fact(N, NF) :-
            fact(1, N, 1, NF).

    fact(X, X, F, F) :- !.

    fact(X, N, FX, F) :-
            X1 is X + 1,
            FX1 is FX * X1,
            fact(X1, N, FX1, F).
</pre>

Now, this looks a bit odd until you become accustomed to the
rather eccentric syntax of Prolog, but the key thing to take
away is that evaluation is accomplished by the four argument
definition of fact().  When the final definition of fact() calls
itself, it is the last item executed, and tail call optimisation
takes note of this and, instead of recursively calling itself,
uses the same stack frame and transforms the recursion into an
iteration.  This allows recursion to an arbitrary depth without
consuming large amounts of stack memory.

</p><p>

Tail call optimisation is common among languages such as Lisp,
Haskell, and Prolog, but GNU Prolog does not implement it, or at
least doesn't do so in a sufficiently general manner as to
permit running benchmarks with a large number of iterations.

</p><p>

As a result, I moved the project from GNU Prolog to <a href="http://www.swi-prolog.org/" target="Fourmilog_Aux">SWI-Prolog</a>:
a mature Prolog system which properly supports tail call
optimisation.  I used Linux version 7.6.0-rc2, which is a stable
and complete implementation of Prolog, using the 64-bit Ubuntu
installation package.

</p><p>

Development of the program was straightforward, with the only
speed bumps my coming to terms with the Prolog way of doing
things.  This program constitutes an “abuse of language” almost
as extreme as the COBOL version.  Prolog is intended for logic
programming where its underlying inference engine does most of
the work in resolving queries where the programmer specifies a
way to find the answer but not the details of how it is to be
evaluated.  Optical design ray tracing couldn't be more
different—the computations must be evaluated procedurally, so I
ended up using Prolog as a functional programming langauge,
writing procedural code as ∧ expressions within rules, while
taking advantage of Prolog's polymorphism and pattern matching
to make the code more expressive of the problem being solved. 
Since Prolog provides full support for floating point arithmetic
and trigonometric functions, there were no problems in
evaluating the expressions used in the benchmark.

</p><p>

After testing the benchmark in SWI-Prolog for accuracy, I ran
the Prolog benchmark for 13,725,893 iterations and obtained the
following run times in seconds for five runs: 
    (287.14,
    286.64,
    288.11,
    288.15,
    286.38)
These runs give a mean time of 287.284 seconds, or 20.9301
microseconds per iteration.

</p><p>

I then ran the C benchmark for 166,051,660 iterations, yielding
run times of:
    (296.89,
    296.37,
    296.29,
    296.76,
    296.37)
seconds, with mean 296.536, for 1.7858 microseconds per
iteration.

</p><p>

Dividing these gives a SWI-Prolog run time of 11.7203 longer
than that of C.  In other words, for this benchmark, SWI-Prolog
runs around 11.72 times slower than C.

</p><p>

I next wanted to compare GNU Prolog with C.  Because of the lack
of tail call optimisation, I was unable to run the benchmark for
the required number of iterations to obtain an “on the record”
run of about five minutes (even when I tried tricks of nesting
iterations in calls), so I estimated its performance as follows.

</p><p>

I ran the benchmark on GNU Prolog with 450000 iterations, which
was the maximum I could use after setting “export
GLOBALSZ=80000000000” to create an enormous global stack.  I
received the following timings in seconds:
    (4.37,
    4.36,
    4.41,
    4.33,
    4.32)
of which the mean is 4.358 seconds.

</p><p>

Then, I ran the same benchmark under SWI-Prolog and measured:
    (8.90,
    8.80,
    8.79,
    8.99,
    8.96)
for a mean of 8.888.  This gives a run time ratio of GNU Prolog
to SWI-Prolog of 0.49032, and applying this to the measured
ratio of SWI-Prolog to C, we can infer a ratio of GNU Prolog to
C of 5.7467.

</p><p>

I do not report this as a primary benchmark for GNU Prolog
because its lack of tail call optimisation prevented it from
fulfilling the conditions of the benchmark: a run of around five
minutes.  It is, however, indicative of the performance of
Prolog which can be obtained by compiling to native code, and is
included because it demonstrates that Prolog can perform within
the range of other compiled languages.

</p><p>

In summary, Prolog did pretty well on this job for which it
wasn't designed.  The program is straightforward and readable,
and the performance in SWI-Prolog is comparable to other
languages which compile to byte code, as does this Prolog
implementation.  GNU Prolog, which compiles to native machine
code, performed better than GNU Common Lisp in compiled mode,
but toward the slow end of machine code compilers (but, since
the full benchmark could not be run, the GNU Prolog results are
not archival).

</p><p>

This directory includes a <b>Makefile</b> which can build the benchmark
using either SWI-Prolog of GNU Prolog (which, of course, must be
installed on your machine).  The SWI-Prolog version of the
benchmark uses that system's nonstandard three argument format/3
predicate to print its results to Prolog atoms, allowing the
program to perform its own accuracy test at the completion of
the benchmark.  GNU Prolog and the ISO standard do not implement
this extension, so alternative code is used which simply prints
the output of the last iteration of the benchmark to standard
output where it is compared with the expected results with <b>diff</b>.

</p><p>

The relative performance of the various language implementations (with C taken as 1) is as follows.   All language implementations of the benchmark listed below produced identical results to the last (11th) decimal place.

</p><p>


<table>
                                                                                                 
<tr>
    <th>Language</th>
    <th>Relative<br /> Time</th>
    <th>Details</th>
</tr>
 
<tr>
    <th>C</th>
    <td>1</td>
    <td>GCC 3.2.3 -O3, Linux</td>
</tr>
 
<tr>
    <th>Visual Basic .NET</th>
    <td>0.866</td>
    <td>All optimisations, Windows XP</td>
</tr>
 
<tr>
    <th>FORTRAN</th>
    <td>1.008</td>
    <td>GNU Fortran (g77) 3.2.3 -O3, Linux</td>
</tr>
 
<tr>
    <th>Pascal</th>
    <td>1.027<br />
                   1.077</td>
    <td>Free Pascal 2.2.0 -O3, Linux<br />
    	    	  GNU Pascal 2.1 (GCC 2.95.2) -O3, Linux</td>
</tr>
 
<tr>
    <th>Swift</th>
    <td>1.054</td>
    <td>Swift 3.0.1, -O, Linux</td>
</tr>

<tr>
    <th>Rust</th>
    <td>1.077</td>
    <td>Rust 0.13.0, --release, Linux</td>
</tr>

<tr>
    <th>Java</th>
    <td>1.121</td>
    <td>Sun JDK 1.5.0_04-b05, Linux</td>
</tr>

<tr>
    <th>Visual Basic 6</th>
    <td>1.132</td>
    <td>All optimisations, Windows XP</td>
</tr>
  
<tr>
    <th>Haskell</th>
    <td>1.223</td>
    <td>GHC 7.4.1-O2 -funbox-strict-fields, Linux</td>
</tr>
  
<tr>
    <th>Scala</th>
    <td>1.263</td>
    <td>Scala 2.12.3, OpenJDK 9, Linux</td>
</tr>

<tr>
    <th>Ada</th>
    <td>1.401</td>
    <td>GNAT/GCC 3.4.4 -O3, Linux</td>
</tr>

<tr>
    <th>Go</th>
    <td>1.481</td>
    <td>Go version go1.1.1 linux/amd64, Linux</td>
</tr>

 <tr>
    <th>Simula</th>
    <td>2.099</td>
    <td>GNU Cim 5.1, GCC 4.8.1 -O2, Linux</td>
</tr>

<tr>
    <th>Lua</th>
    <td>2.515<br />
        22.7</td>
    <td>LuaJIT 2.0.3, Linux<br />
        Lua 5.2.3, Linux</td>
</tr>

<tr>
    <th>Python</th>
    <td>2.633<br /> 30.0</td>
    <td>PyPy 2.2.1 (Python 2.7.3), Linux<br />
        Python 2.7.6, Linux
</td>
</tr>

 <tr>
    <th>Erlang</th>
    <td>3.663<br />
    	    	   9.335</td>
    <td>Erlang/OTP 17, emulator 6.0, HiPE [native, {hipe, [o3]}]<br />
    	         Byte code (BEAM), Linux</td>
</tr>
 <tr>
    <th>ALGOL 60</th>
    <td>3.951</td>
    <td>MARST 2.7, GCC 4.8.1 -O3, Linux</td>
</tr>
  
<tr>
    <th>PL/I</th>
    <td>5.667</td>
    <td>Iron Spring PL/I 0.9.9b beta, Linux</td>
</tr>

<tr>
    <th>Lisp</th>
    <td>7.41 <br />
                   19.8</td>
    <td>GNU Common Lisp 2.6.7, Compiled, Linux<br />
                  GNU Common Lisp 2.6.7, Interpreted</td>
</tr>

 <tr>
    <th>Smalltalk</th>
    <td>7.59</td>
    <td>GNU Smalltalk 2.3.5, Linux</td>
</tr>
   
<tr>
    <th>Forth</th>
    <td>9.92</td>
    <td>Gforth 0.7.0, Linux</td>
</tr>
  
<tr>
    <th>Prolog</th>
    <td>11.72<br />
				   5.747</td>
    <td>SWI-Prolog 7.6.0-rc2, Linux<br />
				  GNU Prolog 1.4.4, Linux, (limited iterations)</td>
</tr>

<tr>
    <th>COBOL</th>
    <td>12.5<br />
				   46.3</td>
    <td>Micro Focus Visual COBOL 2010, Windows 7<br />
				  Fixed decimal instead of computational-2</td>
</tr>

 <tr>
    <th>Algol 68</th>
    <td>15.2</td>
    <td>Algol 68 Genie 2.4.1 -O3, Linux</td>
</tr>

<tr>
    <th>Perl</th>
    <td>23.6</td>
    <td>Perl v5.8.0, Linux</td>
</tr>

<tr>
    <th>Ruby</th>
    <td>26.1</td>
    <td>Ruby 1.8.3, Linux</td>
</tr>

<tr>
    <th>JavaScript</th>
    <td>27.6 <br />
                   39.1 <br />
                   46.9</td>
    <td>Opera 8.0, Linux<br />
                  Internet Explorer 6.0.2900, Windows XP<br />
                  Mozilla Firefox 1.0.6, Linux</td>
</tr>
 
<tr>
    <th>QBasic</th>
    <td>148.3</td>
    <td>MS-DOS QBasic 1.1, Windows XP Console</td>
</tr>

<tr>
    <th>Mathematica</th>
    <td>391.6</td>
    <td>Mathematica 10.3.1.0, Raspberry Pi 3, Raspbian</td>
</tr>

</table>
</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://feedproxy.google.com/~r/TroyHunt/~3/UJupDnFAk5c/">Weekly update 54</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://feeds.feedburner.com/TroyHunt">Troy Hunt</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">security</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Fri Sep 29 2017 07:59:44 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p><a href="http://www.symantec.com/ready-secure-go/it-smb/?om_ext_cid=ws_ad_TROY_DBC-Bulldog_IT-SMB_Encrypt"><strong>Presently sponsored by:</strong> Get a security solution that will keep your website up and running—and keep you sleeping soundly: Symantec Website Security. Learn how</a></p><div><p>Ah, home! It's nice at home, I think I'll stay here. When I got back from Utah on Sunday I checked my TripIt and noticed I'd been away bang on 40% of the year but fortunately, that's it for the 2017 overseas stuff. That said, I've got a bunch of events lined up in Aus for the rest of the year and I'll talk more about those soon.</p>
<p>This week, I've actually had some time to catch up on writing and pumped out a couple of blog posts that have been on my mind for some time. It's stuff I'm passionate about (both for different reasons) and I really hope people find it interesting if not even thought-provoking. Enjoy!</p>
<p><a href="https://itunes.apple.com/au/podcast/troy-hunts-weekly-update-podcast/id1176454699">iTunes podcast</a> | <a href="https://goo.gl/app/playmusic?ibi=com.google.PlayMusic&amp;isi=691797987&amp;ius=googleplaymusic&amp;link=https://play.google.com/music/m/If3tw7npymckucxq4q76762ncny?t%3DTroy_Hunt%27s_Weekly_Update_Podcast">Google Play Music podcast</a> | <a href="http://www.omnycontent.com/d/playlist/1439345f-6152-486d-a9c2-a6bf0067f2b7/3ba9af7f-3bfb-48fd-aae7-a6bf00689c10/fde26e49-9fb8-457d-8f16-a6bf00696676/podcast.rss">RSS podcast</a></p>

References
<ol>
<li><a href="https://www.troyhunt.com/the-ethics-of-running-a-data-breach-search-service/">I give a <em>huge</em> amount of thought to the ethics of running a data breach service</a> (there are good reasons for every aspect of how HIBP runs - this post explains some of the big ones)</li>
<li><a href="https://www.troyhunt.com/how-important-are-qualifications-to-modern-technology-jobs/">Are qualifications in tech jobs actually that important?</a> (biggest thing here is how consistently people said <em>experience</em> is essential, regardless of how you got to this industry)</li>
<li><a href="http://www.symantec.com/ready-secure-go/it-smb/?om_ext_cid=ws_ad_TROY_DBC-Bulldog_IT-SMB_Encrypt">Symantec Website Security is sponsoring my blog this week</a> (thanks guys, big shout-out to them!)</li>
</ol>
</div>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://www.nngroup.com/training/los-angeles/">UX Conference Los Angeles Announced (Feb 25 - Mar 2)</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://www.nngroup.com/feed/rss/">Nielsen Norman Group</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">ux</span>
              <span class="tag">company</span>
            </div>
            <span class="subtitle is-7">Fri Sep 29 2017 04:23:00 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>In-depth, full-day courses, teaching user experience best practices for successful design. Conference focus on long-lasting skills for UX professionals. February 25 - March 2, 2018.</p><br /><br /><a href="/training/los-angeles/">See Full Schedule and Pricing</a>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://www.scottaaronson.com/blog/?p=3468">Michael Cohen (1992-2017)</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://www.scottaaronson.com/blog/?feed=rss2">Shtetl-Optimized</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">quantum</span>
              <span class="tag">compsci</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Fri Sep 29 2017 00:55:07 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p></p>
<p>I first encountered Michael Cohen when, as a freshman newly arrived at MIT, he walked into my office unannounced to ask if I had any open problems for him to solve.  My first reaction was bemused annoyance: <em>who does this punk think he is?  he’s ready to do theory, but doesn’t</em><em> even know enough to make an appointment first?  also, why doesn’t he shave?</em></p>
<p>And then, five minutes later, I was practically <em>begging</em> him to do research with me.  “OK, so you didn’t like that problem?  Wait, I’ve got another one!  Hey, where are you going…”</p>
<p>Within those five minutes, it had become obvious that this was a freshman who I could—must—talk to like an advanced grad student or professor.  Sadly for quantum computing, Michael ultimately decided to go into classical parts of theoretical computer science, such as low-rank approximation and fast algorithms for geometry and linear-algebra problems.  But that didn’t stop him from later taking my graduate course on quantum complexity theory, where he sat in the front and loudly interrupted me every minute, stream-of-consciousness style, so that my “lectures” often turned into dialogues with him.  Totally unforgivable—all the more so because his musings were <em>always</em> on point, constantly catching me in errors or unjustified claims (one of which I <a href="https://www.scottaaronson.com/blog/?p=2072">blogged about previously</a>).</p>
<p>Not once did I ever suspect he did it to show off: he was simply so overtaken by his urge to understand the point at hand, as to be oblivious to all niceties.  Yet somehow, that social obliviousness didn’t stop him from accumulating a huge circle of friends.  (Well, it <em>was</em> MIT.)</p>
<p>Michael stayed on at MIT as a grad student, racking up an <a href="http://dblp.uni-trier.de/pers/hd/c/Cohen:Michael_B=">incredible publication list</a> by age 25.  This semester, he went to visit the Simons Institute for Theory of Computing in Berkeley.</p>
<p>Three days ago, Michael was found dead in his apartment in Berkeley, after having cancelled a scheduled talk because he was feeling unwell.  No cause has been given.</p>
<p>The horrible news came just as I was arriving in Germany for the <a href="http://www.heidelberg-laureate-forum.org/">Heidelberg Laureate Forum</a>, to speak about quantum supremacy.  So I barely had time to process the tragedy—yet it was always in the background, especially as I learned that in his brief life, Michael had also touched many of the other computer scientists who I spoke with in Heidelberg, such as Dan Spielman, whose approach to Ramanujan graphs (with Marcus and Srivastava) Michael had <a href="https://arxiv.org/abs/1604.03544">made constructive</a> in one of his most celebrated works.  Only now is the full weight of what happened bearing down on me.</p>
<p>I understand that memorial events are being planned at both MIT and Berkeley.  Feel free to share memories of Michael in the comments; see also <a href="https://lucatrevisan.wordpress.com/2017/09/26/3915/">Luca’s post</a> and <a href="http://blog.computationalcomplexity.org/2017/09/tragic-losses.html">Lance’s post</a>.</p>
<p>This is an unfathomable loss for Michael’s family, for his many friends and colleagues, and for a field that’s been robbed of decades of breakthroughs.</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://brandur.org/webhooks">Should You Build a Webhooks API?</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://brandur.org/articles.atom">Brandur Leach</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">practices</span>
              <span class="tag">tools</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Thu Sep 28 2017 16:28:56 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>The term “webhook” was coined back in 2007 by Jeff Lindsay
as a “hook” (or callback) for the web; meant to be a
general purpose system to allow Internet systems to be
composed in the same spirit as the Unix pipe. By speaking
HTTP and being symmetrical to common HTTP APIs, they were
an elegant answer to a problem without many options at the
time – <a href="https://en.wikipedia.org/wiki/WebSocket">WebSockets</a> wouldn’t be standardized
until 2011 and would only see practical use much later.
Most other contemporary streaming options were still only a
distant speck on the horizon.</p>

<p>For a few very common APIs like GitHub, Slack, or Stripe,
the push stream available over webhooks might be one of the
best-known features. They’re reliable, can be used to
configure multiple receivers that receive customized sets
of events, and they even work for accounts connected via
OAuth, allowing platforms built on the APIs to tie into the
activity of their users. They’re a great feature and
aren’t going anywhere anytime soon, but they’re also far
from perfect. In the spirit of avoiding <a href="/accidental-evangelist">accidental
evangelism</a>, here we’ll talk about
whether they’re a good pattern for new API providers to
emulate.</p>

<a href="#case">A basic case for webhooks</a>

<p>First, let’s take a look at why webhooks are useful. While
REST APIs commonly make up the backbone for accessing and
manipulating information in a web platform, webhooks are
often used as a second facet that augments it by streaming
real-time updates.</p>

<p>Say you’re going to write a mini-CI service that will build
any branches that are opened via pull request on one of
your GitHub repositories. Like Travis, we want it to be
able to detect a pull request, and then assign it a status
check icon that will only be resolved when the build
completes.</p>


  <p><a href="/assets/webhooks/github-status-check@2x.png"></a></p>
  Travis putting status checks on a pull request that are contingent on a successful build.


<p>GitHub has a <a href="https://developer.github.com/v3/repos/statuses/">status API</a> that can assign or
update statuses associated with a given commit SHA. With
just a REST API, we’d have to poll the list endpoint ever
few seconds to know when new pull requests come in. Luckily
though, there’s much better way: we can listen on for
GitHub’s <code>pull_request</code> webhook, and it’ll notify us when
anything changes.</p>

<p>Our CI service listens for <code>pull_request</code> webhooks, creates
a new status via the REST API when it sees one, and then
updates that status when its corresponding build succeeds
or fails. It’s able to add status checks in a timely manner
(ideally users see a <code>pending</code> status the moment they
open a new pull), and with no inefficient polling involved.</p>


  <p><a href="/assets/webhooks/ci.svg"></a></p>
  A basic webhooks flow to build a simple CI system for GitHub.


<a href="#user-ergonomics">The virtues of user ergonomics</a>

<p>As we see above webhooks are convenient and work pretty
well, but that said, they’re far from perfect in a number
of places. Let’s look at a few ways that using them can be
a little painful.</p>

<h3><a href="#endpoints">Endpoint provisioning and management</a></h3>

<p>Getting an HTTP endpoint provisioned to receive a webhook
isn’t technically difficult, but it can be bureaucratically
so.</p>

<p>The classic example is the large enterprise where getting a
new endpoint exposed to the outside world might be a
considerable project involving negotiations with
infrastructure and security teams, requisitioning new
hardware, and piles of paperwork. In the worst cases,
webhooks might be wholly incompatible with an
organization’s security model where user data is
uncompromisingly kept within a secured perimeter at all
times.</p>


  <p><a href="/assets/webhooks/provisioning-woes.svg"></a></p>
  Difficulty in provisioning an HTTP endpoint that can talk to the outside world.


<p>Development and testing are also difficult cases. There’s
no perfectly fluid way of getting an endpoint from a
locally running environment exposed for a webhook provider
to access. Programs like <a href="https://ngrok.com/">Ngrok</a> are good options,
but still add a step and complication that wouldn’t be
necessary with an alternate scheme.</p>

<h3><a href="#security">Uncertain security</a></h3>

<p>Because webhook endpoints are publicly accessible HTTP
APIs, it’s up to providers to build in a security scheme
to ensure that an attacker can’t issue malicious requests
containing forged payloads. There’s a variety of commonly
seen techniques:</p>

<ol>
<li><strong><em>Webhook signing:</em></strong> Sign webhook payloads and send the
signature via HTTP header so that users can verify it.</li>
<li><strong><em>HTTP authentication:</em></strong> Force users to provide HTTP
basic auth credentials when they configure endpoints to
receive webhooks.</li>
<li><strong><em>API retrieval:</em></strong> Provide only an event identifier in
webhook payload and force recipients to make a
synchronous API request to get the message’s full
contents.</li>
</ol>


  <p><a href="/assets/webhooks/signing-secrets@2x.png"></a></p>
  Endpoint signing secrets in Stripe's dashboard.


<p>Good security is possible, but a fundamental problem with
webhooks is that it’s difficult as a provider to <em>ensure</em>
that your users are following best practices. Of the three
options above, only the third guarantees strong security;
even if you provide signatures you can’t know for sure that
your users are verifying them, and if forced to provide
HTTP basic auth credentials, many users will opt for weak
ones, which combined with endpoints that are probably not
rate limited, leave them vulnerable to brute force attacks.</p>

<p>This is in sharp contrast to synchronous APIs where a
provider gets to choose exactly what API keys will look
like and dictate best practices around how they’re issued
and how often they’re rotated.</p>

<h3><a href="#development">Development and testing</a></h3>

<p>It’s relatively easy to provide a stub or live testmode for
a synchronous API, but a little more difficult for
webhooks because the user needs some mechanic to request
that a test webhook be sent.</p>

<p>At Stripe, we provide a “Send test webhook” function from
the dashboard. This provides a reasonable developer
experience in that at least testing an endpoint is
possible, but it’s manual and not especially conducive to
being integrated into an automated test suite.</p>


  <p><a href="/assets/webhooks/send-test-webhook@2x.png"></a></p>
  Sending a test webhook in Stripe's dashboard.


<p>Most developers will know that manual testing is never
enough. It’ll get a program working today and that program
will probably stay working tomorrow, but without more
comprehensive CI something’s likely to break given a long
enough timeline.</p>

<h3><a href="#order">No ordering guarantees</a></h3>

<p>Transmission failures, variations in latency, and quirks in
the provider’s implementation means that even though
webhooks are sent to an endpoint roughly ordered, there are
no guarantees that they’ll be received that way.</p>

<p>For example, a provider might send a <code>created</code> event for
<code>resource123</code>, but a send failure causes it to be queued
for retransmission. In the meantime, <code>resource123</code> is
deleted and its <code>deleted</code> event sends correctly. Later, the
<code>created</code> event is also sent, but by then the consumer’s
received it after its corresponding <code>deleted</code>. A lot of the
time this isn’t a big problem, but consumers must be built
to be tolerant of these anomalies.</p>


  <p><a href="/assets/webhooks/out-of-order.svg"></a></p>
  A consumer receiving events out of order due to a send failure.


<p>In an ideal world, a real-time stream would be reliable
enough that a consumer could use it as an <a href="https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying">ordered
append-only log</a> which could be used to manage state
in a database. Webhooks are not this system.</p>

<h3><a href="#versioning">Version upgrades</a></h3>

<p>For providers that version their API like we do at Stripe,
version upgrades can be a problem. Normally we allow users
to explicitly request a new version with an API call so
that they can verify that their integration works before
upgrading their account, but with webhooks the provider has
to decide in advance what version to send. Often this leads
to users trying to write code that’s compatible across
multiple versions, and then flipping the upgrade switch and
praying that it works (when it doesn’t, the upgrade must be
rolled back).</p>

<p>Once again, this can be fixed with great tooling, but
that’s more infrastructure that a provider needs to
implement for a good webhook experience. We recently added
a feature that lets users configured the API version that
gets sent to each of their webhook endpoints, but for a
long time upgrades were a scary business.</p>


  <p><a href="/assets/webhooks/upgrade-version@2x.png"></a></p>
  Upgrading the API version sent to a webhook endpoint in Stripe's dashboad.


<a href="#kitchens">The toil in the kitchens</a>

<p>Possibly a bigger problem than any of their user
shortcomings is that webhooks are painful to run. Let’s
look at the specifics.</p>

<h3><a href="#misbehavior">Misbehavior is onerous</a></h3>

<p>If a consumer endpoint is slow to respond or suddenly
starts denying requests, it puts pressure on the provider’s
infrastructure. A big user might have millions of outgoing
webhooks and just them going down might be enough to start
backing up global queues, leading to a degraded system for
everyone.</p>

<p>Worse yet, there’s no real incentive for recipients to fix
the problem because the entirety of the burden lands on the
webhook provider. We’ve been stuck in positions where we
have to email huge users as millions of failed webhooks
pile up in the backlog with something like, “we don’t want
to disable you, but please fix your systems or we’re going
to have to” and hoping that they get back to us before
things are really on fire.</p>

<p>You can put in a system where recipients have to meet
certain uptime and latency SLAs or have their webhooks
disabled, but once again, that needs additional tooling and
documentation, and the additional restrictions won’t make
your users particularly happy.</p>

<h3><a href="#retries">Retries</a></h3>

<p>To ensure receipt, webhook system needs to be built with
retry policies. A recipient could shed a single request due
to an intermittent network problem, so you retry a few
moments later to ensure that all messages make it through.</p>

<p>This is a nice feature, but is expensive and wasteful at
the edges. Say for example that a user takes down one of
their servers without deleting a corresponding endpoint. At
Stripe, we’ll try to redeliver every generated event 72
times (once an hour for three days) before finally giving
up, which could mean tens of thousands wasted connections.</p>

<p>You can mitigate this by disabling endpoints that look like
they’re dead and sending an email to notify their owner,
but again this needs to be tooled and documented. It’s a
bit of a compromise because you have less tech savvy users
who legitimately have a server go down for a day or two,
and may later be surprised that their webhooks are no
longer being delivered. You can also have endpoints that
are “the living dead”: they time out most requests after
tying up your clients for 30 seconds or so, but
successfully respond often enough that they’re never fully
disabled. These are costly to support.</p>

<h3><a href="#chattiness">Chattiness and communication (in)efficiency</a></h3>

<p>Webhooks are one HTTP request for one event. You can apply
a few tricks like keeping connections open to servers that
you deliver to frequently to save a few round trips on
transport construction (for setting up a connection and
negotiating TLS), but they’re a very chatty protocol at
heart.</p>

<p>We’ve got enough modern languages and frameworks that
providers can build massively concurrent implementations
with relative ease, but compared to something like
streaming a few thousand events over a big connected
firehose, webhooks are very inefficient.</p>

<h3><a href="#internal-security">Internal security</a></h3>

<p>The servers sending webhooks are within a provider’s
internal infrastructure, and depending on architecture, may
be able to access other services. A common “first timer”
webhooks provider mistake is to not insulate the senders
from other infrastructure; allowing an attacker to probe it
by configuring webhook endpoints with internal URLs.</p>


  <p><a href="/assets/webhooks/attack.svg"></a></p>
  An attacker crafting a malicious webhook to target an internal service.


<p>This is mitigable (and every big provider has measures in
place to do so), but webhook infrastructure will be
dangerous by default.</p>

<a href="#features">What makes webhooks great</a>

<p>We’ve talked mostly about the shortfalls of webhooks, but
they’ve got some nice properties too. Here are a few of
their best aspects.</p>

<h3><a href="#balancing">Automatic load balancing</a></h3>

<p>A commonly overlooked but <em>amazing</em> feature of webhooks is
that they provide automatic load balancing and allow a
consumer’s traffic to ramp up gracefully.</p>

<p>The alternative to webhooks is some kind of “pull” API
where a provider streams events through a connection. This
is mostly fine, but given enough volume, eventually some
kind of partitioning scheme is going to be needed as the
stream grows past the capacity of any single connection
(think like you’d see in <a href="https://kafka.apache.org/">Kafka</a> or
<a href="http://docs.aws.amazon.com/streams/latest/dev/key-concepts.html">Kinesis</a>). Partitioning works fine, but is
invariably more complicated and makes integrating more
difficult. Getting consumers to upgrade from one to two
partitions when the limits of a single partition are
reached is <em>really</em> difficult.</p>

<p>With webhooks, scaling is almost entirely seamless for
recipients. They need to make sure that their endpoints are
scaled out to handle the extra load, but this is a well
understood problem. Horizontal scaling combined with an
off-the-shelf load balancer (DNS, HAProxy, ELBs, …) will
make this relatively painless.</p>

<h3><a href="#http">Lingua franca</a></h3>

<p>Web servers are absolutely ubiquitous across every
conceivable programming language and framework which means
that everyone can receive a webhook, and without pulling
down any unusual dependencies.</p>

<p>Webhooks are <em>accessible</em> in a way that more exotic
technologies may never be, and that by itself is good
reason to use them. Accessible technologies have a greater
pool of potential developers, and that’s going to lead to
more integrations. An easy API in the form of webhooks has
undoubtedly helped companies like GitHub and Slack grow
their platforms.</p>

<a href="#road-ahead">The road ahead</a>

<p>Lately I’ve been talking about what <a href="/api-paradigms">API paradigms might
look like beyond our current world of
REST</a>, so it seems like a good time to look
at some modern alternatives to webhooks.</p>

<h3><a href="#http-log">The HTTP log</a></h3>

<p>Since the inception of webhooks there’s been a few
technologies that have been standardized that are
well-suited for streaming changes. <a href="https://en.wikipedia.org/wiki/WebSocket">WebSockets</a>
and <a href="https://en.wikipedia.org/wiki/Server-sent_events">server-sent events</a> (SSE) are two great examples.</p>

<p>Consumers would negotiate a stream over HTTP with the
normal RESTish API, and hold onto it listening for new
events from the server as long as they can. Unlike
webhooks, events are easily accessible from any environment
(that allows outgoing connections), fully verified,
ordered, and even potentially versioned according to the
consumer’s request.</p>

<p>A downside is that it’s the consumer’s responsibility to
make requests and track where they left off. This isn’t
an overly difficult requirement, but it’s likely to cause
problems for at least some users as they lose their place
in the stream, or don’t fetch incoming events in time.
Providers would undoubtedly also have to put limits on how
far back in history users are allowed to request, and have
an implementation that makes sending lots of aging event
data efficient.</p>

<h3><a href="#graphql">GraphQL subscriptions</a></h3>

<p>Along with queries and mutations, GraphQL supports a third
type of operation called a “subscription” (<a href="https://facebook.github.io/graphql/#sec-Subscription">see that in the
spec here</a>). A provider provides an available
subscription that describes the type of events that a
recipient will receive:</p>

<pre><code>subscription StoryLikeSubscription($input: StoryLikeSubscribeInput) {
  storyLikeSubscribe(input: $input) {
    story {
      likers { count }
      likeSentence { text }
    }
  }
}
</code></pre>

<p>Along with an input type that recipients will use to
specify the parameters of the stream:</p>

<pre><code>input StoryLikeSubscribeInput {
  storyId: string
  clientSubscriptionId: string
}
</code></pre>

<p>Like with a lot of GraphQL, the specifics around
implementation for subscriptions aren’t strongly defined.
In <a href="http://graphql.org/blog/subscriptions-in-graphql-and-relay/">a blog post announcing the feature</a>, a
Facebook engineer mentions that they receive subscription
events over an <a href="http://mqtt.org/">MQTT</a> topic, but lots of options for
pub/sub technology are available.</p>

<h3><a href="#grpc">GRPC streaming RPC</a></h3>

<p><a href="http://www.grpc.io/">GRPC</a> is a framework created by Google that enables
easy remote procedure calls (RPC) from client to server
across a wide variety of supported languages and platforms.
It builds on top of protocol buffers, a well-vetted
serialization technology that’s been around for more than a
decade.</p>

<p>Although it’s largely used for one-off request/responses,
it also supports <a href="http://www.grpc.io/docs/guides/concepts.html#server-streaming-rpc">streaming remote procedure
calls</a> where a provider can send back any
number of messages before the connection is finalized. This
simple Go example demonstrates roughly how it works (and
keep in mind that the feature is available in GRPC’s
impressive set of supported languages):</p>

<pre><code>stream, err := client.ListFeatures(...)
if err != nil {
    ...
}
for {
    feature, err := stream.Recv()
    if err == io.EOF {
        break
    }
    if err != nil {
        ...
    }
    log.Println(feature)
}
</code></pre>

<p>Bi-directional streams are also supported for back and
forth communication over a single re-used connection.</p>

<a href="#today">What to do today</a>

<p>Webhooks are a fine system for real time streaming and
providers who already offer them and have their operational
dynamics figured out should probably stick with them. They
work well and are widely understood.</p>

<p>However, between somewhat less-than-optimal developer
experience and considerable operational concerns, providers
who are building new APIs today should probably be
considering every available option. Those who are already
building systems on non-REST paradigms like GraphQL or GRPC
have a pretty clear path forward, and for those who aren’t,
modeling something like a log over HTTP/WebSockets/SSE
might be a good way to go.</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://feedproxy.google.com/~r/TroyHunt/~3/H-W0Jm-KDQE/">How Important Are Qualifications to Modern Technology Jobs?</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://feeds.feedburner.com/TroyHunt">Troy Hunt</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">security</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Thu Sep 28 2017 09:09:58 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p><a href="http://www.symantec.com/ready-secure-go/it-smb/?om_ext_cid=ws_ad_TROY_DBC-Bulldog_IT-SMB_Encrypt"><strong>Presently sponsored by:</strong> Get a security solution that will keep your website up and running—and keep you sleeping soundly: Symantec Website Security. Learn how</a></p><div><p>I've been thinking a lot about the relevance of formal education such as university degrees for those of us working in tech lately. Not just degrees, but various other forms of certifications so for the sake of simplicity, let's bundle it all up into &quot;qualifications&quot;:</p>
<blockquote>
<p>qualification<br />
/ˌkwɒlɪfɪˈkeɪʃ(ə)n/</p>
<p>Noun: a pass of an examination or an official completion of a course, especially one conferring status as a recognized practitioner of a profession or activity.</p></blockquote>
<p>This post has actually been in the works for a while, but there were 2 catalysts for finally writing it, both of which occurred very recently and both of which I felt would make worthy contributions to the narrative. But first, let me start with my own story and then my father's because they both add important context.</p>
My Personal Journey
<p>My original thinking was that I'd be a pilot, just like dad. It was glamorous (at least it was for me as a kid in the 80's) and was the only job I really knew because most of our family friends were from the airline industry. Other than my grandfather who was a farmer, I didn't have much more insight into what adults did and milking cows certainly wasn't my thing.</p>
<p>As I grew up, my aeronautical aspirations began to fade. I found computers which, like it is for most kids, began with me toiling away with a PC at home. I would have been about 11 or 12 when my family first bought one so it was all the stuff kids of that era did, namely playing (and cracking) games. A half dozen years later when it came time to think about an actual career, planes were totally off the radar and I was well and truly hooked on technology. I was living in Singapore for the last couple of years of high school around the early to mid-90's and that was an absolute tech haven with cheap, cutting edge PCs, software and even job opportunities. As a 16-year-old, I was able to work in a satellite systems engineering company part time doing hardware maintenance on PCs whilst also tutoring other expatriates on how these newfangled things worked. It was basic stuff - how to use Microsoft Word or make Encarta run - but it was the beginning of real-world experience.</p>
<p>As school was wrapping up, I started thinking about university because that's just what you did back then. So, I came back to Australia and started a computer science degree and with that came my first exposure to the internet (this was now '95). I'd been toying with bulletin board systems whilst back in Singapore but this was something else altogether - and it was awesome! I desperately wanted to start building for the web but there was absolutely <em>nothing</em> in the way of education available at uni. Over the coming years I'd do COBOL and relational database design but that was the closest I ever got to practical knowledge about how to build software for the internet. In amongst all that, I was doing discrete mathematics and chemistry, both of which I <em>hated</em> yet was forced to participate in &quot;because it's a science degree&quot; (the former I actually did very well at, the latter very badly at). It was driving me nuts - I couldn't learn what I was passionate about and I was wasting my time on things that in my view then (and even now), were completely useless.</p>
<p>So in that first year of uni, I bought a book that I still have today:</p>
<p></p>
<p>And that was the beginning of my web development career. Soon after, I was building websites for people in my spare time. A while after that I had an opportunity to actually build a pretty serious web application for very good money, but it required a larger time commitment so I dropped the uni back to part time. Not too much later, the real-world opportunities to build software were growing day by day. Yet even two and a half years after beginning the degree, I <em>still</em> had zero opportunity to formally learn anything remotely related to web development - we were now all the way up to Internet Explorer 4 and still there was absolutely nothing useful I could study at uni! So I dropped out. And I've never looked back. Not once.</p>
When Qualifications Matter
<p>I always remember dad studying incessantly; there were constant checks on his theoretical knowledge, flying ability and of course, his health (mostly I remember mum telling my brother and I to be quiet so we didn't break dad's concentration). There was a huge amount of formal structure around the assessment of his professional capabilities and really, that seems quite reasonable: he was flying jets around with hundreds of people sitting on board, their lives literally in his hands.</p>
<p>Towards the end of his flying career, he was taking off out of the mountainous Kagoshima airport in Japan when <a href="http://avherald.com/h?article=413dcd11">a number of turbine blades on the starboard jet of his 767 exploded from the outboard side of the engine</a>. He took control from the first officer and flew the plane back to the airport on one engine whilst the other one spewed flames, eventually setting the grass on fire once safely back on the ground.</p>
<p></p>
<p>He had to make life and death style decisions the likes of which required extensive training and qualifications. There is no &quot;Landing a Burning Airliner for Dummies&quot; book, you just don't pick this sort of stuff up through practical experience alone.</p>
<p>In so many ways, this is a polar opposite story to my own but it sets the scene for what I want to talk about next. I mentioned two catalysts for finally writing this post, let's start with the first one:</p>
The Equifax CSO's Music Degree
<p>A couple of weeks ago, a story hit the news about <a href="http://www.marketwatch.com/story/equifax-ceo-hired-a-music-major-as-the-companys-chief-security-officer-2017-09-15">the Equifax CSO Susan Mauldin having a music degree</a>. The innuendo was that she was unqualified; how on earth could she hold a C-suite executive position managing the cybers whilst only having studied music?!</p>
<p>I shared the story via a tweet and added a quick bit of context along the lines of &quot;I hope there's more to it than that&quot; which in my mind, meant &quot;I hope there's more to her professional experience that would qualify her for the role than simply having studied something entirely unrelated a long time ago&quot;. Unfortunately, I didn't make that sentiment clear enough and as the internet is prone to do, it got angry thinking that I was agreeing with what the story's headline was implying. Clearly, given my own lack of formal qualifications, that was not my intention.</p>
<p>Let's break this situation into two separate but important issues and I want to start with this tweet from the grugq:</p>
<blockquote><p>It was people without college degrees in infosec that invented the shit they teach in college infosec courses.</p>— the grugq (@thegrugq) <a href="https://twitter.com/thegrugq/status/909006190012592128">September 16, 2017</a></blockquote>

<p>This speaks precisely to the point about the value of competency over qualifications. Technology is an industry in which people have done wonderful things without such qualifications. World-changing things, in some cases; <a href="http://content.time.com/time/specials/packages/article/0,28804,1988080_1988093_1988082,00.htm">Bill Gates dropped out of college.</a> <a href="http://content.time.com/time/specials/packages/article/0,28804,1988080_1988093_1988086,00.html">Steve Jobs dropped out of college.</a> <a href="http://content.time.com/time/specials/packages/article/0,28804,1988080_1988093_1988090,00.html">Mark Zuckerberg dropped out of college.</a>. Now I'm by no means advocating that if you're presently studying you should immediately drop out and become successful, rather I'm acknowledging that there is clearly more to one's competency and success than a formal qualification alone. I'll come back to that again later on.</p>
<p>The second important issue as it relates to Equifax is that under Susan's watch, something went very badly wrong. Actually, <em>many</em> things went very badly wrong ranging from the breach itself that originally hit the press to the debacle with the search site they stood up afterwards to the subsequent discovery of an earlier breach (and we're only just scratching the surface here). <a href="http://money.cnn.com/2017/09/15/news/equifax-top-executives-retiring/index.html">Her subsequent removal from the company</a> (&quot;retirement&quot; in this instance appears to be a term of convenience) and then her apparent scrubbing (or hiding) of her LinkedIn profile is not a good look and it's no wonder that people then probed into her background and started asking questions about her competency.</p>
<p>I have no idea how much of the Equifax debacle was directly Susan's fault. I also have no idea what constraints she was operating under (i.e. tight budgets she fought against) nor how she conducted herself in her role in terms of her professionalism. But I do know that she held multiple infosec roles before her eventual departure from Equifax and that her music degree was gained decades earlier. It might make for easy press-fodder, but her tertiary education <em>is not</em> the issue we should be focusing on here. Clearly there were issues, but a degree in something non-infosec different is not one of them.</p>
Pluralsight
<p>The other catalyst for writing this came as I attend <a href="https://www.pluralsight.com/event-details/2017/pluralsight-live-thank-you">Pluralsight Live last week</a>.</p>
<p></p>
<p>I was watching the presentations around how much content was now in the library, how many hours had been viewed and how it was changing people's lives. Now look, an event like this is always going to be a bit &quot;ra-ra&quot; in terms of talking about how awesome things are, but underpinning all the hype were some fantastic truths about how people were learning technical skills that have made a tangible difference to their lives. Real world, modern and immediately applicable skills that people can use to convert time and expertise into a professional living.</p>
<p>People such as my lovely friend Karoline Klever in Norway who used Pluralsight during her maternity leave to actually <em>progress</em> her career during that period. It's a couple of years old now, but I love this profile on her:</p>

<p>Or my friend John Opdenakker in Belgium who <a href="https://www.peerlyst.com/posts/how-i-prepared-for-the-certified-ethical-hacker-exam-john-opdenakker?utm_source=Twitter&amp;utm_medium=Application_Share&amp;utm_content=peerlyst_post&amp;utm_campaign=peerlyst_shared_post">used the CEH content I worked on to pass his Certified Ethical Hacker exam</a>. In John's case, he <em>did</em> actually achieve a professional qualification and I'll come back to CEH a little later on too.</p>
<p>I love that Pluralsight gives people like Karoline and John these opportunities. I love that it's affordable (personal subscriptions <a href="https://www.pluralsight.com/learn">start at $29/m</a>), I love that it's self-paced and I love that it's available to anyone anywhere in the world regardless of their access to traditional education. And I love that I've been able to play a part in that.</p>
<p>It's 20 years now since I left university and frankly, if I had have had access to Pluralsight then I doubt I would have lasted more than 6 months in the degree. I'm sure universities have progressed since the mid-90's in terms of how current their content is, but I'm equally sure that they rarely have material taught by industry experts who are absolutely at the forefront of their field and are still actively using the technology in a professional capacity.</p>
Opportunity Costs
<p>What really got me whilst still at uni was the opportunity cost; every moment I was sat in a classroom studying the periodic table for that pointless chemistry course, I wasn't learning something that was genuinely useful. I wasn't building anything and I wasn't gaining any practical experience. All of these things grated on me enormously and it felt like a constant weight.</p>
<p>The other cost I was rapidly racking up with the cost of studying the degree. I can't even remember exactly how much debt I built up during the time I was there (in Australia, we have <a href="http://studyassist.gov.au/sites/studyassist/helppayingmyfees/hecs-help/pages/hecs-help-welcome">HECS</a> which is effectively a government subsidised loan), but because I found work early on (and inevitably also because I didn't finish the degree), I paid it down pretty quickly. Most people aren't that fortunate.</p>
<p>Through a quick bit of searching around, I found a piece from last week that suggests <a href="https://www.topuniversities.com/student-info/student-finance/how-much-does-it-cost-study-australia">a science bachelor's degree in Australia will set you back up to A$43,632 <em>per year</em></a> which is a massive cost over a typical 3 year degree (that'd run you over US$100k in total for international readers). That figure seems to be pretty consistent with other sources such as <a href="https://www.quora.com/How-much-does-it-cost-to-study-Bachelors%E2%80%99-in-Computer-science-in-the-United-States">this Quora answer from last year</a>.</p>
<p><strong>Edit:</strong> A few people have pointed out that there are government subsidies available which significantly reduce the cost of tertiary education.</p>
<p>Now none of this is to say that those degrees aren't worth it and I'll come back to that in more detail in a moment, rather it's to say that the opportunity cost is 3 years and a hundred-something grand; what else could you do with that? How would that change your long-term prospects as opposed to doing the degree? At the very least, it warrants careful consideration.</p>
The Perception of Tertiary Qualification Value
<p>Whilst I've tried to be a bit generic about qualifications in general rather than university or college degrees specifically (and I know I've started to drift this way anyway), let me talk a bit about those in particular for a moment.</p>
<p>In thinking back to my own career, one of the things I remember thinking about as it related to my incomplete degree was that if an organisation rejected me on that basis alone, that wasn't the right place for me. I mean if I went for a job and they said &quot;wow, you've got these years of experience and done awesome things but sorry, policy dictates that you need a degree&quot; then frankly, I wouldn't have wanted to work in that environment. I just couldn't bring myself to be surrounded by people with that mentality.</p>
<p>Anecdotally, there seem to be fewer and fewer organisations expecting a tertiary qualification for technology roles. For example, in the wake of the Equifax situation, <a href="https://blog.safestack.io/safestack-stands-with-nztalent-a5c724a6b502">Laura Bell from SafeStack.io made the following clear</a>:</p>
<blockquote>
<p>We are declaring that we do not require a traditional tertiary qualification for a range of skills-based roles within our company.</p>
</blockquote>
<p>She went on to clarify that they've <em>never</em> required a tertiary qualification and that they never will.</p>
<p>One of the things that I myself found during my corporate life was that I simply couldn't correlate formal qualifications with competency. I'd interview people - probably hundreds of them over the years - and naturally, a qualification such as a university degree is one of the first things you'd see on the CV. But in no way whatsoever could I see the value of that represented in any of the technical competency tests we'd do. In fact, difficulty in establishing candidate competency was what led me to write my first ever blog post on <a href="https://www.troyhunt.com/why-online-identities-are-smart-career/">why online identities are smart career moves</a>. In that post, I lamented how hard it was to find people with even basic technical knowledge:</p>
<blockquote>
<p>The number of times I've interviewed people and they struggled with the most fundamental of questions is staggering. Granted, recruitment agencies have a lot of blame to share but at the end of the day if you're calling yourself a senior .NET developer and can't even write code to declare a nullable type or instantiate a generic collection, you've got issues.</p>
</blockquote>
<p>But hey, they had a degree so the recruitment agency sent them through! The very point of that blog post was that you need way more than a CV to actually demonstrate your competence.</p>
<p>(Fun fact: in that first ever blog post I said &quot;I don’t necessarily mean to try and achieve semi celebrity status like Scott Guthrie or Joel Spolsky&quot;. In reading it again now, I just realised the 3 of us delivered keynotes from the same stage at Pluralsight Live last week which I reckon is pretty cool!)</p>
<p>And this is really the heart of the problem: a qualification is not experience. A qualification is evidence you've passed a test. The difference between a qualification alone and experience in the real world is, well, this sums it up beautifully:</p>
<blockquote><p>When you lied on your CV about having previous sheepdog experience. <a href="https://t.co/fecGfhE9YD">pic.twitter.com/fecGfhE9YD</a></p>— Paul Bronks (@BoringEnormous) <a href="https://twitter.com/BoringEnormous/status/912262374756937728">September 25, 2017</a></blockquote>

<p>And now that I've said all that, qualifications can still be enormously useful in the tech industry so let's look at where they make sense.</p>
So Are Formal Qualifications Actually Worth It?
<p>There's an easy answer albeit one that can be hard to arrive it: formal qualifications are worth it when they mean something to other people. For example, there are still organisations that expect to see a degree if you want a job there, albeit not the ones I would want to work for! I obviously don't agree with that prerequisite for tech jobs, but if it's a company (or industry) you genuinely want to work with then tertiary qualifications may be very important to you. It's not that different to how it was for dad flying jets - you don't get to do that unless you can pass tests and have papers to prove it.</p>
<p>The same can be said about certifications. Last year <a href="https://www.troyhunt.com/careers-in-security-ethical-hacking-and-advice-on-where-to-get-started/">I wrote about careers in security</a> and touched on the Certified Ethical Hacker series I'd contributed to building out on Pluralsight. In there, I referenced <a href="https://www.fastlaneus.com/dod-8570">the DoD's Information Assurance Workforce Improvement Program</a> and the CEH prerequisite:</p>
<blockquote>
<p>The United States of America Department of Defense issued Directive 8570 in 2004 to mandate baseline certifications for all Information Assurance “IA” positions. In February of 2010, this directive was enhanced to include the Certified Ethical Hacker across the Computer Network Defense Categories “CND”.</p>
</blockquote>
<p>Clearly, if you want to work for the DoD then a CEH cert (or a number of other certs they recognise) is important. I actually copped some criticism from a few folks for not having sat the CEH exam myself after writing the (predominantly) web security components of it but for me it's easy - the cert wouldn't change anything for me in any tangible fashion so I haven't bothered with it. It's that simple.</p>
<p>Obviously, there are other industries where formal qualifications are far more a necessity than the tech industry and dad's case is one great example. Same again for a profession like medicine; as with aviation, it's heavily regulated because if you screw up, people die. You screw up in my profession and the CSS is a bit wonky in Internet Explorer! Indeed, that's also been a criticism of the tech industry in that anyone can jump up and start working in it with next to no knowledge. Heck, I'm making a career out of folks getting that wrong all the time in security! Of course, there are systems that must operate within regulatory guidelines such as PCI and HIPAA, but that's not quite the same as demanding specific qualifications from the practitioners building those systems.</p>
<p>On the tertiary education side of things, I'm actually <em>for</em> attending university after secondary school for folks that are trying to find their way in the world. A very positive attribute of studying for a degree is that it's a commitment that's much harder to break than, say, simply stopping a Pluralsight course. It helps you learn aspects of technology you may or may not like and it exposes you directly to people that can expand your horizons. Many people <em>need</em> that structure.</p>
<p>But ultimately, all of this is merely a means to an end in that it helps you along that journey to a successful career. If formal qualifications are the thing that does that for you then awesome - that's a win. But it's one path and whereas in times gone by it was considered <em>the</em> way into a professional career, clearly that's changing and <a href="http://www.afr.com/news/policy/industrial-relations/shortage-of-it-graduates-a-critical-threat-20140203-iy4lx">the decline in students starting IT degrees</a> is evidence of that.</p>
<p>The industry has clearly changed a lot over the last couple of decades. The monolithic approach to study which was &quot;make a large financial commitment over many years&quot; is now complimented with the microservices of education: break knowledge acquisition down into smaller decoupled atomic units focusing on discrete topics, Pluralsight style, if you will. But the best thing of all is that now you can choose and hey, maybe that means you do both; study for a degree and compliment that with online learning or additional certifications. Man, I wish I had that choice 20 years ago!</p>
Summary
<p>One of my most infuriating post-dropout moments came some years later when I called up the uni to inquire about what might be involved in finishing the degree. &quot;Oh no&quot;, the lady on the line explained, &quot;all your credits have expired so you'd have to start again&quot;. Wait - so if I'd <em>finished</em> the degree would the knowledge I'd earned still be considered null and void by the uni? To which she replied &quot;No, you'd still have the degree&quot;. That was a real penny-drop moment for me: study alone can be deemed null and void in the future but experience, well, nobody can take that away from you.</p>
<p>Lastly, I thought I'd pump out a quick survey yesterday just to get a sense of where people's backgrounds lie:</p>
<blockquote><p>I'm writing a blog on qualifications in tech jobs and I'm curious: if you're in IT / programming / security, what's your background?</p>— Troy Hunt (@troyhunt) <a href="https://twitter.com/troyhunt/status/912931212213108736">September 27, 2017</a></blockquote>

<p>The poll result didn't surprise me, but the replies to it are really interesting. As you'd expect, there's lots of different backgrounds but the one piece of feedback that resonated consistently was how important experience is. It's like it's the great leveller in an industry that's heavily biased towards what you've done rather than the papers you hold. Regardless of how you've arrived in technology, I love that there are so many different paths here and frankly, that makes it all the more interesting.</p>
</div>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://rachelbythebay.com/w/2017/09/27/2153/">ntpd won&#39;t save you from one particular rogue bit</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://rachelbythebay.com/w/atom.xml">rachelbythebay</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">linux</span>
              <span class="tag">networking</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Thu Sep 28 2017 02:22:11 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>
I found a new way to screw up time on Unix boxes a couple of months ago.
Someone reported a box that looked very very strange, and I took a look.
Sure enough, it was in the year 2153.  Nothing was fixing it.  Normally,
ntpd will fall over if it's really far away from the current time.  
Then we'd notice that ntpd was not running.  But this?  It was totally 
cool with this.
</p><p>
How did it happen?  Near as anyone can tell, when the system came up, it
managed to read a value with a spurious extra bit set from the real time 
clock.  So, instead of only the lower 32 bits being set, one slightly 
higher also was set.
</p><p>
That bit was number 32.  As in 2^32, or 4294967296.  That's about 136
years worth of seconds (yeah yeah, leap seconds, leap years, I know, go 
away, that doesn't matter here).
</p><p>
So instead of the time being, say, 1506560587, as it is as I write this
tonight on September 27, 2017 (local time), it'll be 5801527883, or
<em>November 4, 2153</em>.  Slight difference, right?
</p><p>
ntpd... didn't care.  ntpdate... also didn't care.  Why should they?  
As far as they can tell, the lower 32 bits of time is just fine, so they
think nothing is wrong.  That's all they care about.
</p><p>
If you don't believe me, here, check it out.  I came up with something 
small and stupid which will mess up your clock in a controlled fashion.  
It will take you to the future or back again, and the ntp tools will 
keep on going like nothing ever happened.
</p><p>
First, let me me show you what running ntpdate looks like normally
using my handy testing box:
</p><p>

epenguin:/# date; ntpdate pool.ntp.org; date
<br />
Wed Sep 27 18:06:30 PDT 2017
<br />
27 Sep 18:06:36 ntpdate[765]: adjust time server 69.89.207.99 offset 
0.000582 sec
<br />
Wed Sep 27 18:06:36 PDT 2017
<br />
epenguin:/# 

</p><p>
It takes about six seconds to do its thing, and you can see that my
system clock was pretty close to the time server which it happened to
pick from the pool.
</p><p>
Now here's what happens when your clock is really screwed up.  Notice
the &quot;offset&quot; when it fixes things.
</p><p>

epenguin:/# date -s &quot;Dec 25 2017&quot;; ntpdate pool.ntp.org; date
<br />
Mon Dec 25 00:00:00 PST 2017
<br />
27 Sep 18:08:12 ntpdate[769]: step time server 173.71.73.207 offset 
-7627914.525158 sec
<br />
Wed Sep 27 18:08:12 PDT 2017
<br />
epenguin:/# 

</p><p>
Huge offset, right?  That's because it isn't Christmas yet.
</p><p>
Given this, you'd assume that an offset of hundreds of years would 
generate a ridiculous offset as it dragged the clock backwards, and, 
well, you'd usually be right, except for this special situation where
you hide all of the &quot;extra time&quot; behind that one bit.
</p><p>
Here, I run my time shifting program, and watch what happens.
</p><p>

epenguin:~# date; ./timeshift; date
<br />
Wed Sep 27 18:09:54 PDT 2017
<br />
Your clock is in the present and is boring.  Warping time!
<br />
old tv_sec: 1506560994
<br />
new tv_sec: 5801528290
<br />
Sun Nov  4 00:38:10 PDT 2153
<br />
epenguin:~# 

</p><p>
Uh oh!  Save us, ntpdate!
</p><p>

epenguin:~# date; ntpdate pool.ntp.org; date
<br />
Sun Nov  4 00:38:31 PDT 2153
<br />
 4 Nov 00:38:38 ntpdate[776]: adjust time server 204.9.54.119 offset 
0.002158 sec
<br />
Sun Nov  4 00:38:38 PDT 2153
<br />
epenguin:~# 

</p><p>
Yep, not gonna happen.  You are stuck in the future.
</p><p>
I'm using ntpdate for simplicity here, but trust me, ntpd will accept 
it too.  It'll happily come up, pick a source, and will declare itself
synchronized.
</p><p>
Don't believe me?  Try it for yourself.  Here's the dumb little tool 
which will flip 2^32 in your system clock.
</p><p>
<pre>
#include &lt;errno.h&gt;
#include &lt;stdio.h&gt;
#include &lt;string.h&gt;
#include &lt;sys/time.h&gt;
 
int main() {
  struct timeval tv;
  struct timezone tz;
 
  if (gettimeofday(&amp;tv, &amp;tz) != 0) {
    fprintf(stderr, &quot;gettimeofday failed: %s\n&quot;, strerror(errno));
    return 1;
  }
 
  if (tv.tv_sec &amp; 0x100000000) {
    printf(&quot;Your clock is in the future.  Pulling it back.\n&quot;);
  } else {
    printf(&quot;Your clock is in the present and is boring.  Warping time!\n&quot;);
  }
 
  printf(&quot;old tv_sec: %ld\n&quot;, tv.tv_sec);
 
  tv.tv_sec ^= (long) 0x100000000;
 
  printf(&quot;new tv_sec: %ld\n&quot;, tv.tv_sec);
 
  if (settimeofday(&amp;tv, &amp;tz) != 0) {
    fprintf(stderr, &quot;settimeofday failed: %s\n&quot;, strerror(errno));
    return 1;
  }
 
  return 0;
}
</pre>
</p><p>
Incidentally, if you try to reproduce this by using a bunch of shell 
magic to run 'date +%s', OR it with 2**32, then 'date -s' that value, 
it'll take some luck to reproduce this with ntpd.  Part of the problem 
is that 'date -s' will shear off the fractional bits of the current 
time.  If the resulting time that ntpd sees is more than a few 
milliseconds off, it'll step the clock, and that will clear out the 
future time.  That's why I wrote this program: testing it with a shell 
script involved far too many failed attempts.
</p><p>
Interested in reading more on the topic?  Check out
<a href="http://www.ntp.org/ntpfaq/NTP-s-algo.htm#AEN1895">&quot;How is Time 
encoded in NTP?&quot;</a>.
</p><p>
Be sure to pay attention to the whole thing about the NTP era.
</p><p>
Incidentally, when this happens, 'hwclock --systohc' will probably fail.
It generates some neat error messages.  This means you can't actually
persist the bad time to your RTC.  In all likelihood, it will vanish 
after a reboot, and will not be seen again until you happen to have a
single bit error in the clock at boot.
</p><p>
Of course, if you have enough machines and enough reboots, you'll 
probably see this eventually.  You may have already seen it and just 
didn't know why it happened, or why it persisted.  Hopefully this helps.
</p><p>
Time is hard.
</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://developers.soundcloud.com/blog/deliver-software-faster-by-managing-work-in-progress-not-by-adding-overtime">Deliver software faster by managing work in progress, not by adding overtime</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://developers.soundcloud.com/blog.rss">Soundcloud Developers Backstage Blog</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">devops</span>
              <span class="tag">practices</span>
              <span class="tag">company</span>
            </div>
            <span class="subtitle is-7">Wed Sep 27 2017 01:00:00 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>Product development flow (flow) is the rate at which our products are developed, from idea to deployment. Good flow means that products should pass through the development cycle quickly and continuously.</p>

Problem Statement

<p>SoundCloud faces challenges with its product development flow. SoundCloud’s CTO, Artem Fishman, summed up these challenges succinctly during the May 2017 Engineering Town Hall.</p>

<p></p>

<blockquote>
<p>“We have a ton of [good] product pressure… We work really hard, but are fundamentally slow to deliver… This resonated everywhere. It takes a very long time to ship something here.”</p>
</blockquote>

<p>His conclusion was reached by interviewing over a hundred employees from the Technology Organization. It is clear that employees understand these challenges on a qualitative and intuitive level.</p>

<p>These are classic symptoms of an overloaded product development queue.</p>

<p>The problem was then for project managers to develop quantitative metrics for understanding our product development flow and guidelines for improving the flow issues that the company already knew on qualitative and intuitive levels.</p>

Post Structure

<p>This post describes recent efforts to measure and improve software product development flow at SoundCloud. It demonstrates that we can deliver software faster by managing Work in Progress (WIP) and product quality rather than pushing employees to work overtime.</p>

<p>The second section describes the discovery phase in which we define and measure some variables for improving flow. Project managers used tools from the <a href="http://github.com/soundcloud/project-dev-kpis">Project Development KPIs Project</a> (PDKP) to understand the problems in our workflow. Understanding that data, they derived the strategy below for improving SoundCloud’s product development flow.</p>

<ol>
<li>The company should commit to fewer products at a time</li>
<li>Teams should work on fewer product features at a time</li>
<li>Engineers should work on fewer tasks at a time</li>
<li>Engineering should reduce its code inventory</li>
<li>Engineering should monitor and reduce the number of bugs over time</li>
</ol>

<p>The third section describes our continuous learning process and our first set of guidelines for improving flow.</p>

<p>The fourth and last section describes preliminary results of our efforts.</p>

Defining success and variables

<p>This section identifies lead and cycle time as metrics of success in product development and motivates their use. It identifies variables that have an impact on lead and cycle time.</p>

<h3>Lead and cycle time</h3>

<p>To improve business outcomes, businesses <a href="https://en.wikipedia.org/wiki/SMART_criteria">define the metrics for the desired outcome</a> then <a href="https://en.wikipedia.org/wiki/PDCA">iterate towards achieving that outcome</a> by observing if actions taken improve or worsen the metrics. Members of the Process Optimization Group at SoundCloud use lead and cycle time as two important measures of our success in product development.</p>

<p><strong>Lead time</strong> is the time between the initiation and completion of a product’s development. At SoundCloud, this is the time between when a product owner receives a work request to the time when the work is completed.</p>

<p><strong>Cycle time</strong> is the time between when the production of a product is started and when it is completed. At SoundCloud this is the time between when an engineering team receives a feature request to the time that the software is deployed to users.</p>

<p>Keeping lead and cycle times low has many benefits, four of which are noted here. The first benefit is that we ship products to users faster, before our competitors. Being first increases the chance that we capture market share.</p>

<p>The second benefit is the reduction of backlog size. Product managers are incentivized not to commit to projects too far in advance. Since requirements for new products change rapidly, a minimal backlog means more responsive plans and product ideas are used instead of outdated ones.</p>

<p>The third benefit concerns the size of products shipped. Focusing on minimizing lead time incentivizes teams to deliver new products as MVPs and smaller changes to existing products. This more granular approach decreases the probability of large failures and over-investment in projects that will not succeed.</p>

<p>The fourth and most important benefit is that SoundCloud can gain information on its product development faster. By shipping faster, we get market information before our competitors. We get feedback on a product earlier in its development cycle. We can make small adjustments in strategies towards our desired outcome rather than large corrections after over-investing in a product that had limited success.</p>

<p>How can SoundCloud improve lead and cycle time? There are several important variables we can leverage.</p>

<h3>Variables affecting lead and cycle time</h3>

<p>This section presents the definitions of variables affecting lead and cycle time as well as an explanation for the effect of each.</p>

<h4>The ratio of WIP to contributors</h4>

<p>The ratio of Work in Progress (WIP) to contributors is a measure of how much contributors in a team are multitasking on average. PDKP estimates this by dividing the number of WIP issues in a <a href="https://www.atlassian.com/software/jira">Jira</a> project by the number of engineers active on that project.</p>

<p>Keeping the ratio of WIP to contributors slightly below 1.0 can decrease cycle time in two ways. First, if engineers develop one product at a time, products will be ready sooner than if they multitask. Say an engineer has two tasks, each of which will take two days. They can either work on both in parallel or one after the other. The schedules of these two options are shown below.</p>

<p></p>

<p><div>Scheduling two tasks for one engineer</div><br /></p>

<p>In Schedule 1, the engineer works on both in parallel, task 1 is complete at the end of day 3 and task 4 is complete at the end of day 4. The more the engineer interleaves the work, the longer it takes to complete both tasks. In the extreme case—multitasking within each day—both projects might not be completed until the end of day 4. In Schedule 2, the engineer works on the first and then the second, the first is completed at the end of day two and the second at the end of day four. By working on one thing at a time, the engineers have completed a task earlier without adding work!</p>

<p>The second reason to keep the ratio of WIP to contributors ratio slightly below 1.0 concerns the ability of individual members to change development roles quickly. To understand why, see the idealized model of a team’s workflow below.</p>

<p></p>

<p><div> Software engineering is an invisible “U-shaped flow cell.”</div><br /></p>

<p>Engineers need to quickly transition between planning, developing, reviewing, and fixing bugs. They fulfill multiple roles in the software development process. This collaboration pattern is similar to <a href="http://www.tpslean.com/glossary/cellmfgdef.htm">U-shaped flow cells</a> in Lean manufacturing.</p>

<p>Processing tasks in U-shaped cells can be very efficient, but not if the queue of tasks is overloaded. If more tasks are in progress than there are engineers to complete it, items will form queues waiting for attention. For example, say an engineer developing code is overloaded with new feature requests. A queue of feature requests will form. Further, it is likely that they will not be able to review code completed by their teammates, contributing to the formation of a second queue in the review stage. Either cycle time will rise or quality will suffer. Engineers need to switch roles to where queues are forming to push tasks through the queue.</p>

<p>Analogously, overloaded teams with a high WIP to contributors ratio will be less capable of dealing with incoming bug fixes and ad-hoc requests.</p>

<h4>The number of products in WIP</h4>

<p>“Products in WIP” are products in progress. PDKP estimates the number of products in WIP by counting the number of issues in a Jira project that represent a product. For most teams, these are ‘Epic’ or ‘Story’ issues. Products can be either for internal or external users.</p>

<p>Decreasing the number of products in WIP can decrease lead and cycle time. To illustrate, imagine we have two tasks each of which takes four days and can be shared between two engineers with no overhead. There are two schedules for this work shown below: either each engineer takes one task or they pair on the first, then the second.</p>

<p></p>

<p><div> Scheduling two tasks for two engineers</div><br /></p>

<p>In Schedule 1, each engineer takes a single task, both products ship in four days. In Schedule 2, the first task is ready in two days and the second at four days. We’ve shipped the first product two days earlier without adding any work. Pairing adds some overhead, but getting information from shipping a product sooner is often worth it. These same principles apply to the planning phases of product development.</p>

<h4>Inventory</h4>

<p>In business, inventory is the components of a product that have not been assembled or manifested into a product ready for sale. In engineering, our inventory is anything we work on that results in a feature that is yet undelivered including undeployed code, RFCs, and unfinished Jira issues. PDKP estimates inventory with the number of commits or number of additions and deletions in pull requests in repositories that have been updated in the last three months.</p>

<p>Increasing amounts of code inventory is a symptom of bad flow and has a negative effect on cycle time. The more inventory there is, the more development time has accumulated in work not being seen by our users. Generally, inventory depreciates in value as it ages.</p>

<p>Ideally, we want to achieve what is called a “one piece flow,” a process where all code inventory is actively being worked on. This means that there should be at most one product in progress per team and one task in progress per engineer.</p>

<p>The challenge in accomplishing one piece flow in software development is that code inventory is invisible. In a factory that produces physical goods, inventory is obvious, it stacks up on the factory floor. In a software company, it’s not so obvious.</p>

<p></p>

<p><div> Software inventory isn’t visible in the same way manufacturing inventory is.</div><br /></p>

<p>For this reason, we need to take measures to monitor code inventory and make sure it gets shipped in a timely manner.</p>

<h4>The ratio of features to engineers</h4>

<p>The ratio of features to engineers measures how many features each team member must maintain on average. We estimate it by dividing the number of configuration files indicating an individual feature in a team’s active repositories by the number of engineers active on a team’s Jira board.</p>

<p>Higher ratios correlate with higher rates of bug reports and maintenance overhead. If a team has an abnormally high ratio, the company should consider providing them with more headcount or redistributing their responsibilities to get a more even distribution.</p>

<h4>Bugs</h4>

<p>Our exposure to bugs is estimated by the number of issues of issue type 'Bug' in a Jira project.</p>

<p>Persistent bugs have a negative impact on lead and cycle time. Users and Community Support re-report the same bugs which then have to be reinvestigated before confirming they are duplicates. If engineers build on top of buggy code, the resultant implementation can be flawed, at worst resulting in wasted code that cannot stay in production. Most importantly, bugs make it harder for engineers to learn what is going wrong and to prevent future problems.</p>

<h4>Lower bound on time to backlog completion</h4>

<p>Multiplying a team’s cycle time by the number of products remaining in their backlog estimates a lower bound on how long it would take a team to complete their backlog.</p>

<p>Having a large backlog has three negative effects. First, it increases our lead time. Planning too far in advance creates a queue of work that teams cannot work through quickly and takes development time away from line managers and engineers. Second, it decreases our flexibility in changing plans in the future. Once something is on the backlog, teams feel committed to deliver them. Third, it decreases motivation.</p>

What did we do about it?

<p>This section describes SoundCloud’s continuous learning and improvement cycle where its project managers collect flow metrics and apply analysis thereof in guidelines to improve product development flow.</p>

<p>For the first set of guidelines, project managers used principles from manufacturing and queueing theory to encourage teams to work on fewer products at a time and engineers work on fewer tasks at a time.</p>

<p><em>Importantly, the guidelines don’t suggest that teams work harder. They are intended to increase focus, not the total number of hours worked.</em></p>

<h3>Metrics collection</h3>

<p>Starting in March 2017, PDKP has collected lead time, cycle time, WIP, bugs, contributor, and code inventory metrics from SoundCloud’s Jira and GitHub organizations.</p>

<p>To date, 18 engineering teams have configured the system to collect lead and cycle time metrics. Inventory metrics are collected for all development in SoundCloud’s GitHub organization.</p>

<p>All metrics are aggregated and presented in the PDKP <a href="/assets/posts/pdkp_overview_dashboard-ec0697a42a25d383155615a75db47027.png">dashboard</a> and <a href="/assets/posts/pdkp_summary_dashboard-058b83ec8e3595651dc5c377f51408f5.png">summary dashboard</a>.</p>

<h3>Guidelines and motivating analysis</h3>

<h4>1. The company should commit to fewer products at a time</h4>

<p>SoundCloud can reduce cycle time and increase its flexibility in planning new products by reducing the number of products it commits to. Teams had high lower bounds on time to backlog completion, suggesting that overcommitment was a problem.</p>

<p>Of the 18 teams in the PDKP metrics collection, the median lower bound on time to backlog completion is 766 days, or about 2.1 years. While this number could represent misuse of Jira, it suggested an opportunity to shift more time from planning future products to focusing on more immediate products.</p>

<h4>2. Teams should work on fewer product features at a time</h4>

<p>Provided the work can be parallelized, teams should work on one product at a time in order to maximize flow. The number of products teams had in progress was too high.</p>

<p>Across the 18 teams sampled, PDKP reported the median number of products in progress at once is four. An in-person survey of teams in the Creators Organization reported a similar number.</p>

<p>It is unlikely that there is enough downtime in the development of products that teams needed to have four in progress at any given time to be productive.</p>

<h4>3. Engineers should work on fewer tasks at a time</h4>

<p>In theory, engineers should work on one task at a time until it is completed to maximize flow. The ratio of tasks in WIP to engineers was too high.</p>

<p>Across the 18 teams sampled, PDKP reported the average ratio of WIP to engineers was consistently over 2.0.</p>

<p>Of course, it is not always practical to work on one thing at a time while developing software. For example, teams with long running batch jobs will have enough downtime while waiting for the job to finish to do meaningful work on other tasks. However, like teams with too many products, it is unlikely that the majority of teams with high ratios in the sample have this specific problem.</p>

<p>Importantly, high engineer utilization is not a goal. One important result of queuing theory is that <a href="https://hbr.org/2012/05/six-myths-of-product-development">not all nodes in the queue have to be fully utilized</a> to maximize the throughput of the queue overall. In fact, it’s good if our engineers have a little idle time.</p>

<h4>4. Engineering should reduce its code inventory</h4>

<p>SoundCloud has a high amount of code inventory. Reducing it is a big opportunity for us to improve these metrics.</p>

<p>Over SoundCloud’s GitHub repositories updated in the last three months at the time of the first metrics collection, there have been between 1,200 to 1,500 commits in open pull requests representing 200,000 to 300,000 lines of code. The average age of this inventory was 65 days old. On average, teams have 22 commits in open pull requests. That’s a lot.</p>

<p>Teams at SoundCloud have had success in increasing flow by decreasing inventory. The Content ID Team reduced its inventory over a period of two months, bringing the daily average number of commits in open PRs from 46 to less than 5 and the average age of those commits from 14 to less than 1 day. During this time, the team’s cycle time dropped from 12 days to 3 days.</p>

<p>Given this <a href="https://en.wikipedia.org/wiki/Longitudinal_study">longitudinal experimental design</a>, it’s only possible to show correlation—not causation—between reducing inventory and cycle time. However, the correlation is positive and suggests further effort on this front could help.</p>

<p>Importantly, code review should not be rushed. Rather, it is better to carefully and thoroughly review code as it reaches the review stage promptly or use a continuous integration review process where merging code into the master branch is not blocked by code review.</p>

<h4>5. Engineering should monitor and reduce the number of bugs over time</h4>

<p>Monitoring and reducing the number of bugs in a team’s system has a positive impact on lead and cycle time. SoundCloud is currently implementing a standardized bug reporting process. This is another opportunity to speed up both development and the learning feedback cycle.</p>

<h3>Guidelines for management</h3>

<p>The number of products product management commits to at once caused teams to overload their product development queues. Engineering managers then overloaded engineers by accepting multiple products into WIP at once.</p>

<p></p>

<p><div>Overloaded product queues propagate from product managers to project managers to engineers.</div><br /></p>

<p>Note that the propagation of queue overload can cause both the single engineer and multiple engineers scheduling problems.</p>

<p>Product and engineering management must lead and support changes to the way we work in order to improve flow, especially since some of these practices have become habitual in the company. Accordingly, the following guidelines were given to product managers.</p>

<p><strong>1. The company should commit to fewer products at a time</strong> - Ideally, teams and product leadership should agree on a limit for the number of products that can be in progress for each team.</p>

<p><strong>2. Teams should work on fewer product features at a time</strong> - Product managers should prioritize one product at time, one feature at a time and encourage the teams to deliver likewise if possible.</p>

<p>The following guidelines were given to engineering managers.</p>

<p><strong>2. Teams should work on fewer product features at a time</strong> - Engineering managers should be conscious of pulling disparate product features onto the board at the same time. They should encourage the team to focus on a limited number of features at once, ideally one. They should encourage dividing work for a product feature into subtasks that can be shared in the team to enable that focus.</p>

<p><strong>3. Engineers should work on fewer tasks at a time</strong> - Engineering managers should educate their teams about flow management and encourage them to focus on a task until it’s done, if possible.</p>

<p><strong>4. Engineering should reduce its code inventory</strong> - Engineering managers should monitor their team’s code inventory and encourage them to keep it low.</p>

<p><strong>5. Engineering should monitor and reduce the number of bugs over time</strong> - Engineering managers should monitor their team’s backlog of bugs and schedule them to be fixed.</p>

<h3>Guidelines for teams</h3>

<p>Teams were given the following guidelines.</p>

<p><strong>3. Engineers should work on fewer tasks at a time</strong> - Engineers should learn the basics of flow management and try to focus on one feature at a time if possible.</p>

<p><strong>4. Engineering should reduce its code inventory</strong> - Engineers should review code promptly once it hits the review stage and alert others when work is blocked.</p>

<p><strong>5. Engineering should monitor and reduce the number of bugs over time</strong> - Engineers should fix bugs!</p>

<h3>A tool for improving flow</h3>

<p><a href="https://en.wikipedia.org/wiki/Visual_control">Simple, visual controls</a> are provided as part of PDKP to help project managers improve their product development flow. These are presented on the PDKP Summary Dashboard, shown below.</p>

<p></p>

<p><div>The PDKP Summary Dashboard.</div><br /></p>

<p>The metrics below are displayed on the board.</p>

<ul>
<li>Lead time</li>
<li>Cycle time</li>
<li>Ratio of features to engineers</li>
</ul>

<p>A second set of metrics, shown below, is paired with colors to suggest specific actions. Green means the team’s performance against the metric is good, yellow means performance against the metric is likely OK, red means the metric should be brought down to promote flow.</p>

<ul>
<li>The estimated lower bound of days until backlog completion</li>
<li>The number of unresolved bugs in the team’s backlog</li>
<li>The number of products in WIP</li>
<li>The ratio of WIP to engineers</li>
<li>The number of commits in open PRs</li>
<li>The average age of code in open PRs</li>
</ul>

<p>The motivations for optimizing these metrics was discussed in a previous section.</p>

<p>The actions suggested by the action colors on the Summary Dashboard will not always be appropriate in your day-to-day work. Sometimes they will be wrong. However, simply asking questions about your workflow when a metric lights up red will likely improve your product development flow.</p>

<p><em>Importantly, actions never suggest that a team isn't working hard enough. They are intended to increase focus, not the total number of hours worked.</em></p>

Preliminary results

<p>Since the addition of the last teams to the PDKP project on June 23rd, 2017, SoundCloud has seen a reported 39% decrease in the average lead time from a mean of 106.4 days to 64.7 days and a 52% decrease in average cycle time from a mean of 77.8 days to 37.2 days.</p>

<p></p>

<p><div>Aggregate lead time metrics since June 23rd, 2017.</div><br /></p>

<p></p>

<p><div>Aggregate cycle time metrics since June 23rd, 2017.</div><br /></p>

<p>This decrease is correlated with product and engineering leadership successfully decreasing the average number of products SoundCloud’s teams have in WIP from an average of 7.4 products per team to 3.1 products per team.</p>

<p></p>

<p><div>Aggregate metrics of the number of products in WIP decreasing since June 23rd, 2017.</div><br /></p>

<p>More recently, engineering managers and engineering teams successfully reduced our code inventory from a rolling average of about 1,200 commits to about 500 commits. It is likely that we have not yet seen the full benefits of this reduction reflected in our lead and cycle times.</p>

<p></p>

<p><div>A recent decrease in code inventory.</div><br /></p>

<p>It is possible that much of the improvement in our measured lead and cycle times is due to cleanup and better use of our Jira. Tickets in Jira now better represent what is actually happening in reality. If this is true, at least we are getting a better understanding of what the true numbers are.</p>

<h3>Future improvements</h3>

<p>SoundCloud still has a great deal of room left to improve its product development flow. Specifically, the number of products teams work on at a time, the number of issues engineers work on at a time, and its amount of code inventory needs to be further reduced.</p>

<p>The number of products teams are working on at a time has successfully been reduced, but should be reduced further. Teams still work on an average of 4.5 products at a time. While it does not always make sense for a whole team to work on one product simultaneously, having the average be just slightly below the average number of engineers per team (about five), suggests there is room for improvement.</p>

<p>The number of tasks that engineers have in WIP has not improved and should be reduced. Since June 23rd, 2017, it has remained at an average of about 2.0 per engineer. Bringing this number closer to 1.0 would likely decrease our lead and cycle times.</p>

<p>SoundCloud’s amount of code inventory also remains a problem. Further reducing this number represents another big opportunity to reduce our lead and cycle times.</p>

<p>This said, we’ve improved our product development flow a great deal. We're excited to learn more about our development cycle and to make future improvements.</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://stratechery.com/2017/defining-aggregators/">Defining Aggregators</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="http://stratechery.com/feed/">Stratechery</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">business</span>
              <span class="tag">economics</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Tue Sep 26 2017 15:27:17 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>(<em>Note: this is not a typical Stratechery article; there is no over-arching narrative or reference to current news. Rather, the primary goal is to provide a future point of reference</em>)</p>
<p>Aggregation Theory describes how platforms (i.e. aggregators) come to dominate the industries in which they compete in a systematic and predictable way. Aggregation Theory should serve as a guidebook for aspiring platform companies, a warning for industries predicated on controlling distribution, and a primer for regulators addressing the inevitable antitrust concerns that are the endgame of Aggregation Theory.</p>
<p>Aggregation Theory was first coined in <a href="https://stratechery.com/2015/aggregation-theory/">this eponymously-titled 2015 article</a>. That article followed on the heels of a series of posts about <a href="https://stratechery.com/2015/airbnb-and-the-internet-revolution/">Airbnb</a>, <a href="https://stratechery.com/2015/netflix-and-the-conservation-of-attractive-profits/">Netflix</a>, and <a href="https://stratechery.com/2015/why-web-pages-suck/">web publishing</a> that, I realized, fit together into a broader framework that was applicable to a range of Internet-enabled companies. Over the ensuing two years I have significantly fleshed out the ideas in that original article, yet subsequent articles necessarily link to an article that marked the beginning of Aggregation Theory, not the current state.</p>
<p>That noted, <a href="https://stratechery.com/2015/aggregation-theory/">the original article</a> is very much worth reading, particularly its description of how value has shifted away from companies that control the distribution of scarce resources to those that control demand for abundant ones; the purpose of this article is to catalog exactly what the latter look like.</p>
<h4>The Characteristics of Aggregators</h4>
<p>Aggregators have all three of the following characteristics; the absence of any one of them can result in a very successful business (in the case of Apple, arguably the most successful business in history), but it means said company is not an aggregator.</p>
<p><strong>Direct Relationship with Users</strong></p>
<p>This point is straight-forward, yet the linchpin on which everything else rests: aggregators have a direct relationship with users. This may be a payment-based relationship, an account-based one, or simply one based on regular usage (think Google and non-logged in users).</p>
<p><strong>Zero Marginal Costs For Serving Users</strong></p>
<p>Companies traditionally have had to incur (up to) three types of marginal costs when it comes to serving users/customers directly.</p>
<ul>
<li>The cost of goods sold (COGS), that is, the cost of producing an item or providing a service</li>
<li>Distribution costs, that is the cost of getting an item to the customer (usually via retail) or facilitating the provision of a service (usually via real estate)</li>
<li>Transaction costs, that is the cost of executing a transaction for a good or service, providing customer service, etc.</li>
</ul>
<p>Aggregators incur none of these costs:</p>
<ul>
<li>The goods “sold” by an aggregator are digital and thus have zero marginal costs (they may, of course, have significant fixed costs)<a href="https://stratechery.com/2017/defining-aggregators/#footnote_0_2759">1</a></li>
<li>These digital goods are delivered via the Internet, which results in zero distribution costs<a href="https://stratechery.com/2017/defining-aggregators/#footnote_1_2759">2</a></li>
<li>Transactions are handled automatically through automatic account management, credit cards payments, etc.<a href="https://stratechery.com/2017/defining-aggregators/#footnote_2_2759">3</a></li>
</ul>
<p>This characteristic means that businesses like Apple hardware and Amazon’s traditional retail operations are not aggregators; both bear significant costs in serving the marginal customer (and, in the case of Amazon in particular, have achieved such scale that the service’s relative cost of distribution is actually a moat).</p>
<p><strong>Demand-driven Multi-sided Networks with Decreasing Acquisition Costs</strong></p>
<p>Because aggregators deal with digital goods, there is an abundance of supply; that means users reap value through discovery and curation, and most aggregators get started by delivering superior discovery.</p>
<p>Then, once an aggregator has gained some number of end users, suppliers will come onto the aggregator’s platform on the aggregator’s terms, effectively commoditizing and modularizing themselves. Those additional suppliers then make the aggregator more attractive to more users, which in turn draws more suppliers, in a virtuous cycle.</p>
<p>This means that for aggregators, customer acquisition costs decrease over time; marginal customers are attracted to the platform by virtue of the increasing number of suppliers. This further means that aggregators enjoy winner-take-all effects: since the value of an aggregator to end users is continually increasing it is exceedingly difficult for competitors to take away users or win new ones.</p>
<p>This is in contrast to non-platform companies that face <em>increasing</em> customer acquisition costs as their user base grows. That is because initial customers are often a perfect product-market fit; however, as that fit decreases, the surplus value from the product decreases as well and quickly turns negative. Generally speaking, any business that creates its customer value in-house is not an aggregator because eventually its customer acquisition costs will limit its growth potential.</p>
<p>One additional note: the aforementioned Apple and Amazon do have businesses that qualify as aggregators, at least to a degree: for Apple, it is the App Store (as well as the Google Play Store). Apple owns the user relationship, incurs zero marginal costs in serving that user, and has a network of App Developers continually improving supply in response to demand. Amazon, meanwhile, has Amazon Merchant Services, which is a two-sided network where Amazon owns the end user and passes all marginal costs to merchants (i.e. suppliers).</p>
<h4>Classifying Aggregators</h4>
<p>Aggregation is fundamentally about owning the user relationship and being able to scale that relationship; that said, there are different levels of aggregation based on the aggregator’s relationship to suppliers:</p>
<p><strong>Level 1 Aggregators: Supply Acquisition</strong></p>
<p>Level 1 Aggregators acquire their supply; their market power springs from their relationship with users, but is primarily manifested through superior buying power. That means these aggregators take longer to build and are more precarious in the short-term.</p>
<p>The best example of a Level 1 Aggregator is Netflix. Netflix owns the user relationship and bears no marginal costs in terms of COGS, distribution costs,<a href="https://stratechery.com/2017/defining-aggregators/#footnote_3_2759">4</a> or transaction costs.<a href="https://stratechery.com/2017/defining-aggregators/#footnote_4_2759">5</a> Moreover, Netflix does not create shows, but it does acquire them (increasingly exclusively to Netflix); the more content Netflix acquires, the more its value grows to potential users. And, the more users Netflix gains, the more it can spend on acquiring content in a virtuous cycle.</p>
<p>Level 1 aggregators typically operate in industries where supply is highly differentiated, and are susceptible to competitors with deeper pockets or orthogonal business models.</p>
<p><strong>Level 2 Aggregators: Supply Transaction Costs</strong></p>
<p>Level 2 Aggregators do not own their supply; however, they do incur transaction costs in bringing suppliers onto their platform. That limits the growth rate of Level 2 aggregators absent the incursion of significant supplier acquisition costs.</p>
<p>Uber is a Level 2 Aggregator (and Airbnb in some jurisdictions due to local regulations). Uber owns the user relationship and bears no marginal costs in terms of COGS, distribution costs, or transaction costs. Moreover, Uber does not own cars; those are supplied by drivers who sign up for the platform directly. At that point, though Uber needs to undertake steps like background checks, vehicle verification, etc. that incur transaction costs both in terms of money as well as time. This limits supply growth which ultimately limits demand growth.</p>
<p>Level 2 aggregators typically operate in industries with significant regulatory concerns that apply to the quality and safety of suppliers.</p>
<p><strong>Level 3 Aggregators: Zero Supply Costs</strong></p>
<p>Level 3 Aggregators do not own their supply and incur no supplier acquisition costs (either in terms of attracting suppliers or on-boarding them).</p>
<p>Google is the prototypical Level 3 Aggregator: suppliers (that is, websites) are not only accessible by Google by default, but in fact actively make themselves more easily searchable and discoverable (indeed, there is an entire industry — search engine optimization (SEO) — that is predicated on suppliers <em>paying</em> to get themselves onto Google more effectively).</p>
<p>Social networks are also Level 3 Aggregators: initial supply is provided by users (who are both users and suppliers); over time, as more and more attention is given to the social networks, professional content creators add their content to the social network for free.</p>
<p>Level 3 aggregators are predicated on massive numbers of users, which means they are usually advertising-based (which means they are free to users). An interesting exception is the aforementioned App Stores: in this case the limited market size (relatively speaking) is made up by the significantly increased revenue-per-customer available to app developers with suitable business models (primarily consumable in-app purchases).</p>
<p><strong>The Super-Aggregators</strong></p>
<p>Super-Aggregators operate multi-sided markets with at least <em>three</em> sides — users, suppliers, and advertisers — and have zero marginal costs on all of them. The only two examples are Facebook and Google, which in addition to attracting users and suppliers for free, also have self-serve advertising models that generate revenue without corresponding variable costs (other social networks like Twitter and Snapchat rely to a much greater degree on sales-force driven ad sales).</p>
<p>For more about Super-Aggregators see <a href="https://stratechery.com/2017/the-super-aggregators-and-the-russians/">this article</a>.</p>
<h4>Regulating Aggregators</h4>
<p>Given the winner-take-all nature of Aggregators, there is, at least in theory, a clear relationship between <a href="https://stratechery.com/2016/antitrust-and-aggregation/">Antitrust and Aggregation</a>. However, traditional jurisprudence is limited by three factors:</p>
<ul>
<li>The key characteristic of Aggregators is that they own the user relationship. Critically, the user chooses this relationship because the aggregator offers a superior service. This makes it difficult to make antitrust arguments based on consumer welfare (the standard for U.S. jurisprudence for the last 35 years).</li>
<li>The nature of digital markets is such that aggregators may be inevitable; traditional regulatory relief, like breaking companies up or limiting their addressable markets will likely result in a new aggregator simply taking their place.</li>
<li>Aggregators make it dramatically simpler and cheaper for suppliers to reach customers (which is why suppliers work so hard to be on their platform). This increases the types of new businesses that can be created by virtue of the aggregators existing (YouTube creators, Amazon merchants, small publications, etc.); regulators should take care to preserve these new opportunities (and even protect them).</li>
</ul>
<p>These are guidelines for regulation; determining specifics is an ongoing project for Stratechery, as are the definitions in this article.</p>
<ol><li>And yes, in the very long run, all fixed costs are marginal costs; that said, while the amount of capital costs for aggregators is massive, their userbase is so large that even over the long run the fixed costs per user are infinitesimal, particularly relative to revenue generated</li><li>In terms of the marginal customer; in aggregate there are of course significant bandwidth costs, but see the previous footnote</li><li>Credit card fees are a significant transaction cost that do limit some types of businesses, but will generally be ignored in this analysis</li><li>Obviously bandwidth in the aggregate is a particularly large cost of Netflix</li><li>In all cases, credit card fees excepted</li></ol>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://stratechery.com/2017/books-and-blogs/">Books and Blogs</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="http://stratechery.com/feed/">Stratechery</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">business</span>
              <span class="tag">economics</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Tue Sep 26 2017 15:20:31 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>A book, at least a successful one, has a great business model: spend a lot of time and effort writing, editing, and revising it up front, and then make money selling as many identical copies as you can. The more you sell the more you profit, because the work has already been done. Of course if you are successful, the pressure is immense to write another; the payoff, though, is usually greater as well: it is much easier to sell to customers you have already sold to before than it is to find customers for the very first time.</p>
<p>There is, though, at least from my perspective, a downside to this model: a book, by necessity, is a finished object; that is why it can be printed and distributed at scale. The problem is that one’s thoughts may not be final; indeed, the more vital the subject, the more likely a book, with its many-month production process, is to be obsolete the moment it enters its final state of permanence.</p>
<p>When I started Stratechery <a href="https://stratechery.com/2013/welcome-to-stratechery/">four years ago</a>, with my 384 Twitter followers and little else, the thought of writing a book never crossed my mind; not only did I not have a contract, I didn’t even have a topic beyond the business and strategy of technology, a niche I thought was both under-served and that I had the inklings of a point of view on.</p>
<p>Since then it has been an incredible journey, especially intellectually: instead of writing with a final goal in mind — a manuscript that can be printed at scale — Stratechery has become in many respects a journal of my own attempts to understand technology specifically and the way in which it is changing every aspect of society broadly. And, it turns, out, the business model is even better: instead of taking on the risk of writing a book with the hope of one-time payment from customers at the end, <a href="https://stratechery.com/membership/">Stratechery subscribers</a> fund that intellectual exploration directly and on an ongoing basis; all they ask is that I send them my journals of said exploration every day in email form.</p>
<p>To put it another way, at least in my experience, the lowly blog has fully disrupted the mighty book: the former was long thought to be an inferior alternative, or at best, a complementary piece for an author looking to drum up an audience; slowly but surely, though, the tools have gotten better, everything from social media for marketing to Stripe for payments to WordPress for publishing to tools like <a href="https://memberful.com">Memberful</a> for subscriber management. It became increasingly apparent, to me anyways, that while books remained a fantastic medium for stories, both fiction and non, blogs were not only good enough, they were actually better for ideas closely tied to a world changing far more quickly than any book-related editorial process can keep up with.</p>
<p>To be sure, I had discovered in 2015 what might have been a worthy book topic: <a href="https://stratechery.com/2015/aggregation-theory/">Aggregation Theory</a>. That, though, makes my point: the biggest problem I have with Aggregation Theory is that that old article I keep linking to is incomplete. My thinking on what Aggregation Theory is, what its implications are, and how that should affect strategy both inside and outside of technology and, particularly over the last year, potential regulation, has evolved considerably.</p>
<p>To that end, it is with relief I write the following article: <a href="https://stratechery.com/2017/defining-aggregators">Defining Aggregators</a>. I’ll be honest: it’s more for me than for you; my thinking has evolved, and clarified, and I want to link to something that represents my point of view in 2017, not just 2015. That I can do so by merely hitting ‘Publish’ is a great thing: these ideas are very much alive, and I don’t really see the point of trees that are dead, literally or virtually.</p>
<p><em>Note: This article is meant as an introduction to <a href="https://stratechery.com/2017/defining-aggregators">Defining Aggregators</a>; it is posted as a separate article as I plan to link to that article many times in the future</em></p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://ponyfoo.com/articles/brief-history-of-modularity">A Brief History of Modularity</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://feeds.feedburner.com/ponyfoo">Pony Foo</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">javascript</span>
              <span class="tag">web</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Tue Sep 26 2017 12:30:18 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <div><p>When it comes to JavaScript, modularity is a modern concept. In this article we’ll quickly revisit and summarize the milestones in how modularity evolved in the world of JavaScript. This isn’t meant to be a comprehensive list, by any means, but instead it’s meant to illustrate the major paradigm changes along the history of JavaScript.</p>Script Tags and Closures <p>In the early days, JavaScript was inlined in HTML <code>&lt;script&gt;</code> tags. At best, it was offloaded to dedicated script files, all of which shared a global scope.</p> <p>Any variables declared in one of these files or inline scripts would be imprinted on the global <code>window</code> object, creating leaks across entirely unrelated scripts that might’ve lead to conflicts or even broken experiences, where a variable in one script might inadvertently replace a global that another script was relying on.</p> <p>Eventually, as web applications started growing in size and complexity, the concept of scoping and the dangers of a global scope became evident and more well-known. Immediately-invoking function expressions (IIFE) were invented and became an instant mainstay. An IIFE worked by wrapping an entire file or portions of a file in a function that executed immediately after evaluation. Each function in JavaScript creates a new level of scoping, meaning <code>var</code> variable bindings would be contained by the IIFE. Even though variable declarations are hoisted to the top of their containing scope, they’d never become implicit globals, thanks to the IIFE wrapper, thus suppressing the brittleness of implicit JavaScript globals.</p> <p>Several flavors of IIFE can be found in the next example snippet. The code in each IIFE is isolated and can only escape onto the global context via explicit statements such as <code>window.fromIIFE = true</code>.</p> <pre><code>(function() {
  console.log('IIFE using parenthesis')
})()

~function() {
  console.log('IIFE using a bitwise operator')
}()

void function() {
  console.log('IIFE using the void operator')
}()
</code></pre> <p>Using the IIFE pattern, libraries would typically create modules by exposing and then reusing a single binding on the <code>window</code> object, thus avoiding global namespace pollution. The next snippet shows how we might create a <code>mathlib</code> component with a <code>sum</code> method in one of these IIFE-based libraries. If we wanted to add more modules to <code>mathlib</code>, we could place each of them in a separate IIFE which adds its own methods to the <code>mathlib</code> public interface, while anything else could stay private to the component that defined the new portion of functionality.</p> <pre><code>void function() {
  window.mathlib = window.mathlib || {}
  window.mathlib.sum = sum

  function sum(...values) {
    return values.reduce((a, b) =&gt; a + b, 0)
  }
}()

mathlib.sum(1, 2, 3)
// &lt;- 6
</code></pre> <p>This pattern was, coincidentally, an open invitation for JavaScript tooling to burgeon, allowing developers – for the first time – to safely concatenate every IIFE module into a single file, reducing the strain on the network.</p> <p>The problem in the IIFE approach was that there wasn’t an explicitly dependency tree. This means developers had to manufacture component file lists in a precise order, so that dependencies would load before any modules that dependend on them did – recursively.</p>RequireJS, AngularJS, and Dependency Injection <p>This is a problem we’ve hardly had to think about ever since the advent of module systems like RequireJS or the dependency injection mechanism in AngularJS, both of which allowed us to explicitly name the dependencies of each module.</p> <p>The following example shows we might define the <code>mathlib/sum.js</code> library using RequireJS’s <code>define</code> function, which was added to the global scope. The returned value from the <code>define</code> callback is then used as the public interface for our module.</p> <pre><code>define(function() {
  return sum

  function sum(...values) {
    return values.reduce((a, b) =&gt; a + b, 0)
  }
})
</code></pre> <p>We could then have a <code>mathlib.js</code> module which aggregates all functionality we wanted to include in our library. In our case, it’s just <code>mathlib/sum</code>, but we could list as many dependencies as we wanted in the same way. We’d list each dependency using their paths in an array, and we’d get their public interfaces as parameters passed into our callback, in the same order.</p> <pre><code>define(['mathlib/sum'], function(sum) {
  return { sum }
})
</code></pre> <p>Now that we’ve defined a library, we can consume it using <code>require</code>. Notice how the dependency chain is resolved for us in the snippet below.</p> <pre><code>require(['mathlib'], function(mathlib) {
  mathlib.sum(1, 2, 3)
  // &lt;- 6
})
</code></pre> <p>This is the upside in RequireJS and its inherent dependency tree. Regardless of whether our application contained a hundred or thousands of modules, RequireJS would resolve the dependency tree without the need for a carefully maintained list. Given we’ve listed dependencies exactly where they were needed, we’ve eliminated the necessity for a long list of every component and how they’re related to one another, as well as the error-prone process of maintaining such a list. Eliminating such a large source of complexity is merely a side-effect, but not the main benefit.</p> <p>This explicitness in dependency declaration, at a module level, made it obvious how a component was related to other parts of the application. That explicitness in turn fostered a greater degree of modularity, something that was ineffective before because of how hard it was to follow dependency chains.</p> <p>RequireJS wasn’t without problems. The entire pattern revolved around its ability to asynchronously load modules, which was ill-advised for production deployments due to how poorly it performed. Using the asynchronous loading mechanism, you issued hundreds of networks requests in a waterfall fashion before much of your code was executed. A different tool would have to be used to optimize builds for production. Then there was the verbosity factor, where you’d end up with long lists of dependencies, a RequireJS function call, and the callback for your module. On that note, there were quite a few different RequireJS functions and several ways of invoking those functions, complicating its use. The API wasn’t the most intuitive, because there were so many ways of doing the same thing: declaring a module with dependencies.</p> <p>The dependency injection system in AngularJS suffered from many of the same problems. It was an elegant solution at the time, relying on clever string parsing to avoid the dependency array, using function parameter names to resolve dependencies instead. This mechanism was incompatible with minifiers, which would rename parameters to single characters and thus break the injector.</p> <p>Later in the lifetime of AngularJS v1, a build task was introduced that would transform code like the following:</p> <pre><code>module.factory('calculator', function(mathlib) {
  // …
})
</code></pre> <p>Into the format in the following bit of code, which was minification-safe because it included the explicit dependency list.</p> <pre><code>module.factory('calculator', ['mathlib', function(mathlib) {
  // …
}])
</code></pre> <p>Needless to say, the delay in introducing this little-known build tool, combined with the over-engineered aspect of having an extra build step to unbreak something that shouldn’t have been broken, discouraged the use of a pattern that carried such a negligible benefit anyway. Developers mostly chose to stick with the familiar RequireJS-like hardcoded dependency array format.</p> Node.js and the Advent of CommonJS <p>Among the many innovations hailed by Node.js, one was the CommonJS module system – or CJS for short. Taking advantage of the fact that Node.js programs had access to the file system, the CommonJS standard is more in line with traditional module loading mechanisms. In CommonJS, each file is a module with its own scope and context. Dependencies are loaded using a synchronous <code>require</code> function that can be dynamically invoked at any time in the lifecycle of a module, as illustrated in the next snippet.</p> <pre><code>const mathlib = require('./mathlib')
</code></pre> <p>Much like RequireJS and AngularJS, CommonJS dependencies are also referred to by a pathname. The main difference is that the boilerplate function and dependency array are now both gone, and the interface from a module could be assigned to a variable binding, or used anywhere a JavaScript expression could be used.</p> <p>Unlike RequireJS or AngularJS, CommonJS was rather strict. In RequireJS and AngularJS you could have many dynamically-defined modules per file, whereas CommonJS had a one-to-one mapping between files and modules. At the same time, RequireJS had several ways of declaring a module and AngularJS had several kinds of factories, services, providers and so on – besides the fact that its dependency injection mechanism was tightly coupled to the AngularJS framework itself. CommonJS, in contrast, had a single way of declaring modules. Any JavaScript file was a module, calling <code>require</code> would load dependencies, and anything assigned to <code>module.exports</code> was its interface. This enabled better tooling and code introspection – making it easier for tools to learn the hierarchy of a CommonJS component system.</p> <p>Eventually, Browserify was invented as way of bridging the gap between CommonJS modules for Node.js servers and the browser. Using the <code>browserify</code> command-line interface program and providing it with the path to an entry point module, one could combine an unthinkable amount of modules into a single browser-ready bundle. The killer feature of CommonJS, the npm package registry, was decisive in aiding its takeover of the module loading ecosystem.</p> <p>Granted, npm wasn’t limited to CommonJS modules or even JavaScript packages, but that was and still is by and large its primary use case. The prospect of having thousands of packages (now over half million and steadily growing) available in your web application at the press of a few fingertips, combined with the ability to reuse large portions of a system on both the Node.js web server and each client’s web browser, was too much of a competitive advantage for the other systems to keep up.</p> ES6, <code>import</code>, Babel, and Webpack <p>As ES6 became standardized in June of 2015, and with Babel transpiling ES6 into ES5 long before then, a new revolution was quickly approaching. The ES6 specification included a module system native to JavaScript, often referred to as ECMAScript Modules (ESM).</p> <p>ESM is largely influenced by CJS and its predecessors, offering a static declarative API as well as a promise-based dynamic programmative API, as illustrated next.</p> <pre><code>import mathlib from './mathlib'
import('./mathlib').then(mathlib =&gt; {
  // …
})
</code></pre> <p>In ESM, too, every file is a module with its own scope and context. One major advantage in ESM over CJS is how ESM has – and encourages – a way of statically importing dependencies. Static imports vastly improve the introspection capabilities of module systems, given they can be analyzed statically and lexically extracted from the abstract syntax tree (AST) of each module in the system. Static imports in ESM are constrained to the topmost level of a module, further simplifying parsing and introspection.</p> <p>In Node.js v8.5.0, ESM module support was introduced behind a flag. Most evergreen browsers also support ESM modules behind flags.</p> <p>Webpack is a successor to Browserify that largely took over in the role of universal module bundler thanks to a broader set of features. Just like in the case of Babel and ES6, Webpack has long supported ESM with both its <code>import</code> and <code>export</code> statements as well as the dynamic <code>import()</code> function. It has made a particularly fruitful adoption of ESM, in no little parts thanks to the introduction of a “code-splitting” mechanism whereby it’s able to partition an application into different bundles to improve performance on first load experiences.</p> <p>Given how ESM is native to the language, – as opposed to CJS – it can be expected to completely overtake the module ecosystem in a few years time.</p></div>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://ronjeffries.com/articles/017-08ff/troubling-times/">Troubling Times</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="http://ronjeffries.com/feed.xml">Ron Jeffries</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">practices</span>
              <span class="tag">agile</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Tue Sep 26 2017 05:00:00 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            Some random thoughts on the times we are living through.
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://eklitzke.org/an-unexpected-python-abi-change">An Unexpected Python ABI Change</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://eklitzke.org/atom.xml?type=blog">Evan Klitzke</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">compsci</span>
              <span class="tag">unix</span>
              <span class="tag">python</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Tue Sep 26 2017 01:00:00 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            Earlier this year, a Pyflame user filed a <a href="https://github.com/uber/pyflame/issues/69">GitHub
issue</a> reporting that Pyflame didn’t
work reliably with Python 3.6. Sometimes Pyflame would just print garbage output
(e.g. bogus line numbers), a...
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://feedproxy.google.com/~r/TroyHunt/~3/JO9jZ_2mbRc/">The Ethics of Running a Data Breach Search Service</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://feeds.feedburner.com/TroyHunt">Troy Hunt</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">security</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Mon Sep 25 2017 08:44:35 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p><a href="http://www.symantec.com/ready-secure-go/it-smb/?om_ext_cid=ws_ad_TROY_DBC-Bulldog_IT-SMB_Encrypt"><strong>Presently sponsored by:</strong> Get a security solution that will keep your website up and running—and keep you sleeping soundly: Symantec Website Security. Learn how</a></p><div><p>No matter how much anyone tries to sugar coat it, a service like <a href="https://haveibeenpwned.com/">Have I been pwned</a> (HIBP) which deals with billions of records hacked out of other peoples' systems is always going to sit in a grey area. There are degrees, of course; at one end of the spectrum you have the likes of <a href="https://www.troyhunt.com/random-thoughts-on-the-use-of-breach-data/">Microsoft and Amazon using data breaches to better protect their customers' accounts</a>. At the other end, there's services like <a href="https://www.troyhunt.com/thoughts-on-the-leakedsource-take-down/">the now defunct LeakedSource</a> who happily sold our personal data (including mine) to anyone willing to pay a few bucks for it.</p>
<p>As far as intent goes, HIBP sits at the white end of the scale, as far to that extreme as I can possibly position it. It's one of many things I do in the security space alongside online training, conference talks, in-person workshops and of course writing this blog. All of these activities focus on how we can make security on the web better; increasing awareness, reducing the likelihood of a successful attack and minimising the damage if it happens. HIBP was always intended to amplify those efforts and indeed it has helped me do that enormously.</p>
<p>What I want to talk about here today is why I've made many of the decisions I have regarding the implementation of HIBP. This post hasn't been prompted by any single event, rather it seeks to address questions I regularly see coming up. I want to explain my thinking, explore why I've made many of the decisions I have and invite people to contribute comments with a hope of making it a more useful system for everyone.</p>
The Accessibility of a Publicly Searchable System
<p>The foremost question that comes up as it relates to privacy is &quot;why make the system publicly searchable?&quot; There are both human and technical reasons for this and I want to start with the former.</p>
<p>Returning an immediate answer to someone who literally asks the question &quot;have I been pwned?&quot; is enormously powerful. The immediacy of the response addresses a question that's clearly important to them at that very moment and from a user experience perspective, you simply cannot beat it.</p>
<p>The value in the UX of this model has significantly contributed to the growth of the service and as such, the awareness its raised. A great example is when you see someone take another person through the experience: &quot;here, you just enter your email address and... whoa!&quot; The penny suddenly drops that data breaches are a serious thing and thus begins the discussion about password strength and reuse, multi-step verification and other fundamental account management hygiene issues.</p>
<p>The fact that someone can search for someone who is not them is a double-edged sword; the privacy risk is obvious in that you may discover someone was in a particular data breach and then use that information to somehow disadvantage them. However, people also extensively use the service to help protect other people, for example by identifying exposed spouses, friends, relatives or even customers and advising them accordingly.</p>
<p>I heard a perfect example of this just the other day when speaking to a security bod in a bank. He explained how HIBP was used when communicating with customers who'd suffered an account takeover. By highlighting that they'd appeared in a breach such as LinkedIn, they are able to help the customer understand the potential source of the compromise. Without being able to publicly locate that customer in HIBP, it would be a much less feasible proposition for the bank.</p>
<p>I mitigate the risk of public discoverability adversely impacting someone by flagging certain breaches as &quot;sensitive&quot; and excluding them from publicly visible results. <a href="https://www.troyhunt.com/heres-how-im-going-to-handle-ashley/">This concept came in when the Ashley Madison data hit</a> and the only way to see if you're in that data breach (or any other that poses a higher risk of disadvantaging someone) is to receive an email to the searched address and click on a unique link (I'll come back to why I don't do that for all searches in a moment).</p>
<p>I've actually had many people suggest that it's ok to show the sensitive results I'm presently returning privately because the privacy of these individuals has already been compromised due to the original breach. I don't like this argument and the main reason is because I don't believe the act of someone else having illegally broken into another system means the victims of that breach should be exposed further in ways that would likely disadvantage them. It's not the only time I've heard this, for example after launching <a href="https://haveibeenpwned.com/Passwords">Pwned Passwords</a> last month a number of people said &quot;you should just return email address and password pairs because their data is out there anyway&quot;. Shortly after that, I was told I'm &quot;holding people hostage&quot; by not providing the passwords for compromised email addresses. In fact, I had someone get quite irate about that after loading <a href="https://www.troyhunt.com/inside-the-massive-711-million-record-onliner-spambot-dump/">the Onliner Spambot data</a> with the bloke in question then proceeding to claim that not disclosing it wasn't protecting anyone. Someone else suggested I was &quot;too old fashioned and diplomatic&quot;. No! These all present a significantly greater risk for those individuals and for someone who himself is in HIBP a dozen times now, I'd be pretty upset if I saw any of this happening.</p>
Searching by Email Verification is Fraught with Problems
<p>This is the alternative I most frequently hear – &quot;just email the results&quot;. There are many reasons why this is problematic and I've already touched on the first above: the UX is terrible. There's no immediate response and instead you're stuck waiting for an email to arrive. Now you may argue that a short wait is worth the trade-off, but there's much more to it than that.</p>
<p>HIBP gets shared and used constructively in all sorts of environments that depend on an immediate response. For example, it gets a huge amount of press and a search is regularly shown in news pieces. Many people (particularly in the infosec community) use it at conference talks and they're not about to go opening up their personal email to show a result.</p>
<p>But those are arguments in favour of accessibility and I appreciate not everyone will agree with them so let's move onto hard technical challenges and the first is delivering email. It's very hard. In all honesty, the single most difficult (and sometimes the single most expensive) part of running this service is delivering mail and doing it reliably. Let's start there actually - here's <a href="https://sendgrid.com/pricing/">the cost of sending 700k emails via SendGrid</a>:</p>
<p></p>
<p>Now fortunately, SendGrid helps support the project so I don't end up wearing that cost but you can see the problem. Let's just put the challenge of sending an email on every search in context for a moment: a few weeks ago, I had 2.8 million unique visitors in just one day after making the aforementioned 711 million record Onliner Spambot dump searchable. Each one of those people did <em>at least</em> one search and if I was to pay for that volume, here's what I'd be looking at:</p>
<p></p>
<p>That's one day of traffic. I can't run a free service that way and I hate to think of the discussion I'd be having with my wife if I did! Now that was one <em>exceptional</em> day but even in low periods I'm still talking about many millions of visitors a month. As it is, I'm coming very close to maxing out my email allocation each month just from sending verification emails and notifications when I load breaches. And no, a cheaper service like <a href="https://aws.amazon.com/ses/">Amazon SES</a> is not a viable alternative, I've been down that path before and it was a debacle for many reasons plus would still get very pricey. (Incidentally, large volumes of emails in a spike often causes delivery to be throttled which would further compound the UX problem of people waiting for a search result to land.)</p>
<p>And then there's the deliverability problems. One of the single hardest challenges I have is reliably getting mail through to people's inbox. <a href="http://www.mail-tester.com/web-n5q5m">Here's what my mail setup looks like in terms of spam friendliness</a>:</p>
<p><a href="http://www.mail-tester.com/web-n5q5m"></a></p>
<p>DKIM is good. SPF is good. I have a dedicated IP. I'm not on any black lists. Everything checks out fine yet consistently, I hear people say &quot;your notification went to junk&quot;. I suspect it's due to the abnormal sending patterns of HIBP, namely that when I load a breach there's a sudden massive spike of emails sent but even then, it's only ever to HIBP subscribers who've successfully double-opted-in. So, think of what that would mean in terms of using email as the sole channel for sharing breach exposure: a heap of people are simply going to miss out. They won't know they were exposed in a breach, they won't adapt their behaviour and for them, HIBP becomes useless.</p>
<p>I've seen criticism from other services attempting to do similar things to HIBP based on the fact I'm not just sending emails to answer that &quot;have I been pwned?&quot; question. But they're at a very different stage of maturity and popularity and simply don't have these challenges – it's be a lot easier if I was only sending hundreds of emails a day and not tens or sometimes even hundreds of thousands. They're also often well-funded and commercialise their visitors so you can see why they may not understand the unique challenges I face with HIBP.</p>
<p>In short, this is the best possible middle ground I can find. Not everyone agrees with it, but I hope that even the folks who don't can see it's a reasoned, well thought out conclusion.</p>
Because I Don't Want Your Email Address
<p>There are a number of different services out there which offer the ability to identify various places your data has been spread across the web. It's a similar deal to HIBP insofar as you enter an email address to begin the search, but many then promise to &quot;get back to you&quot; with results. Of course, during this time, they retain your address. How long do they retain it for? Well...</p>
<p>Someone directed me to <a href="https://www.experian.com/consumer-products/free-dark-web-email-scan.html?cc=van_tvr_scan&amp;intcmp=scan-i">Experian's &quot;Dark Web Email Scan&quot; service</a> just recently. I had the feeling just from reading the front page that there was more going on than meets the eye so I took a look at the policies they link to and that (in theory) you must read and agree to before proceeding:</p>
<blockquote><p>It's stunts like <a href="https://twitter.com/Experian">@Experian</a> is pulling that erode trust in these companies: that's 21,494 words you need to agree to: <a href="https://t.co/ulsAKGv6D8">https://t.co/ulsAKGv6D8</a> <a href="https://t.co/k6GPAGZqPu">pic.twitter.com/k6GPAGZqPu</a></p>— Troy Hunt (@troyhunt) <a href="https://twitter.com/troyhunt/status/906386996091686912">September 9, 2017</a></blockquote>

<p>Folks who've <em>actually read all this</em> have subsequently pointed out that as expected, providing your address in this way now opts you into all sorts of things you really don't want. In fact, I saw it myself first hand:</p>
<blockquote><p>Geez the Experian &quot;dark web search&quot; is terrible: several days to get a result, useless info in the report and 2 subsequent spam mails since <a href="https://t.co/ccFEbZStxq">pic.twitter.com/ccFEbZStxq</a></p>— Troy Hunt (@troyhunt) <a href="https://twitter.com/troyhunt/status/908070197436702720">September 13, 2017</a></blockquote>

<p>In other words, the service is a marketing funnel. The premise of &quot;just leave us with your address and we'll get back to you&quot; is often a thin disguise to build up a list of potential customers. Part of the beauty of HIBP returning results immediately is that the searched address never goes into a database. The only time this happens is when the user explicitly opts in to the notification service in which case I obviously need the address in order to contact them later should they appear in a new data breach. It's data minimisation to the fullest extent I can; I don't want anything I don't absolutely, positively need.</p>
<p>Incidentally, by Experian not explicitly identifying the site the breach occurred on it makes it <em>extremely</em> difficult for people to actually action the report. They're not the only ones - I've seen other services do this too - and it leaves the user thinking &quot;what the hell do I do now?!&quot; I know this because it's precisely the feedback I had after loading the Onliner Spambot data I mentioned earlier, the difference being that I simply didn't know with any degree of confidence where that data originated from. But when I do, I tell people - it's just the right thing to do.</p>
The API Is an Important Part of the Ecosystem
<p>One of the best things I did very early on in terms of making the service accessible to a broad range of people was to <a href="https://haveibeenpwned.com/API/v2">publish an API</a>. In additional to that, <a href="https://haveibeenpwned.com/API/Consumers">I list a number of the consumers of the service</a> and they've done some great things done with it. There are many other very good use cases you won't see publicly listed and that I can't talk about here, but you can imagine the types of positive implementations ingenious people have come up with.</p>
<p>In many cases, the API has enabled people to do great things for awareness. For example, this implementation at a user group I spoke at in the Netherlands recently (and yes, opt-in was optional):</p>
<blockquote><p>So <a href="https://twitter.com/Ordina">@ordina</a> set up facial recognition via photos uploaded at registration &amp; checked people against <a href="https://twitter.com/haveibeenpwned">@haveibeenpwned</a> on arrival  <a href="https://twitter.com/hashtag/TroyAtOrdina?src=hash">#TroyAtOrdina</a> <a href="https://t.co/3C1gFA4Q4t">pic.twitter.com/3C1gFA4Q4t</a></p>— Troy Hunt (@troyhunt) <a href="https://twitter.com/troyhunt/status/882634045129191424">July 5, 2017</a></blockquote>

<p>The very nature of having an API that can search breaches in the fashion means the data has to be publicly searchable. Even if I put API keys on the thing, I'd then have the challenge of working out who I can issue them to then policing their use of the service. For all the reasons APIs make sense for other software projects, they make sense for HIBP.</p>
<p>Now, having said all that, the API has had to evolve over time. Last year <a href="https://www.troyhunt.com/the-have-i-been-pwned-api-rate-limiting-and-commercial-use/">I introduced a rate limit</a> after seeing usage patterns that were not in keeping with ethical use of the service. As a result, one IP can now only make a request every 1.5 seconds and anything over that is blocked. Keep it up and <a href="https://www.troyhunt.com/azure-functions-in-practice/">the IP is presented with a JavaScript challenge at Cloudflare for 24 hours</a>. Yes, you can still run a lot of searches but instead of 40k a minute as I was often seeing from a single IP, we're down to 40. In other words, the worst-case scenario is only one one-thousandth of what it previously was. What that's done is forced those seeking to abuse the system to seek the data out from other places as the effectiveness of using HIBP has plummeted.</p>
<p>Like many of the decisions I've made to protect individuals who end up in HIBP, this one has also garnered me criticism. Very often I feel like I'm damned if I do and I'm damned if I don't; some people were unhappy in this case because it made some of the things they used to do suddenly infeasible. Yes, it slashed malicious use but you can also see how it could impact legitimate use of the API too. I'm never going to be able to make everyone happy with these decisions, I just have to do my best and continue trying to strike the right balance.</p>
I'm Still Adamant About Not Sharing Passwords Attached to Email Addresses
<p>A perfect example of where I simply don't see eye to eye with some folks is sharing passwords attached to email addresses. I've maintained since day 1 that <a href="https://www.troyhunt.com/here-are-all-the-reasons-i-dont-make-passwords-available-via-have-i-been-pwned/">this poses many risks</a> and indeed there are many logistical problems with actually doing this, not least of which is the increasing use of stronger hashing algorithms in the source data breaches.</p>
<p>Not everyone has the same tolerance to risk in this regard. I mentioned earlier how some especially shady services will provide your personal data to anyone else willing to pay; passwords, birth dates, sexualities - it's all up for grabs. Others will email either the full password or a masked portion of it, both of which significantly increase the risk to the owner of that password should that email be obtained by a nefarious party.</p>
<p>I've tried to tackle the gap between providing a full set of credentials and only the email address by <a href="https://www.troyhunt.com/introducing-306-million-freely-downloadable-pwned-passwords/">launching the Pwned Passwords service last month</a>. Whilst the primary motivation here was to provide organisations with a means of identifying at-risk passwords during signup, it also helps individuals directly impacted by data breaches; find both your email address and a password you've used before on HIBP and that's a pretty solid sign you want to revisit your account management hygiene.</p>
<p>At the end of the day, no matter how well I was to implement a solution that attached email addresses to other classes of personal data, there's simply no arguing with the basic premise of I cannot lose what I do not have. I have to feel comfortable with the balance I strike in terms of how I handle this data and at present, that means not putting it online.</p>
There Are Still a Lot of Personal Judgement Calls
<p>I've been asked a few times now what the process for flagging a breach as sensitive is and the answer is simply this: I make a personal judgement call. I have to look at the nature of the service and question what the impact would be if HIBP was used as a vector to discover if someone has an account on that site. I don't always get this right; I didn't originally flag <a href="https://haveibeenpwned.com/PwnedWebsites#FurAffinity">the Fur Affinity breach</a> as sensitive because I didn't understand how furries can be perceive until someone explained it to me. (For the curious, it's <a href="https://en.wikipedia.org/wiki/Furry_fandom#Sexual_aspects">the sexual aspects of furries</a> that came as news to me.)</p>
<p>HIBP is a constant series of judgement calls when it comes to the ethics of running the service. The data I should and should not load is another example. I didn't load <a href="https://www.troyhunt.com/the-red-cross-blood-service-australias-largest-ever-leak-of-personal-data/">the Australian Red Cross Blood Service breach</a> because we managed to clean up all known copies of it (there are multiple reasons why I'm confident in that statement) and they committed to promptly notifying all impacted parties which they summarily did. <a href="https://www.troyhunt.com/have-i-been-pwned-opting-out-vtech-and/">I removed the VTech data breach</a> because it gave parents peace of mind that data relating to kids was removed from all known locations. In both those cases, it was a judgement call made entirely of my own free volition; there were no threats of any kind, it was just the right thing to do.</p>
<p>HIBP is not about trying to maximise the data in the system, it's about helping people and organisations deal with serious criminal acts. Frankly, the best possible outcome would be for there to be no more breaches to load. This is what all my courses, workshops, conference talks and indeed hundreds of blog posts are trying to drive us towards – fixing the problems that have led to data breach search services being a thing in the first place. Not everyone has those same motives though, and that's leading us to some pretty shady practices.</p>
The &quot;No Shady Practices&quot; Rule
<p>As I said in the intro, there's no sugar-coating the fact that handling data breaches is always going to sit in a grey area. This makes it enormously important that every possible measure is taken to avoid any behaviour whatsoever that could be construed as shady. It probably shouldn't surprise anyone, but this is not a broadly held belief amongst those dealing with this class of data.</p>
<p>I mentioned LeakedSource earlier on; there are still multiple sites following the same business model of &quot;give us a few bucks and we'll give you other people's data&quot;. There's a total disregard not just for the privacy of people like you and I, but for the impact it can then have on our lives. People bought access to my own data – I know this because someone once sent it to me! Many of these services operate with impunity under the assumption that they're anonymous; great lengths are gone to in order to obfuscate and shield the identity of the operators although as we saw with Leaked Source, <a href="https://krebsonsecurity.com/2017/02/who-ran-leakedsource-com/">anonymity can be fleeting</a>.</p>
<p>There are also multiple organisations paying for data breaches. What this leads to is criminal incentivisation; rewarding someone for breaking into a system and pilfering the data in no way improves the very problem these services set out to address. Mind you, the argument could be made that the purpose these services primarily serve is to be profitable and viewed in that light, paying for data and then charging for access to it probably makes sense from an ROI perspective. I've never paid for data and I never intend to and yes, that means that it sometimes takes longer for it to appear on HIBP, but it's the right thing to do.</p>
<p>Ambulance chasing is another behaviour that's well and truly into the dark end of shady. I recently had a bunch of people contact me after an organisation emailed them to advise that addresses from their company were found in a breach. Then I watched just last month as someone representing another org hijacked Twitter threads mentioning HIBP in order to promote their own service (I then had to explain what was wrong with this practice, something <a href="https://twitter.com/troyhunt/status/905579801947160576?refsrc=email&amp;s=11">I later highlighted in another thread</a>). In all these cases, financial incentive either from directly monetising the service itself or indirectly promoting other services associated to the organisation appear to be the driver for shady practices.</p>
<p>We should all be beyond reproach when handling this data.</p>
Summary
<p>Being completely honest, it would have to be less than one in one thousand pieces of feedback I get that are critical or even the least bit concerned about the HIBP model as it stands today. It's a very rare thing and that may make you wonder why I even bothered writing this in the first place, but the truth is that it helped me get a few things straight in my own head whilst also providing a reference point for those who <em>do</em> express genuine concern.</p>
<p>HIBP remains a service that first and foremost serves to further ethical objectives. This primarily means raising awareness of the impact data breaches are having and helping those of us that have been stung by them to recover from the event. Even as I've built out commercial services for organisations that have requested them, you won't find a single reference to this on this site; there's no &quot;products&quot; or &quot;pricing&quot;, no up-sell, no financial model for consumers, no withholding of information in an attempt to commercialise it, no shitty terms and conditions that you have to read before searching and not even any advertising or sponsorship. All of this is simply because I don't want <em>anything</em> detracting from that original objective I set forth.</p>
<p>I'll close this post out by saying that there will almost certainly be changes to this in the future. Indeed, it's constantly changed already; sensitive breaches, rate limits and <a href="https://www.troyhunt.com/pastes-on-have-i-been-pwned-are-no-longer-publicly-listed/">the removal of the pastes listing</a> are all examples of where I've stepped back, looked at the system and thought &quot;this needs to be done better&quot;. Very often, that decision has come from community feedback and I'd like to welcome more of that in the comments below. Thank you for reading.</p>
</div>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://www.nngroup.com/articles/presenting-remotely/">5 Strategies for Presenting UX Remotely</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://www.nngroup.com/feed/rss/">Nielsen Norman Group</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">ux</span>
              <span class="tag">company</span>
            </div>
            <span class="subtitle is-7">Sun Sep 24 2017 17:00:00 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p><strong>Summary:</strong> Master remote presentations by creating the right environment, being human, reducing distractions, taking control, and telling a story.</p><hr /><br /><p> There’s nothing like being face-to-face when giving an important presentation. You naturally take cues from the others in the room: eye contact, body language, and facial expressions.</p><p> However, presenting in person isn’t always an option. 80% of corporate presentations are being delivered remotely. In these cases, making a virtual connection with your audience is key, especially when presenting UX designs. The most articulate person always wins; the more successfully we communicate, the higher the chance that our designs become reality. Use the tips below to resonate with your audience, even while presenting remotely.</p> 1. Create a Presentation-Ready Environment<p> Nothing screams ‘webinar’ like a pixelated image of someone’s face and poor audio quality. Break out of the mold and use what you can to create a presentation-like atmosphere.</p><br /><br /><a href="/articles/presenting-remotely/">Read Full Article</a>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://www.nngroup.com/articles/agile-not-easy-ux/">Agile Is not Easy for UX: (How to) Deal with It</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://www.nngroup.com/feed/rss/">Nielsen Norman Group</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">ux</span>
              <span class="tag">company</span>
            </div>
            <span class="subtitle is-7">Sun Sep 24 2017 17:00:00 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p><strong>Summary:</strong> Agile and UX work well together when management values UX, UX practitioners show leadership, the process isn’t strict, and UX is embedded on teams.</p><hr /><br /> Introduction<p> Agile has taken over the software-development world. In recent years, it’s become <a href="https://www.nngroup.com/articles/state-ux-agile-development/">  the most popular software-development methodology </a> . Agile development has a lot of benefits: an incremental approach, the ability to change direction based on customer and stakeholder feedback, short timeframes that keep the teams focused.</p><p> However, Agile methodologies are focused on developers. They grew out of programmers’ attempts to solve common pain points experienced during big software development projects. Notoriously, the <em>  Agile Manifesto </em> (still the primary document delineating Agile principles) did not include UX people, nor did it account for the time, resources, and research that UX professionals need in order to create excellent designs.</p><p> Under an Agile paradigm, the entire team works on the same elements of a project simultaneously in order to avoid “throwing it over the wall” (i.e. hand it off  from one team to another, waterfall-style). The work is done in “sprints” — commonly 2-week periods when the team focuses on certain features, and then moves on. As a result, designers are under enormous pressure to create, test, refine, and deliver their output unrealistically fast, and with little of the context and big-picture thinking that suits consistent, user-centered designs.</p><br /><br /><a href="/articles/agile-not-easy-ux/">Read Full Article</a>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://www.fourmilab.ch/fourmilog/archives/2017-09/001713.html">Floating Point Benchmark: PL&#x2F;I Language Added</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://www.fourmilab.ch/fourmilog/atom_10.xml">Fourmilog - Fourmilab Change Log</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Sun Sep 24 2017 16:06:37 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            I have posted an update to my trigonometry-intense <a href="/fbench/" target="Fourmilog_Aux">floating point benchmark</a> which adds
<a href="https://en.wikipedia.org/wiki/PL/I" target="Fourmilog_Aux">PL/I</a> to the list of languages in which the benchmark is implemented.  A new release of the
<a href="/fbench/" target="Fourmilog_Aux">benchmark collection</a>
including PL/I is now available for downloading.

<p>

I have always had a fondness for PL/I.  Sure, it was an
archetypal product of IBM at the height of the supremacy of Big
Blue, and overreached and never completely achieved its goals,
but it was a product of the 1960s, when technological ambition
imagined unifying programming languages as diverse as Fortran,
COBOL, and Algol into a single language which would serve for
all applications and run on platforms ranging from the most
humble to what passed for supercomputers at the time. It was the
choice for the development of <a href="https://en.wikipedia.org/wiki/Multics" target="Fourmilog_Aux">Multics</a>, one of the most ambitious
operating system projects of all time (and, after its wave of
enthusiasm broke on the shore of reality, inspired Unix), and
the C language inherits much of its syntax and fundamentals from
PL/I.

</p><p>

Few people recall today, but the <a href="http://www.fourmilab.ch/autofile/e5/?chapter=chapter2_14" target="Fourmilog_Aux">first version of AutoCAD</a>
shipped to customers was written in PL/I.  When we were
developing AutoCAD in 1982, we worked in parallel, using Digital
Research's PL/I-80 (a subset of PL/I, but completely adequate
for the needs of AutoCAD) on 8080/Z-80 CP/M systems and C on
8086/8088 machines.  As it happened, AutoCAD-80, the PL/I
version, shipped first, so the first customer who purchased
AutoCAD received a version written in PL/I.

</p><p>

The goal of PL/I was the grand unification of programming
languages, which had bifurcated into a scientific thread
exemplified by Fortran and variants of Algol and a commercial
thread in which COBOL was dominant.  The idea was that a single
language, and a single implementation would serve for all, and
that programmers working in a specific area could learn the
subset applicable to their domain, ignoring features irrelevant
to the tasks they programmed.

</p><p>

By the standards of the time, the language feature set was
ambitious. Variables could be declared in binary or decimal,
fixed point or floating, with variable precision.  Complex
structures and arrays could be defined, including arrays of
structures and structures containing arrays.  A variety of
storage classes were available, allowing the creation of
dynamically or manually allocated storage, and access through
pointers.  Support for recursive procedures and reentrant code
was included.  The language was defined before the structured
programming craze, but its control structures are adequate to
permit structured programming should a programmer choose that
style.  Object orientation was undreamt of at the time, and
support for it was added only much later.

</p><p>

<a href="http://www.iron-spring.com/" target="Fourmilog_Aux">Iron Spring Software</a>
is developing a reasonably complete implementation of PL/I which
runs on Linux and OS/2.  It adheres to ANSI standard X3.74-1987
(ISO/IEC 6522:1992), the so-called “Subset G” of the language,
but, in its present beta releases, does not support all features
of the standard,  The current state of the beta version is
documented in the <a href="http://www.iron-spring.com/prog_guide.html" target="Fourmilog_Aux">Programming Guide</a>.
With one exception (the ASIN mathematical builtin function),
none of the missing features are required by the floating point
benchmark, and ASIN is easily emulated by the identity
(expressed in PL/I)<br />
          asin(x) = atan(x, sqrt(1 − (x * x)))<br />
implemented as a procedure in the program.

</p><p>

The Iron Spring PL/I compiler is closed source, and will be
offered for sale upon general release, but during the beta
period downloads are free.  The runtime libraries (much of which
are written in PL/I) are open source and licensed under the GNU
<a href="https://en.wikipedia.org/wiki/GNU_Lesser_General_Public_License" target="Fourmilog_Aux">Lesser General Public License</a> (LGPL).  There are no restrictions
or licenses required to redistribute programs compiled by the
compiler and linked with its libraries.

</p><p>

This version of the benchmark was developed using the 0.9.9b
beta release of the Iron Spring PL/I compiler.  Current Linux
downloads are 32-bit binaries and produce 32-bit code, but work
on 64-bit systems such as the one on which I tested it.  It is
possible that a native 64-bit version of the compiler and
libraries might outperform 32-bit code run in compatibility
mode, but there is no way at present to test this.

</p><p>

The PL/I implementation of Fbench is a straightforward port
derived from the Ada version of the program, using the same
imperative style of programming and global variables.  All
floating point values are declared as FLOAT BINARY(49), which
maps into IEEE double-precision (64 bit) floating point.  As
noted above, the missing ASIN (arc sine) builtin function was
emulated by an internal procedure named l_asin to avoid conflict
with the builtin on compilers which supply it.

</p><p>

Development of the program was straightforward, and after I
recalled the idioms of a language in which I hadn't written code
for more than thirty years, the program basically worked the
first time.  Support for the PUT STRING facility allowed making
the program self-check its results without any need for user
interaction.  To avoid nonstandard system-dependent features,
the iteration count for the benchmark is compiled into the
program.

</p><p>

After I get the benchmark running and have confirmed that it
produces the correct values, I then time it with an modest
iteration count, then adjust the iteration count to obtain a run
time of around five minutes, which minimises start-up and
termination effects and accurately reflects execution speed of
the heart of the benchmark.  Next, I run the benchmark five
times on an idle system, record the execution times, compute the
mean value, and from that calculate the time in microseconds per
iteration.  This is then compared with the same figure from runs
of the C reference implementation of the benchmark to obtain the
relative speed of the language compared to C.

</p><p>

When I first performed this process, it was immediately apparent
that something had seriously gang agley.  The C version of the
benchmark ran at 1.7858 microseconds per iteration, while the
PL/I implementation took an aching 95.1767
microseconds/iteration—fully 53 times slower!  This was, in its
own way, a breathtaking result.  Most compiled languages come in
between somewhere between the speed of C and four times slower,
with all of the modern heavy-hitter languages (Fortran, Pascal,
Swift, Rust, Java, Haskell, Scala, Ada, and Go) benchmarking no
slower than 1.5 times the C run time. Most interpreted languages
(Perl, Python, Ruby) still benchmark around twice as fast as
this initial PL/I test.  It was time to start digging into the
details.

</p><p>

First of all, I made sure that all of the floating point
variables were properly defined and didn't, for example, declare
variables as fixed decimal.  When I tried this with the <a href="/fourmilog/archives/2012-09/001399.html" target="Fourmilog_Aux">COBOL
version</a> of the benchmark, I indeed got a comparable execution
time (46 times slower than C).  But the variables were declared
correctly, and examination of the assembly language listing of
the code generated by the compiler confirmed that it was
generating proper in-line floating-point instructions instead of
some horror such as calling library routines for floating point
arithmetic.

</p><p>

Next, I instrumented the program to verify that I hadn't
blundered and somehow made it execute more iterations of the
inner loop than were intended.  I hadn't.

</p><p>

This was becoming quite the mystery.  It was time to take a
deeper look under the hood.  I downloaded, built, and installed
<a href="http://oprofile.sourceforge.net/about/" target="Fourmilog_Aux">OProfile</a>,
a hardware-supported statistical profiling tool which, without
support by the language (which is a good thing, because the PL/I
compiler doesn't provide any) allows measurement of the
frequency of instruction execution within a program run under
its supervision. I ran the benchmark for five minutes under
OProfile:<br />
         operf ./fbench<br />
(because the profiling process uses restricted kernel calls,
this must be done as the super-user), and then annotated the
results with:<br />
         opannotate --source --assembly fbench &gt;fbench.prof

</p><p>

The results were flabbergasting.  I was flabber-aghast to
discover that in the entire five minute run, only 5.6% of the
time was spent in my benchmark program: all the rest was spent
in system libraries!  Looking closer, one of the largest time
sinks was the PL/I EXP builtin function, which was distinctly
odd, since the program never calls this function.  I looked at
the generated assembly code and discovered, however, that if you
use the normal PL/I or Fortran idiom of “x ** 2” to square a
value, rather than detecting the constant integer exponent and
compiling the equivalent code of “x * x”, the compiler was
generating a call to the general exponential function, able to
handle arbitrary floating point exponents.  I rewrote the three
instances in the program where the “**” operator appeared to use
multiplication, and when I re-ran the benchmark it was almost
ten times faster (9.4 times to be precise)!

</p><p>

Examination of the Oprofile output from this version showed that
44% of the time was spent in the benchmark, compared to 5.6%
before, with the rest divided mostly among the mathematical
library builtin functions used by the program.  Many of the
library functions which chewed up time in the original version
were consequences of the calls on EXP and melted away when it
was replaced by multiplication.  Further experiments showed that
these library functions, most written in PL/I, were not
particularly efficient: replacing the builtin ATAN function with
my own implementation ported from the INTRIG version of the C
benchmark sped up the benchmark by another 6%.  But the goal of
the benchmark is to test the compiler and its libraries as
supplied by the vendor, not to rewrite the libraries to tweak
the results, so I set this version aside and proceeded with
timing tests.

</p><p>

I ran the PL/I benchmark for 29,592,068 iterations and obtained
the following run times in seconds for five runs
    (299.23,
    300.59,
    298.52,
    300.94,
    298.07).
These runs give a mean time of 299.47 seconds, or 10.1209
microseconds per iteration.

</p><p>

I then ran the C benchmark for 166,051,660 iterations, yielding
run times of
    (296.89,
    296.37,
    296.29,
    296.76,
    296.37)
seconds, with mean 296.536, for 1.7858 microseconds per iteration.

</p><p>

Dividing these gives a PL/I run time of 5.667 longer than that
of C.  In other words, for this benchmark, PL/I runs around 5.7
times slower than C.

</p><p>

This is toward the low end of compiled languages in which the
benchmark has been implemented.  Among those tested so far, it
falls between ALGOL 60 (3.95 times C) and GNU Common Lisp
(compiled, 7.41 times C), and it is more than twice as fast as
Micro Focus Visual COBOL in floating point mode (12.5 times C).
It should be remembered, however, that this is a beta test
compiler under active development, and that optimisation is
often addressed after full implementation of the language.  And
since the libraries are largely written in PL/I, any
optimisation of compiler-generated code will improve library
performance as well.  The lack of optimisation of constant
integer exponents which caused the initial surprise in timing
tests will, one hopes, be addressed in a subsequent release of
the compiler. Further, the second largest consumer of time in
the benchmark, after the main program itself with 44%, was the
ATAN function, with 23.6%.  But the ATAN function is only used
to emulate the ASIN builtin, which isn't presently implemented.
If and when an ASIN function is provided, and if its
implementation is more efficient (for example, using a <a href="http://mathworld.wolfram.com/MaclaurinSeries.html" target="Fourmilog_Aux">Maclaurin
series</a>) than my emulation, a substantial increase in performance
will be possible.

</p><p>

Nothing inherent in the PL/I language limits its performance.
Equivalent code, using the same native data types, should be
able to run as fast as C or Fortran, and mature commercial
compilers from IBM and other vendors have demonstrated this
performance but at a price.  The Iron Spring compiler is a
promising effort to deliver a professional quality PL/I compiler
for personal computers at an affordable price (and, in its
present beta test incarnation, for free).

</p><p>

The relative performance of the various language implementations (with C taken as 1) is as follows.   All language implementations of the benchmark listed below produced identical results to the last (11th) decimal place.

</p><p>


<table>
                                                                                                 
<tr>
    <th>Language</th>
    <th>Relative<br /> Time</th>
    <th>Details</th>
</tr>
 
<tr>
    <th>C</th>
    <td>1</td>
    <td>GCC 3.2.3 -O3, Linux</td>
</tr>
 
<tr>
    <th>Visual Basic .NET</th>
    <td>0.866</td>
    <td>All optimisations, Windows XP</td>
</tr>
 
<tr>
    <th>FORTRAN</th>
    <td>1.008</td>
    <td>GNU Fortran (g77) 3.2.3 -O3, Linux</td>
</tr>
 
<tr>
    <th>Pascal</th>
    <td>1.027<br />
                   1.077</td>
    <td>Free Pascal 2.2.0 -O3, Linux<br />
    	    	  GNU Pascal 2.1 (GCC 2.95.2) -O3, Linux</td>
</tr>
 
<tr>
    <th>Swift</th>
    <td>1.054</td>
    <td>Swift 3.0.1, -O, Linux</td>
</tr>

<tr>
    <th>Rust</th>
    <td>1.077</td>
    <td>Rust 0.13.0, --release, Linux</td>
</tr>

<tr>
    <th>Java</th>
    <td>1.121</td>
    <td>Sun JDK 1.5.0_04-b05, Linux</td>
</tr>

<tr>
    <th>Visual Basic 6</th>
    <td>1.132</td>
    <td>All optimisations, Windows XP</td>
</tr>
  
<tr>
    <th>Haskell</th>
    <td>1.223</td>
    <td>GHC 7.4.1-O2 -funbox-strict-fields, Linux</td>
</tr>
  
<tr>
    <th>Scala</th>
    <td>1.263</td>
    <td>Scala 2.12.3, OpenJDK 9, Linux</td>
</tr>

<tr>
    <th>Ada</th>
    <td>1.401</td>
    <td>GNAT/GCC 3.4.4 -O3, Linux</td>
</tr>

<tr>
    <th>Go</th>
    <td>1.481</td>
    <td>Go version go1.1.1 linux/amd64, Linux</td>
</tr>

 <tr>
    <th>Simula</th>
    <td>2.099</td>
    <td>GNU Cim 5.1, GCC 4.8.1 -O2, Linux</td>
</tr>

<tr>
    <th>Lua</th>
    <td>2.515<br />
        22.7</td>
    <td>LuaJIT 2.0.3, Linux<br />
        Lua 5.2.3, Linux</td>
</tr>

<tr>
    <th>Python</th>
    <td>2.633<br /> 30.0</td>
    <td>PyPy 2.2.1 (Python 2.7.3), Linux<br />
        Python 2.7.6, Linux
</td>
</tr>

 <tr>
    <th>Erlang</th>
    <td>3.663<br />
    	    	   9.335</td>
    <td>Erlang/OTP 17, emulator 6.0, HiPE [native, {hipe, [o3]}]<br />
    	         Byte code (BEAM), Linux</td>
</tr>
 <tr>
    <th>ALGOL 60</th>
    <td>3.951</td>
    <td>MARST 2.7, GCC 4.8.1 -O3, Linux</td>
</tr>
  
<tr>
    <th>PL/I</th>
    <td>5.667</td>
    <td>Iron Spring PL/I 0.9.9b beta, Linux</td>
</tr>

<tr>
    <th>Lisp</th>
    <td>7.41 <br />
                   19.8</td>
    <td>GNU Common Lisp 2.6.7, Compiled, Linux<br />
                  GNU Common Lisp 2.6.7, Interpreted</td>
</tr>

 <tr>
    <th>Smalltalk</th>
    <td>7.59</td>
    <td>GNU Smalltalk 2.3.5, Linux</td>
</tr>
   
<tr>
    <th>Forth</th>
    <td>9.92</td>
    <td>Gforth 0.7.0, Linux</td>
</tr>

<tr>
    <th>COBOL</th>
    <td>12.5<br />
				   46.3</td>
    <td>Micro Focus Visual COBOL 2010, Windows 7<br />
				  Fixed decimal instead of computational-2</td>
</tr>

 <tr>
    <th>Algol 68</th>
    <td>15.2</td>
    <td>Algol 68 Genie 2.4.1 -O3, Linux</td>
</tr>

<tr>
    <th>Perl</th>
    <td>23.6</td>
    <td>Perl v5.8.0, Linux</td>
</tr>

<tr>
    <th>Ruby</th>
    <td>26.1</td>
    <td>Ruby 1.8.3, Linux</td>
</tr>

<tr>
    <th>JavaScript</th>
    <td>27.6 <br />
                   39.1 <br />
                   46.9</td>
    <td>Opera 8.0, Linux<br />
                  Internet Explorer 6.0.2900, Windows XP<br />
                  Mozilla Firefox 1.0.6, Linux</td>
</tr>
 
<tr>
    <th>QBasic</th>
    <td>148.3</td>
    <td>MS-DOS QBasic 1.1, Windows XP Console</td>
</tr>

<tr>
    <th>Mathematica</th>
    <td>391.6</td>
    <td>Mathematica 10.3.1.0, Raspberry Pi 3, Raspbian</td>
</tr>

</table>
</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://jvns.ca/blog/2017/09/24/profiling-go-with-pprof/">Profiling Go programs with pprof</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://jvns.ca/atom.xml">Julia Evans</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">practices</span>
              <span class="tag">linux</span>
              <span class="tag">networking</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Sun Sep 24 2017 10:43:36 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>Last week me and my cool coworker Josh were debugging some memory problems in a Go program using <a href="https://golang.org/pkg/runtime/pprof/">pprof</a>.</p>

<p>There’s a bunch of pprof documentation on the internet but I found a few things confusing so here
are some notes so I can find them easily.</p>

<p>First – when working with pprof it’s good to be running a recent version of Go! For example Go 1.8
adds <a href="https://rakyll.org/mutexprofile/">mutex profiles</a> so you can see mutex contention.</p>

<p>in this post I’ll</p>

<ul>
<li>link to the useful pprof resource I found</li>
<li>explain what a pprof profile is</li>
<li>give an example of how to look at a heap profile of a Go program</li>
<li>explain a few things about the heap profiler works (what do the stack traces mean? how are they collected?)</li>
<li>most importantly (to me), deconstruct an example pprof protobuf file so we understand what a pprof profile
actually is</li>
</ul>

<p>This post won’t really explain in detail how to to use pprof to diagnose performance issues in Go
programs, but I think these fundamentals (“what even is a pprof file”) will help me do that more
easily.</p>

<h3>pprof basics</h3>

<p>pprof lets you collect CPU profiles, traces, and heap profiles for your Go programs. The normal way
to use pprof seems to be:</p>

<ol>
<li>Set up a webserver for getting Go profiles (with <code>import _ &quot;net/http/pprof&quot;</code>)</li>
<li>Run <code>curl localhost:$PORT/debug/pprof/$PROFILE_TYPE</code> to save a profile</li>
<li>Use <code>go tool pprof</code> to analyze said profile</li>
</ol>

<p>You can also generate pprof profiles in your code using the <a href="https://golang.org/pkg/runtime/pprof/"><code>pprof</code> package</a> but I haven’t done that.</p>

<h3>Useful pprof reading</h3>

<p>Here is every useful link I’ve found so far about pprof on the internet. Basically the material on
the internet about pprof seems to be the official documentation + rakyll’s amazing blog.</p>

<ul>
<li>Setting up a pprof webserver: <a href="https://golang.org/pkg/net/http/pprof/">https://golang.org/pkg/net/http/pprof/</a></li>
<li>Generating pprof profiles in code: <a href="https://golang.org/pkg/runtime/pprof/">https://golang.org/pkg/runtime/pprof/</a></li>
<li><a href="https://github.com/google/pprof">https://github.com/google/pprof</a> (from which I found out that <code>pprof</code> can read perf files!!)</li>
<li>The developer docs: <a href="https://github.com/google/pprof/blob/master/doc/pprof.md">https://github.com/google/pprof/blob/master/doc/pprof.md</a></li>
<li>The output of <code>go tool pprof --help</code> (I pasted the output on my system <a href="https://gist.github.com/jvns/6deaa10500f375a8581a06da8d8a967b">here</a>)</li>
<li><a href="https://twitter.com/rakyll">@rakyll</a>’s blog, which has a huge number of great posts about pprof: <a href="https://rakyll.org/archive/">https://rakyll.org/archive/</a>. In particular <a href="https://rakyll.org/custom-profiles/">this post on custom pprof profile types</a> and <a href="https://rakyll.org/mutexprofile/">this on the newish profile type for seeing contended mutexes</a> are great.</li>
</ul>

<p>(there are probably also talks about pprof but I am too impatient to watch talks, that’s part of why
I write lots of blog posts and give few talks)</p>

<h3>What’s a profile? What kinds of profiles can I get?</h3>

<p>When understanding how things work I like to start at the beginning. What is a “profile” exactly?</p>

<p>Well, let’s read the documentation! The 7th time I looked at <a href="https://golang.org/pkg/runtime/pprof/">the runtime/pprof docs</a>, I read this very useful sentence:</p>

<blockquote>
<p>A Profile is a collection of stack traces showing the call sequences that led to instances of a
particular event, such as allocation. Packages can create and maintain their own profiles; the most
common use is for tracking resources that must be explicitly closed, such as files or network
connections.</p>

<p>Each Profile has a unique name. A few profiles are predefined:</p>
</blockquote>

<pre><code>goroutine    - stack traces of all current goroutines
heap         - a sampling of all heap allocations
threadcreate - stack traces that led to the creation of new OS threads
block        - stack traces that led to blocking on synchronization primitives
mutex        - stack traces of holders of contended mutexes
</code></pre>

<p>There are 7 places you can get profiles in the default webserver: the ones mentioned above</p>

<ul>
<li><a href="http://localhost:6060/debug/pprof/goroutine">http://localhost:6060/debug/pprof/goroutine</a></li>
<li><a href="http://localhost:6060/debug/pprof/heap">http://localhost:6060/debug/pprof/heap</a></li>
<li><a href="http://localhost:6060/debug/pprof/threadcreate">http://localhost:6060/debug/pprof/threadcreate</a></li>
<li><a href="http://localhost:6060/debug/pprof/block">http://localhost:6060/debug/pprof/block</a></li>
<li><a href="http://localhost:6060/debug/pprof/mutex">http://localhost:6060/debug/pprof/mutex</a></li>
</ul>

<p>and also 2 more: the CPU profile and the CPU trace.</p>

<ul>
<li><a href="http://localhost:6060/debug/pprof/profile">http://localhost:6060/debug/pprof/profile</a></li>
<li><a href="http://localhost:6060/debug/pprof/trace?seconds=5">http://localhost:6060/debug/pprof/trace?seconds=5</a></li>
</ul>

<p>To analyze these profiles (lists of stack traces), the tool to use is <code>go tool pprof</code>, which is a bunch of
tools for visualizing stack traces.</p>

<p><strong>super confusing note</strong>: the trace endpoint (<code>/debug/pprof/trace?seconds=5</code>), unlike all the rest, outputs a file that is <strong>not</strong> a
pprof profile. Instead it’s a <strong>trace</strong> and you can view it using <code>go tool trace</code> (not <code>go tool pprof</code>).</p>

<p>You can see the available profiles with <a href="http://localhost:6060/debug/pprof/">http://localhost:6060/debug/pprof/</a> in your browser. Except
it doesn’t tell you about  <code>/debug/pprof/profile</code> or <code>/debug/pprof/trace</code> for some reason.</p>

<p>All of these kinds of profiles (goroutine, heap allocations, etc) are just collections of
stacktraces, maybe with some metadata attached. If we look at the <a href="https://github.com/google/pprof/blob/master/proto/profile.proto">pprof protobuf definition</a>, you see that a profile is mostly a bunch of <code>Sample</code>s.</p>

<p>A sample is basically a stack trace. That stack trace might have some extra information attached to
it! For example in a heap profile, the stack trace has a number of bytes of memory attached to it. I
think the Samples are the most important part of the profile.</p>

<p>We’re going to deconstruct what <strong>exactly</strong> is inside a pprof file later, but for now let’s start by
doing a quick example of what analyzing a heap profile looks like!</p>

<h3>Getting a heap profile with pprof</h3>

<p>I’m mostly interested in debugging memory problems right now. So I decided to write a program that
allocates a bunch of memory to profile with pprof.</p>

<pre><code>func main() {
    // we need a webserver to get the pprof webserver
    go func() {
        log.Println(http.ListenAndServe(&quot;localhost:6060&quot;, nil))
    }()
    fmt.Println(&quot;hello world&quot;)
    var wg sync.WaitGroup
    wg.Add(1)
    go leakyFunction(wg)
    wg.Wait()
}

func leakyFunction(wg sync.WaitGroup) {
    defer wg.Done()
    s := make([]string, 3)
    for i:= 0; i &lt; 10000000; i++{
        s = append(s, &quot;magical pandas&quot;)
        if (i % 100000) == 0 {
            time.Sleep(500 * time.Millisecond)
        }
    }
}
</code></pre>

<p>Basically this just starts a goroutine <code>leakyFunction</code> that allocates a bunch of memory and then
exits eventually.</p>

<p>Getting a heap profile of this program is really easy – we just need to run <code>go tool pprof
http://localhost:6060/debug/pprof/heap</code>. This puts us into an interactive mode where we run <code>top</code></p>

<pre><code>$ go tool pprof  http://localhost:6060/debug/pprof/heap
    Fetching profile from http://localhost:6060/debug/pprof/heap
    Saved profile in /home/bork/pprof/pprof.localhost:6060.inuse_objects.inuse_space.004.pb.gz
    Entering interactive mode (type &quot;help&quot; for commands)
(pprof) top
    34416.04kB of 34416.04kB total (  100%)
    Showing top 10 nodes out of 16 (cum &gt;= 512.04kB)
          flat  flat%   sum%        cum   cum%
       33904kB 98.51% 98.51%    33904kB 98.51%  main.leakyFunction
</code></pre>

<p>I can also do the same thing outside interactive mode with <code>go tool pprof -top  http://localhost:6060/debug/pprof/heap</code>.</p>

<p>This basically tells us that <code>main.leakyFunction</code> is using 339MB of memory. Neat!</p>

<p>We can also generate a PNG profile like this: <code>go tool pprof -png  http://localhost:6060/debug/pprof/heap &gt; out.png</code>.</p>

<p>Here’s what that looks like (I ran it at a different time so it’s only using 100MBish of memory).</p>

<div>

</div>

<h3>what do the stack traces in a heap profile mean?</h3>

<p>This is not complicated but also was not 100% obvious to me. The stack traces in the heap profile
are the stack trace at time of allocation.</p>

<p>So the stack traces in the heap profile might be for code that is not running anymore – like maybe
a function allocated a bunch of memory, returned, and a different function that should be freeing
that memory is misbehaving. So the function to blame for the memory leak might be totally different
than the function listed in the heap profile.</p>

<h3>alloc_space vs inuse_space</h3>

<p>go tool pprof has the option to show you either <strong>allocation counts</strong> or <strong>in use memory</strong>. If
you’re concerned with the amount of memory being <strong>used</strong>, you probably want the inuse metrics, but
if you’re worried about time spent in garbage collection, look at allocations!</p>

<pre><code>  -inuse_space      Display in-use memory size
  -inuse_objects    Display in-use object counts
  -alloc_space      Display allocated memory size
  -alloc_objects    Display allocated object counts
</code></pre>

<p>I was originally confused about this works – the profiles have already be collected! How can I make
this choice after the fact? I think how the heap profiles work is – allocations are recorded at
some sample rate. Then every time one of those allocation is <strong>freed</strong>, that’s also recorded. So you
get a history of both allocations and frees for some sample of memory activity. Then when it comes
time to analyze your memory usage, you can decide where you want inuse memory or total allocation counts!</p>

<p>You can read the source for the memory profiler here: <a href="https://golang.org/src/runtime/mprof.go">https://golang.org/src/runtime/mprof.go</a>. It
has a lot of useful comments! For example here are the comments about setting the sample rate:</p>

<pre><code>// MemProfileRate controls the fraction of memory allocations
// that are recorded and reported in the memory profile.
// The profiler aims to sample an average of
// one allocation per MemProfileRate bytes allocated.

// To include every allocated block in the profile, set MemProfileRate to 1.
// To turn off profiling entirely, set MemProfileRate to 0.

// The tools that process the memory profiles assume that the
// profile rate is constant across the lifetime of the program
// and equal to the current value. Programs that change the
// memory profiling rate should do so just once, as early as
// possible in the execution of the program (for example,
// at the beginning of main).
</code></pre>

<h3>pprof fundamentals: deconstructing a pprof file</h3>

<p>When I started working with pprof I was confused about what was actually happening. It was
generating these heap profiles named like <code>pprof.localhost:6060.inuse_objects.inuse_space.004.pb.gz</code>
– what is that? How can I see the contents?</p>

<p>Well, let’s take a look!! I wrote an even simpler Go program to get the simplest possible heap
profile.</p>

<pre><code>package main

import &quot;runtime&quot;
import &quot;runtime/pprof&quot;
import &quot;os&quot;
import &quot;time&quot;

func main() {
    go leakyFunction()
    time.Sleep(500 * time.Millisecond)
    f, _ := os.Create(&quot;/tmp/profile.pb.gz&quot;)
    defer f.Close()
    runtime.GC()
    pprof.WriteHeapProfile(f);
}

func leakyFunction() {
    s := make([]string, 3)
    for i:= 0; i &lt; 10000000; i++{
        s = append(s, &quot;magical pprof time&quot;)
    }
}
</code></pre>

<p>This program just allocates some memory, writes a heap profile, and exits. Pretty simple. Let’s look
at this file <code>/tmp/profile.pb.gz</code>! You can download a gunzipped version <code>profile.pb</code>
<a href="https://gist.github.com/jvns/828b5b99d3d7c875175c1e8a1d832161/raw/fc90af99da22bd7b4d444aa516c3d495f289d94b/profile.pb">here: profile.pb</a>. I installed protoc using <a href="https://gist.github.com/sofyanhadia/37787e5ed098c97919b8c593f0ec44d8">these directions</a>.</p>

<p><code>profile.pb</code> is a protobuf file, and it turns out you can view protobuf files with <code>protoc</code>, the
protobuf compiler.</p>

<pre><code>go get github.com/google/pprof/proto
protoc --decode=perftools.profiles.Profile  $GOPATH/src/github.com/google/pprof/proto/profile.proto --proto_path $GOPATH/src/github.com/google/pprof/proto/
</code></pre>

<p>The output of this is a bit long, you can view it all here: <a href="https://gist.githubusercontent.com/jvns/828b5b99d3d7c875175c1e8a1d832161/raw/4effe4f58a0f250093695c6f1675181b93c772c2/profile.pb.txt">output</a>.</p>

<p>Here’s a summary though of what’s in this heap profile file! This contains 1 sample. A sample is a
stack trace, and this stack trace has 2 locations: 1 and 2. What are locations 1 and 2? Well they
correspond to mappings 1 and 2, which in turn correspond to filenames 7 and 8.</p>

<p>If we look at the string table, we see that filenames 7 and 8 are these two:</p>

<pre><code>string_table: &quot;/home/bork/work/experiments/golang-pprof/leak_simplest&quot;
string_table: &quot;[vdso]&quot;
</code></pre>

<pre><code>sample {
  location_id: 1
  location_id: 2
  value: 1
  value: 34717696
  value: 1
  value: 34717696
}
mapping {
  id: 1
  memory_start: 4194304
  memory_limit: 5066752
  filename: 7
}
mapping {
  id: 2
  memory_start: 140720922800128
  memory_limit: 140720922808320
  filename: 8
}
location {
  id: 1
  mapping_id: 1
  address: 5065747
}
location {
  id: 2
  mapping_id: 1
  address: 4519969
}
string_table: &quot;&quot;
string_table: &quot;alloc_objects&quot;
string_table: &quot;count&quot;
string_table: &quot;alloc_space&quot;
string_table: &quot;bytes&quot;
string_table: &quot;inuse_objects&quot;
string_table: &quot;inuse_space&quot;
string_table: &quot;/home/bork/work/experiments/golang-pprof/leak_simplest&quot;
string_table: &quot;[vdso]&quot;
string_table: &quot;[vsyscall]&quot;
string_table: &quot;space&quot;
time_nanos: 1506268926947477256
period_type {
  type: 10
  unit: 4
}
period: 524288
</code></pre>

<h3>pprof files don’t always contain function names</h3>

<p>One interesting thing about this pprof file <code>profile.pb</code> is that it doesn’t contain the names of the
functions we’re running! But If I run <code>go tool pprof</code> on it, it prints out the name of the leaky
function. How did you do that, <code>go tool pprof</code>?!</p>

<pre><code>go tool pprof -top  profile.pb 
59.59MB of 59.59MB total (  100%)
      flat  flat%   sum%        cum   cum%
   59.59MB   100%   100%    59.59MB   100%  main.leakyFunction
         0     0%   100%    59.59MB   100%  runtime.goexit
</code></pre>

<p>I answered this with strace, obviously – I straced <code>go tool pprof</code> and this is what I saw:</p>

<pre><code>5015  openat(AT_FDCWD, &quot;/home/bork/pprof/binaries/leak_simplest&quot;, O_RDONLY|O_CLOEXEC &lt;unfinished ...&gt;
5015  openat(AT_FDCWD, &quot;/home/bork/work/experiments/golang-pprof/leak_simplest&quot;, O_RDONLY|O_CLOEXEC) = 3
</code></pre>

<p>So it seems that <code>go tool pprof</code> noticed that the filename in <code>profile.pb</code> was /home/bork/work/experiments/golang-pprof/leak_simplest, and then it just opened up that file on my computer and used that to get the function names. Neat!</p>

<p>You can also pass the binary to <code>go tool pprof</code> like <code>go tool pprof -out $BINARY_FILE myprofile.pb.gz</code>. Sometimes pprof files contain function names and
sometimes they don’t, I haven’t figured out what determines that yet.</p>

<h3>pprof keeps improving!</h3>

<p>also I found out that thanks to the great work of people like rakyll, pprof keeps getting better!! For example There’s
this pull request <a href="https://github.com/google/pprof/pull/188">https://github.com/google/pprof/pull/188</a> which is being worked on RIGHT NOW which
adds flamegraph support to the pprof web interface. Flamegraphs are the best thing in the universe
so I’m very excited for that to be available.</p>

<p>If I got someting wrong (I probably did) let me know!!</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://feedproxy.google.com/~r/TroyHunt/~3/uFrVBRWygNM/">Weekly update 53 (Salt Lake City edition)</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://feeds.feedburner.com/TroyHunt">Troy Hunt</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">security</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Sat Sep 23 2017 05:45:27 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p><a href="http://www.symantec.com/ready-secure-go/it-smb/?om_ext_cid=ws_ad_TROY_DBC-Bulldog_IT-SMB_Encrypt"><strong>Presently sponsored by:</strong> Get a security solution that will keep your website up and running—and keep you sleeping soundly: Symantec Website Security. Learn how</a></p><div><p>What a week! Epic hardly describes the experience I've just had at Pluralsight Live in Utah, not least of which was this stage:</p>
<p></p>
<p>No new writing this week but I did want to comment on the Equifax CSO degree story (and my poorly worded tweet about it) as well as the ongoing concern I keep hearing from people about biometric auth, especially in the US. So that's just a quick intro, I'm rushing this one out a bit as it seems that the one place in the world with worse connectivity than my home in Australia is US airports...</p>
<p><a href="https://itunes.apple.com/au/podcast/troy-hunts-weekly-update-podcast/id1176454699">iTunes podcast</a> | <a href="https://goo.gl/app/playmusic?ibi=com.google.PlayMusic&amp;isi=691797987&amp;ius=googleplaymusic&amp;link=https://play.google.com/music/m/If3tw7npymckucxq4q76762ncny?t%3DTroy_Hunt%27s_Weekly_Update_Podcast">Google Play Music podcast</a> | <a href="http://www.omnycontent.com/d/playlist/1439345f-6152-486d-a9c2-a6bf0067f2b7/3ba9af7f-3bfb-48fd-aae7-a6bf00689c10/fde26e49-9fb8-457d-8f16-a6bf00696676/podcast.rss">RSS podcast</a></p>

References
<ol>
<li><a href="http://www.marketwatch.com/story/equifax-ceo-hired-a-music-major-as-the-companys-chief-security-officer-2017-09-15">Here's the story on the Equifax CSO and her music degree</a> (I'll write something more on this later but in short, it's the experience that's critical)</li>
<li><a href="https://twitter.com/troyhunt/status/909158110924652545">Here's the tweet asking people about police access to phones</a> (the responses are worth reading if for no other reason than it understand the sentiment of so many people out there)</li>
<li><a href="https://stackhackr.barkly.com/?utm_source=troy-hunt&amp;utm_medium=paid&amp;utm_campaign=stackhackr">Barkly is sponsoring my blog again this week</a> (big thanks to those guys for being a repeat sponsor!)</li>
</ol>
</div>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://technology.condenast.com/story/not-progressive-alien-web-apps">Progressive Web Apps? No, we are building Alien Web Apps</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://technology.condenast.com/feed/rss">Condé Nast Technology</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">web</span>
              <span class="tag">company</span>
            </div>
            <span class="subtitle is-7">Thu Sep 21 2017 17:00:11 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            Progressive Web Apps? No, we are building Alien Web Apps
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://ronjeffries.com/articles/017-08ff/ipad-m-two-weeks/">Two week hiatus for reasons. I do a bit of planning, but no work.</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="http://ronjeffries.com/feed.xml">Ron Jeffries</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">practices</span>
              <span class="tag">agile</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Thu Sep 21 2017 05:00:00 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            Just a bit of reflection after two weeks off. Next week, I hope we'll actually do something. Today I remember the Boy Scout Refactoring Rule.
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://jvns.ca/blog/answer-questions-well/">How to answer questions in a helpful way</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://jvns.ca/atom.xml">Julia Evans</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">practices</span>
              <span class="tag">linux</span>
              <span class="tag">networking</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Thu Sep 21 2017 02:37:48 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>Your coworker asks you a slightly unclear question. How do you answer? I think asking questions
is a skill (see <a href="https://jvns.ca/blog/good-questions/">How to ask good questions</a>) and that answering questions
in a helpful way is also a skill! Both of them are super useful.</p>

<p>To start out with – sometimes the people asking you questions don’t respect
your time, and that sucks. I’m assuming here throughout that that’s not what
happening – we’re going to assume that the person asking you questions is a
reasonable person who is trying their best to figure something out and that you
want to help them out. Everyone I work with is like that and so that’s the
world I live in :)</p>

<p>Here are a few strategies for answering questions in a helpful way!</p>

<h3>If they’re not asking clearly, help them clarify</h3>

<p>Often beginners don’t ask clear questions, or ask questions that don’t have the necessary information to answer the questions. Here are some strategies you can use to help them clarify.</p>

<ul>
<li><strong>Rephrase a more specific question</strong> back at them (“Are you asking X?”)</li>
<li><strong>Ask them for more specific information</strong> they didn’t provide (“are you using IPv6?”)</li>
<li><strong>Ask what prompted their question</strong>. For example, sometimes people come into my team’s channel with questions about how our service discovery works. Usually this is because they’re trying to set up/reconfigure a service. In that case it’s helpful to ask “which service are you working with? Can I see the pull request you’re working on?”</li>
</ul>

<p>A lot of these strategies come from the <a href="https://jvns.ca/blog/good-questions/">how to ask good questions</a> post. (though I would never
say to someone “oh you need to read this Document On How To Ask Good Questions
before asking me a question”)</p>

<h3>Figure out what they know already</h3>

<p>Before answering a question, it’s very useful to know what the person knows already!</p>

<p>Harold Treen gave me a great example of this:</p>

<blockquote>
<p>Someone asked me the other day to explain “Redux Sagas”. Rather than dive in and say “They are like worker threads that listen for actions and let you update the store!” <br />
I started figuring out how much they knew about Redux, actions, the store and all these other fundamental concepts. From there it was easier to explain the concept that ties those other concepts together.</p>
</blockquote>

<p>Figuring out what your question-asker knows already is important because they may
be confused about fundamental concepts (“What’s Redux?”), or they may be an
expert who’s getting at a subtle corner case. An answer building on concepts
they don’t know is confusing, and an answer that recaps things they know is
tedious.</p>

<p>One useful trick for asking what people know – instead of “Do you know X?”,
maybe try “How familiar are you with X?”.</p>

<h3>Point them to the documentation</h3>

<p>“RTFM” is the classic unhelpful answer to a question, but pointing someone to a
specific piece of documentation can actually be really helpful! When I’m asking
a question, I’d honestly rather be pointed to documentation that actually
answers my question, because it’s likely to answer other questions I have too.</p>

<p>I think it’s important here to make sure you’re linking to documentation that
actually answers the question, or at least check in afterwards to make sure it
helped. Otherwise you can end up with this (pretty common) situation:</p>

<ul>
<li>Ali: How do I do X?</li>
<li>Jada: &lt;link to documentation&gt;</li>
<li>Ali: That doesn’t actually explain how to X, it only explains Y!</li>
</ul>

<p>If the documentation I’m linking to is very long, I like to point out the
specific part of the documentation I’m talking about. The <a href="https://linux.die.net/man/1/bash">bash man page</a> is 44,000 words (really!), so just
saying “it’s in the bash man page” is not that helpful :)</p>

<h3>Point them to a useful search</h3>

<p>Often I find things at work by searching for some Specific Keyword that I know will find me the answer. That keyword might not be obvious to a beginner! So saying “this is the search I’d use to find the answer to that question” can be useful. Again, check in afterwards to make sure the search actually gets them the answer they need :)</p>

<h3>Write new documentation</h3>

<p>People often come and ask my team the same questions over and over again. This is obviously not the fault of the people (how should <em>they</em> know that 10 people have asked this already, or what the answer is?). So we’re trying to, instead of answering the questions directly,</p>

<ol>
<li>Immediately write documentation</li>
<li>Point the person to the new documentation we just wrote</li>
<li>Celebrate!</li>
</ol>

<p>Writing documentation sometimes takes more time than just answering the question, but it’s often worth it! Writing documentation is especially worth it if:</p>

<p>a. It’s a question which is being asked again and again
b. The answer doesn’t change too much over time (if the answer changes every week or month, the documentation will just get out of date and be frustrating)</p>

<h3>Explain what you did</h3>

<p>As a beginner to a subject, it’s really frustrating to have an exchange like this:</p>

<ul>
<li>New person: “hey how do you do X?”</li>
<li>More Experienced Person: “I did it, it is done.”</li>
<li>New person: ….. but what did you DO?!</li>
</ul>

<p>If the person asking you is trying to learn how things work, it’s helpful to:</p>

<ul>
<li>Walk them through how to accomplish a task instead of doing it yourself</li>
<li>Tell them the steps for how you got the answer you gave them!</li>
</ul>

<p>This might take longer than doing it yourself, but it’s a learning opportunity
for the person who asked, so that they’ll be better equipped to solve such
problems in the future.</p>

<p>Then you can have WAY better exchanges, like this:</p>

<ul>
<li>New person: “I’m seeing errors on the site, what’s happening?”</li>
<li>More Experienced Person: (2 minutes later) “oh that’s because there’s a database failover happening”</li>
<li>New person: how did you know that??!?!?</li>
<li>More Experienced Person: “Here’s what I did!”:

<ol>
<li>Often these errors are due to Service Y being down. I looked at $PLACE and it said Service Y was up. So that wasn’t it.</li>
<li>Then I looked at dashboard X, and this part of that dashboard showed there was a database failover happening.</li>
<li>Then I looked in the logs for the service and it showed errors connecting to the database, here’s what those errors look like.</li>
</ol></li>
</ul>

<p>If you’re explaining how you debugged a problem, it’s useful both to explain how you found out what the problem was, and how you found out what the problem wasn’t. While it might feel good to look like you knew the answer right off the top of your head, it feels even better to help someone improve at learning and diagnosis, and understand the resources available.</p>

<h3>Solve the underlying problem</h3>

<p>This one is a bit tricky. Sometimes people think they’ve got the right path to
a solution, and they just need one more piece of information to implement that
solution. But they might not be quite on the right path! For example:</p>

<ul>
<li>George: I’m doing X, and I got this error, how do I fix it</li>
<li>Jasminda: Are you actually trying to do Y? If so, you shouldn’t do X, you should do Z instead</li>
<li>George: Oh, you’re right!!! Thank you! I will do Z instead.</li>
</ul>

<p>Jasminda didn’t answer George’s question at all! Instead she guessed that George didn’t actually want to be doing X, and she was right. That is helpful!</p>

<p>It’s possible to come off as condescending here though, like</p>

<ul>
<li>George: I’m doing X, and I got this error, how do I fix it?</li>
<li>Jasminda: Don’t do that, you’re trying to do Y and you should do Z to accomplish that instead.</li>
<li>George: Well, I am not trying to do Y, I actually want to do X because REASONS. How do I do X?</li>
</ul>

<p>So don’t be condescending, and keep in mind that some questioners might be attached to the steps they’ve taken so far! It might be appropriate to answer both the question they asked and the one they should have asked: “Well, if you want to do X then you might try this, but if you’re trying to solve problem Y with that, you might have better luck doing this other thing, and here’s why that’ll work better”.</p>

<h3>Ask “Did that answer your question?”</h3>

<p>I always like to check in after I <em>think</em> I’ve answered the question and ask
“did that answer your question? Do you have more questions?”.</p>

<p>It’s good to pause and wait after asking this because often people need a
minute or two to know whether or not they’ve figured out the answer. I
especially find this extra “did this answer your questions?” step helpful after
writing documentation! Often when writing documentation about something I know
well I’ll leave out something very important without realizing it.</p>

<h3>Offer to pair program/chat in real life</h3>

<p>I work remote, so many of my conversations at work are text-based. I think of
that as the default mode of communication.</p>

<p>Today, we live in a world of easy video conferencing &amp; screensharing! At work I
can at any time click a button and immediately be in a video call/screensharing
session with someone. Some problems are easier to talk about using your
voices!</p>

<p>For example, recently someone was asking about capacity planning/autoscaling
for their service. I could tell there were a few things we needed to clear up
but I wasn’t exactly sure what they were yet. We got on a quick video call and
5 minutes later we’d answered all their questions.</p>

<p>I think especially if someone is really stuck on how to get started on a task,
pair programming for a few minutes can really help, and it can be a lot more
efficient than email/instant messaging.</p>

<h3>Don’t act surprised</h3>

<p>This one’s a rule from the Recurse Center: <a href="https://jvns.ca/blog/2017/04/27/no-feigning-surprise/">no feigning surprise</a>. Here’s a
relatively common scenario</p>

<ul>
<li>Human 1: “what’s the Linux kernel?”</li>
<li>Human 2: “you don’t know what the LINUX KERNEL is?!!!!?!!!???”</li>
</ul>

<p>Human 2’s reaction (regardless of whether they’re <em>actually</em> surprised or not)
is not very helpful. It mostly just serves to make Human 1 feel bad that they
don’t know what the Linux kernel is.</p>

<p>I’ve worked on actually pretending not to be surprised even when I actually am
a bit surprised the person doesn’t know the thing and it’s awesome.</p>

<h3>Answering questions well is awesome</h3>

<p>Obviously not all these strategies are appropriate all the time, but hopefully
you will find some of them helpful! I find taking the time to answer questions
and teach people can be really rewarding.</p>

<p>
Special thanks to Josh Triplett for suggesting this post and making many helpful additions, and to Harold Treen, Vaibhav Sagar, Peter Bhat Harkins, Wesley Aptekar-Cassels, and Paul Gowder for reading/commenting.
</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://prestonbyrne.com/2017/09/20/absolute-must-read/">Absolute must-read</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://prestonbyrne.com/feed/">Preston Byrne</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">legal</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Wed Sep 20 2017 16:43:19 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            Ray Dillinger, early Bitcoin code auditor, writes in LinkedIn about his view of the ICO phenomenon: …the Trustless nature of Bitcoin was the main thing that convinced me Satoshi wasn’t scamming. He built a highway with no toll bridge. People could use Bitcoin without creating any obligation to pay him anything ever. He wasn’t selling coins, he was giving them away for solving hashes. He reserved nothing for himself.   He wasn’t trying to line his own pockets at the expense of others. In fact I don’t think I’ve ever encountered someone so completely uninterested in personal wealth. You know...
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://blog.ncase.me/how-i-make-an-explorable-explanation/">How I Make Explorable Explanations</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="http://blog.ncase.me/rss/">Nicky Case</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">systems</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Wed Sep 20 2017 16:40:42 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>You want to share a powerful idea – an idea that could really enrich the lives of whoever you gift it to! But communication is hard. So how do you share an idea, in such a way that makes sure the message is received?</p>

<p>Well, that's easy. Do it like this:</p>

<p></p>

<p>Nah, I'm kidding. The actual process is a lot more painful.</p>

<p>In this post, I'm going to share how I make <a href="http://explorabl.es/">explorable explanations</a>: interactive things that help you <em>learn by playing</em>! Although my creative process involves a lot of backtracking and wrong turns and general flailing about, I <em>have</em> found a nice &quot;pattern&quot; for teaching things. There are no plug-and-chug formulas, but hopefully this post can help you help others learn something new – whether that's through reading, through watching, or through <em>playing</em>.</p>

<p>And the first thing to do is start with...</p>

<p></p>

1) Start With 🤔?

<p><em>“What makes [traditional teaching] so ineffective is that it answers questions the student hasn’t thought to ask. [...] You have to help them love the questions.”</em></p>

<p><em>~ Steven Strogatz, <a href="http://www.ams.org/notices/201403/rnoti-p286.pdf">&quot;Writing about Math for the Perplexed &amp; the Traumatized&quot;</a></em></p>

<p>Practicing what I preach: this very blog post starts with an important question that everyone cares about – <em>&quot;how do you share an idea?&quot;</em></p>

<p>But you don't have to make the question so blatantly in-your-face. In <a href="http://ncase.me/trust/">The Evolution of Trust</a>, I posed the question in the form of a <em>story:</em> why &amp; how did WWI soldiers create peace in the trenches? And in <a href="http://ncase.me/polygons/">Parable of the Polygons</a>, I posed the question in the form of a <em>game:</em> why &amp; how does a small individual bias result in large collective segregation?</p>

<p>However you choose to do it, you've <em>got</em> to make your reader / viewer / player curious – <strong>you've got to make them love your question.</strong></p>

<p>Only then, will they be motivated to make the long, hard climb up the...</p>

<p></p>

2) Up The Ladder of Abstraction

<p>Yeah I'm mixing my metaphors a bit here with hills and ladders but WHATEVER, the point is you've got to start grounded, then move your way up, step by step, <em>slowly</em>.</p>

<p>You may think that's obvious. But, seeing how many lecturers spew abstract jargon – talking in the clouds while their audience is still on the ground – yeah, no. Apparently it's <em>not</em> obvious. (Alternatively, some people try to &quot;dumb it down&quot; for the public. But the goal shouldn't be to <em>dumb the ideas down</em>, it should be to <em>smart the people up</em>.)</p>

<p>So: <strong>start on the ground</strong>. The very first thing you should do is give the reader a <em>concrete experience</em>. In <em>Parable of the Polygons</em>, you start by directly dragging &amp; dropping a neighborhood of shapes. In <em>The Evolution of Trust</em>, you start by directly playing against a bunch of opponents. The trick is to pick an experience that will be a good foundation for <em>everything else</em> you'll be building on top of it.</p>

<p>Then, <strong>move up, step by step.</strong> I think a good logical argument is like a good story: it shouldn't be &quot;one damn thing after another&quot;. Matt Stone &amp; Trey Parker once said that instead of making stories like this: <em>&quot;this happens, and then that happens, and then that happens, etc&quot;</em>... you should make stories like this: <em>&quot;this happens, THEREFORE that happens, BUT that happens, THEREFORE this happens, etc&quot;</em>. <br />
(for more on this idea, watch Tony Zhou's brilliant <a href="https://vimeo.com/123759973">video essay on structuring video essays</a>)</p>

<p>The same is true of any good explanation. In <em>The Evolution of Trust</em>, I tried to connect as many points as I could with BUT: &quot;You can both win if you both cooperate BUT in a single game you'll both cheat BUT in a repeated game cooperation can succeed BUT in this scenario cheaters take over in the short term BUT in the long term the cooperators succeed again BUT...&quot; and so on, and so on.</p>

<p>I like these big BUTs, and I cannot lie: it means I can show off a new counter-intuitive idea every few minutes! That's a story that's <em>packed</em> with plot twists. <br />
(note: you may also sometimes want to step back <em>down</em> from the abstract to the concrete. check out Bret Victor's <a href="http://worrydream.com/LadderOfAbstraction/">Up &amp; Down The Ladder of Abstraction</a>, which has inspired, like, 90% of my work.)</p>

<p>Anyway, once you've helped your reader reach the top of the hill / ladder / whatever metaphor we're using here, it's best to end with...</p>

<p></p>

3) End With 🤔?

<p>You want to share a powerful idea – <em>why's</em> it powerful? How does your idea let people <em>see further?</em></p>

<p>At the end of most of my explorables, I have a &quot;Sandbox Mode&quot;. There's a sandbox at the end of <em>Polygons</em>, <em>Trust</em>, <em><a href="http://ncase.me/ballot/">Ballot</a></em>, <em><a href="http://ncase.me/fireflies/">Fireflies</a></em>, <em><a href="http://ncase.me/simulating/">Emoji Simulator</a></em>... yeah to be honest, it's a bit of a cliché for me at this point, but here's the reason why I have those sandboxes:</p>

<p><strong>In the beginning, I start by giving the player <em>my</em> question. And at the end, I want them to explore their <em>own</em> questions.</strong></p>

<p>Once you've helped someone get to the top of a hill, your student can now see not just other hills that <em>they</em> didn't see before, but other hills that even <em>you</em> didn't see before. That's the true value of ending on an open-ended question: <em>it allows the student to go beyond the teacher</em>.</p>

<p>. . .</p>

<p>I feel like I've finally made it to the top of a tiny hill. I made my first explorable explanation 3½ years ago: <a href="http://ncase.me/sight-and-light/">a tutorial on making a cool visual effect for 2D games</a>. And I've learnt a heck of a lot since then!</p>

<p>But the more I learn, the more I realize how much I've <em>yet</em> to learn. There's so much I want to try out. Heck, here's a list:</p>

<ul>
<li>Explorables that aren't just single-player</li>
<li>Explorables that use real-world data</li>
<li>Explorables where you actually <a href="https://www.youtube.com/watch?v=w1_zmx-wU0U">solve problems, not just puzzles</a></li>
<li>Explorables that don't follow a set linear story: it can change its lesson based on the reader's interests &amp; prior knowledge.</li>
<li>Explorables that are partially user-generated</li>
<li>Explorables that allow dialogue between peer learners</li>
<li>Explorables that aren't standalone experiences, but something you can come back to again and again over time.</li>
<li>Explorables in VR, or AR, <a href="https://vimeo.com/71278954">or just... R.</a></li>
<li>Explorables where you can actually make your own projects... such as <em>making an explorable!</em></li>
</ul>

<p>Trying out all of that seems pretty daunting, but 1) &quot;How do you eat an elephant? One bite at a time.&quot; And 2) a lot of other people are also interested in making explorables! It's impossible for any one person to climb all these hills, but collectively, we can explore this wild, weird terrain – and <em>together</em>, we can bite a <em>lot</em> of elephants! okay my metaphors are getting really mixed here</p>

<p>But the point is this: <strong>TRUE learning is a never-ending process.</strong> You start with a 🤔, you end with more 🤔. Like Sisyphus, every time we get to the top of a hill, we'll just have to go back down, to perform the climb again.</p>

<p>And I wouldn't have it any other way.</p>

<p>  
<br />  
<em>“Tiger got to hunt, bird got to fly;<br />  
Man got to sit and wonder 'why, why, why?'<br />  
Tiger got to sleep, bird got to land;<br />  
Man got to tell himself he understand.”</em>  
<br /><br />  
~ Kurt Vonnegut
</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://brandur.org/job-drain">Transactionally Staged Job Drains in Postgres</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://brandur.org/articles.atom">Brandur Leach</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">practices</span>
              <span class="tag">tools</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Wed Sep 20 2017 15:58:14 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>Background jobs are one of the most common patterns in web
programming, and for good reason. Slow API calls and other
heavy lifting is deferred to out-of-band workers so that a
user’s request is executed as quickly as possible. In web
services, fast is a feature.</p>

<p>But when it comes to working with background jobs in
conjunction with ACID transactions of the likes you’d find
in Postgres, MySQL, or SQL Server, there are a few sharp
edges that aren’t immediately obvious. To demonstrate,
let’s take a simple workflow that starts a transaction,
executes a few DB operations, and queues a job somewhere in
the middle:</p>

<pre><code>DB.transaction do |t|
  db_op1(t)
  queue_job()
  db_op2(t)
end
</code></pre>

<p>It’s not easy to spot, but if your queue is fast, the job
enqueued by <code>queue_job()</code> is likely to fail. A worker
starts running it before its enclosing transaction is
committed, and it fails to access data that it expected to
be available.</p>

<p>As an easy example, imagine <code>db_op1()</code> inserts a user
record. <code>queue_job()</code> puts a job in the queue to retrieve
that record, and add that user’s email address (along with
a unique internal ID) to an email whitelist managed by
another service. A background worker dequeues the job, but
finds that the user record it’s looking for is nowhere to
be found in the database.</p>


  <p><a href="/assets/job-drain/job-failure.svg"></a></p>
  A job failing because the data it relies on is not yet committed.


<p>A related problem are transaction rollbacks. In these cases
data is discarded completely, and jobs inserted into the
queue will <em>never</em> succeed no matter how many times they’re
retried.</p>

<a href="#complex-problem">For every complex problem ...</a>

<p>Sidekiq has <a href="https://github.com/mperham/sidekiq/wiki/FAQ#why-am-i-seeing-a-lot-of-cant-find-modelname-with-id12345-errors-with-sidekiq">a FAQ on this exact subject</a>:</p>

<blockquote>
<p><em>Why am I seeing a lot of “Can’t find ModelName with
ID=12345” errors with Sidekiq?</em></p>

<p>Your client is creating the Model instance within a
transaction and pushing a job to Sidekiq. Sidekiq is
trying to execute your job before the transaction has
actually committed. Use Rails’s <code>after_commit :on =&gt;
:create</code> hook or move the job creation outside of the
transaction block.</p>
</blockquote>

<p>Not to pick on Sidekiq in particular (you can find similar
answers and implementations all over the web), but this
solution solves one problem only to introduce another.</p>

<p>If you queue a job <em>after</em> a transaction is committed, you
run the risk of your program crashing after the commit, but
before the job makes it to the queue. Data is persisted, but
the background work doesn’t get done. It’s a problem that’s
less common than the one Sidekiq is addressing, but one
that’s far more nefarious; you almost certainly won’t
notice when it happens.</p>

<p>Other common solutions are equally as bad. For example,
another well-worn pattern is to allow the job’s first few
tries to fail, and rely on the queue’s retry scheme to
eventually push the work through at some point after the
transaction has committed. The downsides of this
implementation is that it thrashes needlessly (lots of
wasted work is done) and throws a lot of unnecessary
errors.</p>

<a href="#transactions-as-gates">Transactions as gates</a>

<p>We can dequeue jobs gracefully by using a
<em>transactionally-staged job drain</em>.</p>

<p>With this pattern, jobs aren’t immediately sent to the job
queue. Instead, they’re staged in a table within the
relational database itself, and the ACID properties of the
running transaction keep them invisible until they’re ready
to be worked. A secondary <strong><em>enqueuer</em></strong> process reads the
table and sends any jobs it finds to the job queue before
removing their rows.</p>

<p>Here’s some sample DDL for what a <code>staged_jobs</code> table might
look like:</p>

<pre><code>CREATE TABLE staged_jobs (
    id       BIGSERIAL PRIMARY KEY,
    job_name TEXT      NOT NULL,
    job_args JSONB     NOT NULL
);
</code></pre>

<p>And here’s what a simple enqueuer implementation that sends
jobs through to Sidekiq:</p>

<pre><code># Only one enqueuer should be running at any given time.
acquire_lock(:enqueuer) do

  loop do
    DB.transaction(isolation: :repeatable_read) do
      # For best efficiency, pull jobs in large batches.
      job_batch = StagedJobs.order('id').limit(1000)

      if job_batch.count &gt; 0
        # Insert each job into the real queue.
        job_batch.each do |job|
          Sidekiq.enqueue(job.job_name, *job.job_args)
        end

        # And finally, in the same transaction remove the
        # records that made it to the queue.
        StagedJobs.where('id &lt;= ?', job_batch.last).delete
      end
    end

    # If `staged_jobs` was empty, sleep for some time so
    # we're not continuously hammering the database with
    # no-ops.
    sleep_with_exponential_backoff
  end

end
</code></pre>

<p>Transactional isolation means that the enqueuer is unable
to see jobs that aren’t yet commmitted (even if they’ve
been inserted into <code>staged_jobs</code> by an uncommitted
transaction), so jobs are never worked too early.</p>


  <p><a href="/assets/job-drain/transaction-isolation.svg"></a></p>
  Jobs are invisible to the enqueuer until their transaction is committed.


<p>It’s similarly protected against rollbacks. If a job is
inserted within a transaction that’s subsequently
discarded, the job is discarded with it.</p>

<p>The enqueuer is also totally resistant to job loss. Jobs
are only removed <em>after</em> they’re successfully transmitted
to the queue, so even if the worker dies partway through,
it will pick back up again and send along any jobs that it
missed. <em>At least once</em> delivery semantics are guaranteed.</p>


  <p><a href="/assets/job-drain/job-drain.svg"></a></p>
  Jobs being sequestered in a staging table and enqueued when they're ready to be worked.


<a href="#in-database-queues">Advantages over in-database queues</a>

<p><a href="https://github.com/collectiveidea/delayed_job">Delayed_job</a>, <a href="https://github.com/chanks/que">que</a>, and
<a href="https://github.com/QueueClassic/queue_classic">queue_classic</a> use a similar transactional
mechanic to keep jobs hidden, and take it even a step
further by having workers dequeue jobs directly from within
the database.</p>

<p>This is workable at modest to medium scale, but the frantic
pace at which workers try to lock jobs doesn’t scale very
well for a database that’s experiencing considerable load.
For Postgres in particular, <a href="/postgres-queues">long-running
transactions</a> greatly increase the amount
of time it takes for workers to find a job that they can
lock, and this can lead to the job queue spiraling out of
control.</p>

<p>The transactionally-staged job drain avoids this problem by
selecting primed jobs in bulk and feeding them into another
store like Redis that’s better-suited for distributing jobs
to competing workers.</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://8thlight.com/blog/mike-knepper/2017/09/20/framework-seams.html">Framework Seams</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://8thlight.com/blog/feed/atom.xml">8th Light Blog</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">practices</span>
              <span class="tag">principles</span>
              <span class="tag">company</span>
            </div>
            <span class="subtitle is-7">Wed Sep 20 2017 06:00:00 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>I recently created an open source plugin for <a href="https://jekyllrb.com/">Jekyll</a>, the Ruby static site generator.
<em>Jekyll LilyPond Converter</em> (henceforth JLC; <a href="https://github.com/mikeknep/jekyll-lilypond-converter">source</a>, <a href="https://rubygems.org/gems/jekyll-lilypond-converter">RubyGems</a>) converts specified code snippets in Markdown blog posts to music images using <a href="http://lilypond.org/">LilyPond</a>.
(A small demo exists on my <a href="https://mikeknep.com/2017/08/19/demoing-jlc.html">personal blog</a>.)
The project was just complex enough to warrant using a pattern I often use on larger projects, as well as bite me with a sneaky, related &quot;gotcha&quot;, both of which are worth reviewing and sharing.
This pattern relates to a concept I call a &quot;seam&quot; that I find myself carefully considering more and more in all the software I write.</p>

<h3>Seams! What are they (good for)?</h3>

<p>First and foremost, this concept is completely separate from Michael Feathers's idea of seams from <a href="http://www.informit.com/articles/article.aspx?p=359417&amp;seqNum=3">Working Effectively With Legacy Code</a>.
Instead, I offer this definition:</p>

<blockquote>
<p>Seams are locations in a codebase where ownership transitions from a third-party library to the application developer.</p>
</blockquote>

<p>For example, controllers form the primary seams within a Rails application.
Provided you follow its API and conventions, the framework takes care of accepting HTTP requests, representing them as Ruby objects, and routing them to specified classes and methods.
Once there, Rails hands control over to the application developer, who can execute whatever domain logic they please.
The developer eventually returns some value from that controller method, and upon doing so implicitly passes ownership back to Rails for it to transform the value into an HTTP response and send it back over the wire.</p>

<p>On my past several projects, I've worked harder and harder to keep my controller methods as minimal as possible—ideally they know nothing more than which &quot;Handler&quot; and &quot;Presenter&quot; to use:</p>
<div><pre><code>class WidgetsController &lt; ApplicationController
  def index
    result = WidgCo::GetWidgetsHandler.new(params).execute
    presenter = WidgCo::GetWidgetsPresenter.new(result)
    render json: presenter.json, status: presenter.status
  end

  def create
    result = WidgCo::CreateWidgetHandler.new(params).execute
    presenter = WidgCo::CreateWidgetPresenter.new(result)
    render json: presenter.json, status: presenter.status
  end
end
</code></pre></div>
<p>The controller methods above are quite stark, featuring abrupt transitions from Rails to my own namespce (<code>WidgCo</code>) that has no external dependencies.
This pattern helps decouple business logic from the framework, which makes the application both easier to test and less prone to updates to the framework introducing bugs.</p>

<p>Switching context to JLC, Jekyll's <a href="https://jekyllrb.com/docs/plugins/">plugin documentation</a> outlines the different classes from which the developer can inherit in order to hook into Jekyll's build process.
In the case of JLC, I needed to create a custom <code>Converter</code> in order to alter the content of individual posts.
The simplest example available of a custom <code>Converter</code> is one that capitalizes the entire blog post:</p>
<div><pre><code>class ScreamingConverter &lt; Jekyll::Converter

  # given the extension (`ext`) of the file Jekyll is building,
  # return a boolean indicating whether the converter should operate on that file
  def matches(ext)
    /md|markdown/.match?(ext)
  end

  # return the output extension of the file
  def output_ext(ext)
    &quot;.html&quot;
  end

  def convert(content)
    content.upcase
  end
end
</code></pre></div>
<p>The <code>convert</code> method is where the magic happens—Jekyll provides the content of the current file being built as a <code>String</code>, and Ruby's wonderful standard library is at the developer's disposal.
But just as I don't want to couple Widget creation rules to Rails by defining those rules in a controller, I don't want to couple LilyPond snippet conversion logic to Jekyll by defining it in a converter.
Instead, JLC forms an explicit seam inside <code>convert</code> by <a href="https://github.com/mikeknep/jekyll-lilypond-converter/blob/26543e9d59250156b70489cb57595a9eb6d9cf53/lib/jekyll_ext/converter.rb#L23-L31">immediately delegating</a> to a separate class with no direct dependencies on Jekyll.</p>

<h3>The Seam Test &quot;Gotcha&quot;, and Some Alternatives</h3>

<p>I mentioned above that making seams direct and abrupt can simplify testing.
This is because the code added within the framework context does nothing but map from the entrypoint that framework exposes to some custom object, class, or function.
Simply assert that the right thing is called and you're set.</p>

<p>However, these tests become more difficult as additional dependencies are introduced.
In JLC, the <code>Handler</code> domain object is <a href="https://github.com/mikeknep/jekyll-lilypond-converter/blob/26543e9d59250156b70489cb57595a9eb6d9cf53/lib/jekyll_lilypond_converter/handler.rb#L3">constructed</a> with several other dependencies in addition to the content Jekyll provides.
I decided to use some of RSpec's stubbing functionality in my <code>Converter</code> test (simplified in this post for concision, but you can <a href="https://github.com/mikeknep/jekyll-lilypond-converter/blob/26543e9d59250156b70489cb57595a9eb6d9cf53/spec/jekyll_ext/converter_spec.rb">view the source</a>):</p>
<div><pre><code>describe Converter do
  it &quot;delegates to a Handler&quot; do
    content = &quot;abc&quot;
    handler_spy = HandlerSpy.new

    allow(Handler).to receive(:new).with({
      content: content,
      naming_policy: instance_of(NamingPolicy),
      image_format: &quot;svg&quot;,
      site_manager: SiteManager.instance,
      file_builder: StaticFileBuilder
    }).and_return(handler_spy)

    converter.convert(content)

    expect(handler_spy.execute_was_called).to eq(true)
  end
end
</code></pre></div>
<p>This test ensures that the <code>Converter</code> delegates to the specified class, with the specified dependencies, and calls the specified method.
If for some reason we change that in the <code>Converter</code>, our test will catch the error.
However, we've introduced something far more subtle and dangerous: the potential for a false-positive in our test suite.</p>

<p>During the course of development, I realized the <code>Handler</code> required different dependencies.
I was test-driving this code as much as possible, so I started by updating the <code>Handler</code> unit tests to pass different values to <code>Handler#initialize</code>.
These tests of course failed, requiring me to change <code>Handler#initialize</code> and some other private methods.
Once finished, my test suite was passing... but I had introduced a bug!
The <code>Converter</code> was still constructing the <code>Handler</code> with the old set of dependencies.
However, the <code>Converter</code> spec above didn't fail, because all it does is assert that the <code>Converter</code> instantiates a <code>Handler</code> a certain way (or at least attempts to).
Sure enough, of course, the <code>Converter</code> was still instantiating it that same, but now out-of-date way.
The unit tests for each of the two classes were in sync with their respective production code, but the <em>relationship between</em> those two classes was not tested, and thus I had a green test suite with a runtime exception.</p>

<p>A statically typed language would catch this problem earlier, because the code wouldn't even compile.
Alas, we are not all so fortunate, and when working in a dynamic language like Ruby this is a trickier situation.
I don't have a perfect solution, but I have considered a few options.</p>

<h4>The Registry Option</h4>

<p>One idea is to have the <code>Converter</code> access the <code>Handler</code> through a <code>Registry</code>.
The <code>Registry</code> can be a static class that by default returns the regular <code>Handler</code> through a simple lookup, but in the test environment could be configured to return a subclass of <code>Handler</code>.
The subclassing spy overrides <code>#execute</code> to prevent running code we test elsewhere, but inherits the production <code>Handler</code>'s constructor so that they change in lockstep.
Meanwhile, passing through the <code>Registry</code> relieves the test of having to stub the <code>Handler#new</code> call.
The code could look like this (several details elided):</p>
<div><pre><code>class Registry
  def self.register(key, klass)
    registry[key] = klass
  end

  def self.for(key)
    registry[key]
  end

  def registry
    @registry ||= { lilypond_conversion: Handler }
  end
end


class MyConverter &lt; Jekyll::Converter
  def convert(content)
    handler = Registry.for(:lilypond_conversion).new({
      content: content,
      naming_policy: NamingPolicy.new,
      image_format: &quot;svg&quot;,
      site_manager: SiteManager.instance,
      file_builder: StaticFileBuilder
    }).execute
  end
end


class HandlerSpy &lt; Handler
  def execute
    @@execute_was_called = true
  end

  def execute_was_called?
    @@execute_was_called
  end
end


describe MyConverter do
  before { Registry.register(:lilypond_conversion, HandlerSpy) }

  it &quot;delegates to a Handler&quot; do
    converter.convert(&quot;abc&quot;)
    expect(Registry.for(:conversion).execute_was_called?).to eq(true)
  end
end
</code></pre></div>
<p>I did not implement this in JLC, for a few reasons.
First, it felt like more boilerplate and indirection than was ultimately necessary or worthwhile given the size of the project.
Second, Jekyll doesn't expose a particularly useful place to configure the <code>Registry</code> in production,
so <code>Registry.register</code> is production code that exists solely for the tests, which is definitely a smell.
Finally, because the <code>Converter</code> calls <code>#new</code> on the handler class retrieved from the <code>Registry</code>, we can't use Ruby's <code>Singleton</code> module (which privatizes <code>#new</code>);
instead we're forced to use a class variable (<code>@@execute_was_called</code>), and in my experience once you start using class variables you're just asking for trouble.</p>

<p>Having said all that, in a Rails app, the <code>Registry</code> may make much more sense:
there are more handlers to register, better justifying the &quot;added weight&quot;;
<a href="http://guides.rubyonrails.org/configuring.html#using-initializer-files">initializers</a> provide another seam at application start time where production classes can be registered;
and if the <code>Registry</code> were to hold pre-constructed instances (perhaps created in earlier initializers) instead of classes to be instantiated, the awkward and dangerous pseudo-singleton technique could be avoided.</p>

<p>Ultimately, this approach has pros and cons, and, like so many things in software development, needs to be evaluated on a case-by-case basis.</p>

<h4>The KISS Option</h4>

<p>The <code>Registry</code> concept seemed too heavyweight for JLC, so after evaluating it I considered an opposite approach—what if I made <em>fewer</em> classes?
I mentioned above that aggressive seams simplify testing and help decouple business logic from a framework.
However, unlike some libraries I've seen in which objects require quite a bit of global state to set up properly, instantiating a <code>Jekyll::Converter</code> in a test is not particularly unwieldy.
Furthermore, it's pretty unlikely I'll want to port the core logic of JLC to integrate with other static site generators.
The simplest option, then, is to just lift the <code>Handler</code> tests up into the <code>Converter</code> and test the entire system through that outer third-party shell.</p>

<p>This analysis ultimately suggests I've over-engineered the project, which in a vacuum may be true.
However, the tool is primarily for my own personal use and has helped me continue to develop my thoughts and opinions about this concept and approach to integrating with third-party frameworks,
so some over-engineering here does not bother me.
There is definitely a takeaway for client work here, though.
Designing a system to be flexible and easy to change makes several implicit assumptions about what kinds of changes may happen.
A healthy dose of pragmatism must be kept in mind.</p>

<h4>The Impossible-In-Ruby Option</h4>

<p>The last and most interesting approach I considered is a theoretical one inspired by Elm.
In Elm, effectful (i.e. &quot;having side effects&quot;) actions like HTTP requests are represented by value objects describing the <em>intent</em> of what to do,
but the actual <em>execution</em> of those values is handled entirely by the Elm Runtime.
If such a system existed in Ruby, theoretically I would not <a href="https://github.com/mikeknep/jekyll-lilypond-converter/blob/26543e9d59250156b70489cb57595a9eb6d9cf53/lib/jekyll_lilypond_converter/handler.rb#L39-L41">shell out</a> to LilyPond directly in the <code>Handler</code>, but instead create and return a value representing that action.
My unit tests could then simply make assertions about that value, and the Fantasy-Elmlike-Ruby Runtime would understand how to interpret that value as a command to execute certain instructions on the filesystem.
If you're curious about this idea and want to learn more, I highly recommend reading the <a href="https://guide.elm-lang.org/architecture/effects/">official guide</a>, and in particular the section on <a href="https://guide.elm-lang.org/">Effects</a>.</p>

<h3>Liberate Your Code</h3>

<p>Despite the lack of a perfect, one-size-fits-all testing strategy, I find it valuable to be conscious of the seams in a codebase.
Being aware of seams affords freedom to application developers.
Typically, READMEs and other example resources provide the absolute simplest demonstration of how to integrate with a framework without any comment on how that style of integration scales.
It is easy to start learning about a new tool or platform and believe that your code needs to live inside a foreign class of unknown complexity.
Of course there are certain rules to follow, but when a code example includes some trivial stand-in for your domain logic,
I recommend thinking carefully about whether you want your code to stay in that framework-defined location, or if you'd rather treat the seam as the entrypoint to an entirely separate world of your own.
The decision is yours!</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://www.fourmilab.ch/fourmilog/archives/2017-09/001711.html">UNUM Updated to Unicode 10, HTML5</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://www.fourmilab.ch/fourmilog/atom_10.xml">Fourmilog - Fourmilab Change Log</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Tue Sep 19 2017 19:49:06 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            I have just posted version 2.1 of <a href="/webtools/unum/" target="Fourmilog_Aux">UNUM</a>.  This update, the first since 2006, updates the database of <a href="https://en.wikipedia.org/wiki/Unicode" target="Fourmilog_Aux">Unicode</a> characters to Unicode version 10.0.0 (June 2017) and adds, for the first time, full support for the entire set of Chinese, Japanese, and Korean (<a href="https://en.wikipedia.org/wiki/CJK_Unified_Ideographs" target="Fourmilog_Aux">CJK</a>) ideographic characters, for a total of 136,755 characters in all.  CJK characters are identified by their nomenclature in commonly used lexicons, and, where specified in the Unicode database, English definitions.

<p>

The table of HTML named character references (the sequences like “&amp;lt;” you use in HTML source code when you need to represent a character which has a syntactic meaning in HTML or which can't be directly included in a file with the character encoding you're using to write it) has been updated to the <a href="https://www.w3.org/TR/html5/syntax.html#named-character-references" target="Fourmilog_Aux">list published</a> by the <a href="https://www.w3.org/" target="Fourmilog_Aux">World Wide Web Consortium</a> (W3C) for <a href="https://en.wikipedia.org/wiki/HTML5" target="Fourmilog_Aux">HTML5</a>.

</p><p>

It used to be that HTML named character references were a convenient text-based shorthand so that, for example, if your keyboard or content management system didn't have a direct way to specify the Unicode character for a right single quote, you could write “&amp;rsquo;” instead of “&amp;#8217;”, the numeric code for the character.  This was handy, made the HTML easier to understand, and made perfect sense and so, of course, it had to be “improved”.  Now, you can specify the same character as either “&amp;rsquo;”, “&amp;CloseCurlyQuote;”, or “&amp;rsquor;” as well.  “Close Curly Quote”—are there also Larry and Moe quotes?  Now, apparently to accommodate dim people who can't remember or be bothered to look up the standard character references which have been in use for more than a decade (and how many of them are writing HTML, anyway?), we have lost the ability to provide a unique HTML character reference for Unicode code points which have them.  In other words, the mapping from code points to named character references has gone from one-to-one to one-to-many.

</p><p>

Further, named character references have been extended from a symbolic nomenclature for Unicode code points to specify logical character definitions which are composed of multiple (all the current ones specify only two) code points which are combined to generate the character.  For example, the character reference “&amp;nGtv;”, which stands for the mathematical symbol “not much greater than”, is actually composed of code points U+226B (MUCH GREATER-THAN) and U+0338 (COMBINING LONG SOLIDUS OVERLAY).

</p><p>

Previously, UNUM could assume a one-to-one mapping between HTML character references and Unicode code points, but thanks to these innovations this is no longer the case.  Now, when a character is displayed, if it has more than one HTML name, they are all displayed in the HTML column, separated by commas.  If the user looks up a composite character reference, all of the Unicode code points which make it up are displayed, one per line, in the order specified in the W3C specification.

</p><p>

The addition of the CJK characters makes the code point definition table, which was already large, simply colossal.  The Perl code for UNUM including this table is now almost eight megabytes.  To cope with this, there is now a compressed version of UNUM in which the table is compressed with the <b><a href="https://en.wikipedia.org/wiki/Bzip2" target="Fourmilog_Aux">bzip2</a></b> utility, which reduces the size of the program to less than a megabyte.  This requires a modern version of Perl and a Unix-like system on which <b>bzip2</b> is installed.  Users who lack these prerequisites may download an uncompressed version of UNUM, which will work in almost any environment which can run Perl.

</p><p>

<b><a href="/webtools/unum/" target="Fourmilog_Aux">UNUM Documentation and Download Page</a></b>

</p><p>

<b>Update:</b> Version 2.2 improves compatibility of the compressed version of the utility by automatically falling back to Perl's core IO::Uncompress::Bunzip2 module if the host system does not have <b>bunzip2</b> installed.  (2017-09-19 18:47 UTC)</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://8thlight.com/blog/fabien-townsend/2017/09/19/myths-about-unit-tests.html">Myths about Unit Tests</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://8thlight.com/blog/feed/atom.xml">8th Light Blog</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">practices</span>
              <span class="tag">principles</span>
              <span class="tag">company</span>
            </div>
            <span class="subtitle is-7">Tue Sep 19 2017 06:00:00 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>Most modern languages that we use today provide tools and documentation for unit
tests, which make it simpler to test our code. If unit testing is possible with
any language, we can witness the simplicity to unit test languages like Rust,
Elixir, or Ruby as compared to C, C++, or Assembly.</p>

<p>During my apprenticeship at 8th Light, all my code had unit tests. This has
allowed me to increase my experience with unit tests and improve my knowledge
surrounding them. I can say that I understand how they are beneficial, and where
those benefits run into a limit. In this article, I want to quash three myths
about unit tests that I once believed.</p>

Myth 1: Tests == Quality

<p>I used to believe that one of the benefits of unit tests was to improve the
quality of the software. Although it is true that they encourage us to
modularise our code and document it, unit tests do not check the quality of our
code.</p>

<p>Unit tests cannot guarantee that our code is of high quality even with software
that has 100% test coverage. Our tests do not check the consistency of our code.
For example, they won't check if we have followed a guideline or left extra spaces.</p>

<pre><code>  class MathHelper
    def calculFactorial(x)
      return 1 if x == 0
        
        x*self.calculFactorial(x - 1)

    end
  end

    RSpec.describe do
      it &quot;test&quot; do
        expect(MathHelper.new().calculFactorial(3)).to eq(6)
    end
    end</code></pre>

<pre><code>  class Factorial
    def self.for(number)
      return 1 if number == 0

      number * self.for(number - 1)
    end
  end

  RSpec.describe do
    it &quot;returns 6 when factorial of 3&quot; do
      expect(Factorial.for(3)).to eq(6)
    end
  end</code></pre>

<p>In both of the above code samples the test will be green, indicating that both
code samples are correct, but we will not have any feedback telling us which
one is better.</p>

<p>If our tests do not check our adherence to the style of our code, it also cannot
check the following:</p>

<ul>
<li>If your colleague can understand your code.</li>
<li>If you have coded with intention.</li>
<li>If you have used appropriate names.</li>
</ul>

<p>Of course, these factors remain important for the quality of our software even if
they cannot be checked by unit tests. Our tests and code reviews actively work
together to ensure we produce software of high quality. Without these two
important elements working together, we increase the risk of technical debt in
our software.</p>

2: Tested Software Is Bug-Free

<p>Every programmer is different, but we all want software without incorrect or
unexpected results. It doesn't matter if the effect of the bug slows down or
crashes our software, they are never welcome. One way to check the correctness
of our software is to write unit tests. The role of our unit tests will be to
validate the expectations of the methods.</p>

<blockquote>
<p>Program testing can be a very effective way to show the presence of bugs, but it
is hopelessly inadequate for showing their absence.</p>

<p>Edsger W. Dijkstra, &quot;The Humble Programmer&quot; (1972)</p>
</blockquote>

<p>This led me to believe that my unit tests were helping me to build bug-free
software. But, these unit tests could be replaced by manual tests and they
would catch the same amount of bugs.</p>

<p>What unit tests provide over manual testing is speed and thus, this saves us
time. This speed will give you the opportunity to run your test whenever you
modify your software, without wasting time and ensure to not create regression.</p>

<p></p>

3: Tests Are Optional

<p>We could believe that implementation and testing are two different tasks. After
all, unit tests do not make a new feature work, no matter how beneficial they are.
This made me feel that the unit tests were the cherry on the cake, not something
integral to the taste.</p>

<p>If the implementation is needed to make the feature work, then its tests will be
responsible for the correctness of the application, which is expected from a
software. The unit tests have the same advantages and flaws as any code; they
are fast-executing, repetitive tasks, but they can be difficult to maintain. We
risk not putting as much effort into the tests and the implementation of a
feature if we consider our unit tests as an optional extra. Underestimating the importance of quality tests over your production code will create a quality gap in both, where the cost of maintaining your tests will overtake its benefits. Quality tests should enable you to work faster, while also serving as good documentation for others to understand.</p>

<p>The test is part of feature, therefore should be considered upfront during the
estimation of any new tasks.</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://dustycloud.org/blog/drm-will-unravel-the-web/">DRM will unravel the Web</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="http://dustycloud.org/blog/index.xml">DustyCloud</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">web</span>
              <span class="tag">architecture</span>
              <span class="tag">practices</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Mon Sep 18 2017 20:30:00 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>I'm a web standards author and I participate in the W3C.
I am co-editor of the <a href="https://www.w3.org/TR/activitypub/">ActivityPub</a>
protocol, participate in a few other community groups and working
groups, and I consider it an honor to have been able to participate in
the W3C process.
What I am going to write here though represents me and my feelings
alone.
In a sense though, that makes this even more painful.
This is a blogpost I don't have time to write, but here I am writing it;
I am emotionally forced to push forward on this topic.
The W3C has
<a href="https://www.networkworld.com/article/3225456/internet/w3c-drm-appeal-fails-votes-kept-secret.html">allowed DRM to move forward on the web</a>
through the
<a href="https://www.w3.org/TR/encrypted-media/">EME specification</a>
(which is,
<a href="http://dustycloud.org/blog/memories-of-a-march-against-drm/">to paraphrase Danny O'Brien from the EFF</a>,
a &quot;DRM shaped hole where nothing else but DRM fits&quot;).
This threatens to unravel the web as we know it.
How could this happen?  How did we get here?</p>
<p>Like many of my generation, I grew up on the web, both as a citizen of
this world and as a developer.
&quot;Web development&quot;, in one way or another, has principally been my work
for my adult life, and how I have learned to be a programmer.
The web is an enormous, astounding effort of many, many participants.
Of course, Tim Berners-Lee is credited for much of it, and deserves much
of this credit.
I've had the pleasure of meeting Tim on a couple of occasions;
when you meet Tim it's clear how deeply he cares about the web.
Tim speaks quickly, as though he can't wait to get out the ideas that
are so important to him, to try to help you understand how wonderful
and exciting this system it is that we can build together.
Then, as soon as he's done talking, he returns to his computer and gets
to hacking on whatever software he's building to advance the web.
You don't see this dedication to &quot;keep your hands dirty&quot; in the gears
of the system very often, and it's a trait I admire.
So it's very hard to reconcile that vision of Tim with someone who
would intentionally unravel their own work... yet by allowing the
W3C to approve DRM/EME, I believe that's what has happened.</p>
<p>I had an opportunity to tell Tim what I think about DRM and EME on the
web, and unfortunately I blew it.
At TPAC (W3C's big conference/gathering of the standards minds) last
year, there was a protest against DRM outside.
I was too busy to take part, but I did talk to a friend who is close
to Tim and was frustrated about the protests happening outside.
After I expressed that I sympathized with the protestors (and that
I had even indeed
<a href="http://dustycloud.org/blog/memories-of-a-march-against-drm/">protested myself in Boston</a>),
I explained my position to my friend.
Apparently I was convincing enough where they encouraged me to talk to
Tim and offer my perspective; they offered to flag them down for a
chat.
In fact Tim and I did speak over lunch, but -- although we had met
in person before -- it was my first time talking to Tim one-on-one, and
I was embarassed for that first interaction would me to be talking about
DRM and what I was afraid was a sore subject for him.
Instead we had a very pleasant conversation about the work I was doing
on ActivityPub and some related friends' work on other standards (such
as Linked Data Notifications, etc).
It was a good conversation, but when it was over I had an enormous
feeling of regret that has been on the back of my mind since.</p>
<p>Here then, is what I wish I had said.</p>
<p>Tim, I have
<a href="https://www.w3.org/blog/2017/02/on-eme-in-html5/">read your article on why the W3C is supporting EME</a>,
and that I know you have thought about it a great deal.
I think you believe what you are doing what is right for the web,
but I believe you are making an enormous miscalculation.
You have fought long and hard to build the web into the system it
is... unfortunately, I think DRM threatens to undo all that work
so thoroughly that allowing the W3C to effectively green-light
DRM for the web will be, looking back on your life, your greatest
regret.</p>
<p>You and I both know the dangers of DRM: it creates content that is
illegal to operate on using any of the tooling you or I will ever be
able to write.
The power of DRM is not in its technology but in the surrounding laws;
in the United States through the DMCA it is a criminal offense to
inspect how DRM systems work or to talk about these vulnerabilities.
DRM is also something that clearly cannot itself be implemented as a
standard; it relies on proprietary secrecy in order to be able to
function.
Instead, EME defines a DRM-shaped hole, but we all know what goes into
that hole... unfortunately, there's no way for you or I to build an
open and interoperable system that can fit in that EME hole, because
DRM is antithetical to an interoperable, open web.</p>
<p>I think, from reading your article, that you believe that DRM will be
safely contained to just &quot;premium movies&quot;, and so on.
Perhaps if this were true, DRM would still be serious but not as
enormous of a threat as I believe it is.
In fact, we already know that
<a href="https://www.wired.com/2015/04/dmca-ownership-john-deere/">DRM is being used by companies like John Deere</a>
to say that you don't even own your own tractor, car, etc.
If DRM can apply to tractors, surely it will apply to more than just
movies.</p>
<p>Indeed, there's good reason to believe that some companies will want
to apply DRM to every layer of the web.
Since the web has become a full-on &quot;application delivery system&quot;, of
course the same companies that apply DRM to software will want to
apply DRM to their web software.
The web has traditionally been a book which encourages being opened;
I learned much of how to program on the web through that venerable
&quot;view source&quot; right-click menu item of web browsers.
However I fully expect with EME that we will see application authors
begin to lock down HTML, CSS, Javascript, and every other bit of their
web applications down with DRM.
(I suppose in a sense this is already happening with javascript
obfuscation and etc, but the web itself was at least a system of open
standards where anyone could build an implementation and anyone could
copy around files... with EME, this is no longer the case.)
Look at the prevelance of DRM in proprietary applications
elsewhere... once the option of a W3C-endorsed DRM-route exists,
do you think these same application developers will not reach for it?
But I think if you develop the web with the vision of it being
humanity's greatest and most empowering knowledge system, you must
be against this, because if enough of the web moves over to this model
the assumptions and properties of the web as we've known it, as an
open graph to free the world, cannot be upheld.
I also know the true direction you'd like the web to go, one of
linked data systems (of which ActivityPub is somewhat quietly one).
Do you think such a world will be possible to build with DRM?
I for one do not see how it is possible, but I'm afraid that's the
path down which we are headed.</p>
<p>I'm sure you've thought of these things too, so what could be your
reason for deciding to go ahead with supporting DRM anyway?
My suspicion is it's two things contributing to this:</p>
<ol>
<li>Fear that the big players will pick up their ball and leave.
I suspect there's fear of another WHATWG, that the big players
will simply pick up their ball and leave.</li>
<li>Most especially, and related to the above, I suspect the funding
and membership structure of the W3C is having a large impact on
this.
Funding structures tend to have a large impact on decision making,
as a kind of <a href="https://en.wikipedia.org/wiki/Conway%27s_law">Conway's Law</a>
effect.
W3C is reliant on its <a href="http://manu.sporny.org/2016/rebalancing/">&quot;thin gruel&quot;</a>
of funding from member organizations (which means that large
players tend to have a larger say in how the web is built today).</li>
</ol>
<p>I suspect this is most of all what's driving the support for DRM
within the W3C.
However, I know a few W3C staff members who are clearly not excited
about DRM, and two who have quit the organization over it, so it's not
that EME is internally a technology that brings excitement to the
organziation.</p>
<p>I suppose at this point, this is where I diverge with the things I
could have said in the past and did not say as an appeal to not
allow the W3C to endorse EME.
Unfortunately, today
<a href="https://arstechnica.com/gadgets/2017/09/drm-for-html5-published-as-a-w3c-recommendation-after-58-4-approval/">EME made it to Recommendation</a>.
At the very least, I think the W3C could have gone forward with the
<a href="https://www.eff.org/pages/objection-rechartering-w3c-eme-group#covenant">Contributor Covenant proposed by the EFF</a>,
but did not.
This is an enormous disappointment.</p>
<p>What do we do now?
I think the best we can do at this point, as individual developers and
users, is speak out against DRM and refuse to participate in it.</p>
<p>And Tim, if you're listening, perhaps there's no chance now to stop
EME from becoming a Recommendation.
But your voice can still carry weight.
I encourage you to join in speaking out against the threat DRM brings
to unravel the web.</p>
<p>Perhaps if we speak loud enough, and push hard enough, we can still
save the web we love.
But today is a sad say, and from here I'm afraid it is going to be
an uphill battle.</p>
<p><strong>EDIT:</strong> If you haven't yet read Cory Doctorow / the EFF's
<a href="https://www.eff.org/deeplinks/2017/09/open-letter-w3c-director-ceo-team-and-membership">open letter to the W3C</a>, you should.</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://stratechery.com/2017/the-super-aggregators-and-the-russians/">The Super-Aggregators and the Russians</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="http://stratechery.com/feed/">Stratechery</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">business</span>
              <span class="tag">economics</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Mon Sep 18 2017 13:56:58 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>In August 2011, just a day or two into my career at Microsoft, I sat in on a monthly review meeting for Hotmail (now known as Outlook.com); the product manager running the meeting was going through the various geographies and their relevant metrics — new users, churn, revenue, etc. — and it was, well, pretty boring. It was only later that I realized just how astounding “boring” was; a small group of people in a conference room going over numbers that represented hundreds of millions of people and dollars in revenue, and most of us cared far more about what was on the menu for lunch.</p>
<p>I’ve <a href="https://twitter.com/benthompson/status/854381355026702336">reflected on that meeting often</a> over the years, particularly when it comes to Facebook and controversies like <a href="https://stratechery.com/2016/facebook-versus-the-media/">censoring too much</a>, <a href="https://stratechery.com/2017/facebook-content-guidelines-facebook-video-amazon-prime-video-on-apple-tv/">censoring too little</a>, or <a href="https://stratechery.com/2016/fake-news/">“fake news”</a>, and I was reminded of it again with <a href="https://twitter.com/markwarner/status/908431496737935361">this tweet</a>:</p>
<div>
<blockquote>
<p>And they paid in RUBLES. Seriously. <a href="https://t.co/D35tvhUEcj">https://t.co/D35tvhUEcj</a></p>
<p>— Mark Warner (@MarkWarner) <a href="https://twitter.com/MarkWarner/status/908431496737935361">September 14, 2017</a></p></blockquote>
<p> </p></div>
<p>Mark Warner, the senior Senator from Virginia, is referring to a Russian company, thought to be linked to the Kremlin’s propaganda efforts, having bought $100,000 worth of political ads on Facebook, some number of which directly mentioned 2016 presidential candidates Donald Trump and Hillary Clinton. Facebook has <a href="https://newsroom.fb.com/news/2017/09/information-operations-update/">released limited details about the ads</a>, likely due to its <a href="https://www.ftc.gov/sites/default/files/documents/cases/2012/08/120810facebookdo.pdf">2012 consent decree with the FTC</a>, which bars the company from unilaterally making private information public, as well as the problematic precedent of releasing information without a clear order compelling said release. To that end, it was reported over the weekend that special counsel Robert Mueller received <a href="https://www.wsj.com/articles/facebook-gave-special-counsel-robert-mueller-more-details-on-russian-ad-buys-than-congress-1505514552?mg=prod/accounts-wsj">a much more comprehensive set of data</a> from Facebook <a href="http://money.cnn.com/2017/09/15/media/facebook-mueller-ads/index.html">after obtaining a search warrant</a>.</p>
<p>Even with all that context, though, I found Senator Warner’s tweet puzzling: how else would the propaganda group have paid? Facebook’s self-service ad portal lets you buy ads in 55 different currencies, including the Russian Ruble:<a href="https://stratechery.com/2017/the-super-aggregators-and-the-russians/#footnote_0_2742">1</a></p>
<p><a href="https://stratechery.com/wp-content/uploads/2017/09/Screen-Shot-2017-09-18-at-5.09.22-PM-1.png"></a></p>
<p>That, though, brought me back to that Hotmail meeting: that I, and probably many more in the tech industry, find the idea of Facebook selling ads in rubles to strangers to be utterly unremarkable, even as thousands find it equally outrageous and damning, is a reminder of just how unprecedented and misunderstood aggregators like Facebook continue to be, and what a challenge it will be to regulate them.</p>
<h4>The Cellular Network Company</h4>
<p>Senator Warner, it should be noted, is considered one of the most technologically literate people in the entire Senate — and <a href="https://en.wikipedia.org/wiki/List_of_current_members_of_the_United_States_Congress_by_wealth">the richest</a>. Warner originally made his fortune by <a href="https://www.theatlantic.com/magazine/archive/2006/05/the-man-with-the-golden-phone/304777/">facilitating the sale of cellular phone licenses</a>; he then co-founded <a href="https://www.crunchbase.com/organization/columbia-capital">Columbia Capital</a>, a venture capital firm which specialized in cellular businesses: the firm’s early investments included Nextel, BroadSoft, and MetroPCS.</p>
<p>A cellular network company is certainly a new kind of business that is similar to today’s tech giants in many respects:</p>
<ul>
<li>At a fundamental level, cellular network companies are about the movement of information — voice and text, in Warner’s era — not physical goods. Moreover, because this information is digital, there are no marginal distribution costs in its transfer. This is the same characteristic of companies like Google and Facebook.</li>
<li>A cellular network company has massive fixed costs and minimal marginal costs; one more minute of talk time costs practically nothing to provide, unless the network is saturated, at which point significant capital investment is necessary. Today’s internet services are similar: marginal usage is effectively free, although significant capital investments in data centers are necessary (as well as significant ongoing bandwidth costs, which are effectively zero to serve any one individual but huge in aggregate).</li>
<li>A cellular network company is, quite obviously, a network. That means the value of the service increases as the number of customers increases. This produces a powerful virtuous cycle in which new customers increase the value of the network such that it becomes attractive to new marginal customers, further increasing the value of the network for the next set of marginal customers; this “network effect” is the most common driver of the sort of “scalable advantage in customer acquisition costs” that I discussed <a href="https://stratechery.com/2017/ubers-new-ceo/">in the case of Uber</a>, and is a hallmark of Facebook in particular (but also Google and all of the aggregators).</li>
<li>Cellular network companies have direct relationships with their customers.</li>
</ul>
<p>These four characteristics may seem familiar: they are all parts of <a href="https://stratechery.com/2015/aggregation-theory/">Aggregation Theory</a>, and I’ve written about each of those components in the two years since I first wrote about the theory.<a href="https://stratechery.com/2017/the-super-aggregators-and-the-russians/#footnote_1_2742">2</a> There is one more piece, though, that I have only mentioned in passing: zero transaction costs. This is the piece that apparently sets Facebook beyond Senator Warner’s understanding,<a href="https://stratechery.com/2017/the-super-aggregators-and-the-russians/#footnote_2_2742">3</a> and it is perhaps the key reason why Facebook and other aggregators are unlike any other company we have seen before; oh, and it explains this Russian ad buy.</p>
<h4>Transaction Costs</h4>
<p>Go back to the generic cellular network company I discussed above, and think about what is entailed in adding a new customer (and leaving aside the marketing expenditure to make them aware of and desirous of the service in the first place):</p>
<ul>
<li>Talk with the customer on the phone or in person</li>
<li>Collect identifying details and run a credit check</li>
<li>Provision a SIM card and/or a phone</li>
<li>Receive payment</li>
<li>Manage contract renewals and cancellations and other customer service</li>
</ul>
<p>While some of these activities could be automated, the reality is that the cost of customer management had a linear curve: more customers meant more costs. Moreover, these costs accumulated, limiting the natural size of any company; at some point the complexity of managing some finite number of customers across some finite number of geographic areas cost more than the marginal profit of adding one more customer, and that limited how big a company could grow (which, to be clear, could be very large indeed!).</p>
<p>What makes aggregators unique, though, is that thanks to the Internet they have zero transaction costs: for Google, or Airbnb, or Uber, or Netflix, or Amazon, or the online travel agents, adding one more customer is as simple as adding one more row in a database. Everything else is automated, from sign-up to billing to the delivery of the service in question. This is why all of these companies are global, often from day one, and, as I explained in <a href="https://stratechery.com/2015/beyond-disruption/">Beyond Disruption</a>, why they start at the high end of a market and work their way down.</p>
<p>Note that aggregators can deal with the physical world and still have zero transaction costs, at least on the consumer side: Airbnb deals with rooms, but bears no transaction costs when it comes to signing up new customers; Amazon and Uber are similar with regards to e-commerce and transportation, respectively. Netflix doesn’t deal in physical goods (beyond its old DVD business), although it does bear significant transaction costs when it comes to sourcing content (in addition to actually paying for the content), but when it comes to customers there are no marginal costs at all.</p>
<p>Facebook and Google, though are a special case: they are (and yes, I know this is the least imaginative term ever) super-aggregators.</p>
<h4>Super-Aggregators</h4>
<p>What makes Facebook and Google unique is that not only do they have zero transaction costs when it comes to serving end users, they also have zero transaction costs when it comes to both suppliers and advertisers.</p>
<p>Start with supply: not only is the vast majority of online content accessible to Google’s search engine (unsurprisingly, the biggest exception is Facebook), but in fact that content <em>wants</em> to be discovered by Google. Nearly every site on the web has a sitemap that is intended not for humans but for web crawlers, Google’s in particularly, and there is an entire industry dedicated to search engine optimization (SEO). Netflix is on the opposite side of the spectrum here (unlike YouTube, it should be noted): the company has to actively source content and pay for it. Uber and Airbnb and Amazon are in the middle: theoretically there is an open platform for suppliers but there are costs involved in bringing them online.</p>
<p>Facebook takes this to another level: its users are its most important content providers, and they do it for free. Professional content providers aren’t far behind, not only linking to all of their content but increasingly putting said content on Facebook directly (to the extent Facebook is paying for content it is to juice this cycle of self-interested content production on Facebook).</p>
<p>That said, there are a few more companies that have a similar content model: Twitter, Snapchat, LinkedIn, Yelp, etc. All run on user-generated content augmented by professional content placing links or original material on their services. However, there is still one more thing that separates Facebook and Google from the rest: advertisers.</p>
<p>Super-aggregators not only have zero transaction costs when it comes to users and content, but also when it comes to making money. This is at the very core of why Google and Facebook are so much more powerful than any of the other purely information-centric networks. The vast majority of advertisers on both networks never deal with a human (and if they do, it’s in customer support functionality, not sales and account management): they simply use the self-serve ad products like the one pictured above (or a more comprehensive tool built on the companies’ self-serve API).</p>
<p>This is the level that the other social networks have not reached: Twitter grew revenue, but primarily through its sales team, which meant that <a href="https://stratechery.com/2016/googles-earnings-the-problem-with-alphabet-twitter-earnings-layoffs-vine/">costs increased inline with revenue</a>; the company never gained the leverage that comes from having a self-serve ad platform (specifically, the self-serve platform costs are fixed but the revenue is marginal).</p>
<p><a href="https://stratechery.com/2016/googles-earnings-the-problem-with-alphabet-twitter-earnings-layoffs-vine/"></a></p>
<p>Snap is following in Twitter’s footsteps: to date the vast majority of the company’s revenue has come from its sales team; the company has a perfunctory API for self-serve ads, but most of the volume springs from the aforementioned deals made by its sales team. Similar stories can be told about <a href="https://stratechery.com/2016/the-reality-of-missing-out/">LinkedIn, Yelp, and other advertising-based businesses</a>.</p>
<p>This, then, is a super-aggregator: zero transaction costs not just in terms of user acquisition, but also supply acquisition, and most importantly, revenue acquisition, and Google and Facebook are the ultimate examples.</p>
<h4>Facebook and the Russians</h4>
<p>This is why I was confused that Senator Warner made a big deal out of the fact Facebook was paid in Russian Rubles: the entire premise of the company’s revenue model is that anyone can run an ad without having to talk to another human, and obviously a key component of such a model is supporting multiple currencies.</p>
<p>Again, though, this is the first such model in economic history: it seems I am the one who was blinded by my having experienced the meaning of scale. In that Hotmail meeting everyone and everything was reduced to a number on a spreadsheet: the United States, Japan, Brazil, Russia, all were simply another row. So I naturally assume it is in the case of Facebook ads: that some advertisers buy in dollars, some in Yen, some in Real, others in Rubles is unremarkable to me, and, I suspect, many of the folks working at these companies.</p>
<p>And yet, it is not at all unrealistic that this be very remarkable to everyone else, even someone with the technical and business background of Senator Warner. It would immediately be eyebrow-raising should any of the companies he managed or was invested in suddenly started transacting in Russian Rubles! For a super-aggregator, though, it is not only unremarkable, it is the system working as designed.</p>
<p>This applies to the content of those ads, too: last week, when <a href="https://www.propublica.org/article/facebook-enabled-advertisers-to-reach-jew-haters">ProPublica reported that Facebook enabled anti-Semitic targeting</a>, I told a friend that a similar story would come out about Google within a week; <a href="https://www.buzzfeed.com/alexkantrowitz/google-allowed-advertisers-to-target-jewish-parasite-black">it only took one day</a>. When you makes something <a href="https://stratechery.com/2013/friction/">frictionless</a> — which is another way of describing zero transaction costs — it becomes easier to do <em>everything</em>, both good and evil.</p>
<h4>Regulating the Super-Aggregators</h4>
<p>This should probably be another article — indeed, it’s an article I’ve been working towards for a long time now — but this appreciation of what Super-Aggreagators are, and how it is a Russian propaganda outfit could buy Facebook ads that likely <a href="https://www.law.cornell.edu/uscode/text/52/30121">broke the law</a>, gives insight into a number of principles that should guide people like Senator Warner as they consider potential regulation:</p>
<ul>
<li><strong>Don’t Force the Super-Aggregators to Make Editorial Decisions:</strong> It has been distressing to see how quickly some folks have resorted to insisting that Google and Facebook start having a point-of-view on content on their platforms. The problem is not that they might be effective, but rather that it is inevitable that they will be. I wrote in <a href="https://stratechery.com/2017/manifestos-and-monopolies/">Manifestos and Monopolies</a>:<br />
<blockquote><p>
  My deep-rooted suspicion of Zuckerberg’s manifesto has nothing to do with Facebook or Zuckerberg; I suspect that we agree on more political goals than not. Rather, my discomfort arises from my strong belief that centralized power is both inefficient and dangerous: no one person, or company, can figure out optimal solutions for everyone on their own, and history is riddled with examples of central planners ostensibly acting with the best of intentions — at least in their own minds — resulting in the most horrific of consequences; those consequences sometimes take the form of overt costs, both economic and humanitarian, and sometimes those costs are foregone opportunities and innovations. Usually it’s both.
</p></blockquote>
<p>The best solution in my estimation is enforced neutrality; to the extent limitations are put in place they should be enforced by another entity with far more accountability to the people than either of these Super-Aggregators. That probably means the government (with the obvious caveat that authoritarian governments would certainly prefer to use Facebook for their own ends).</p>
</li>
<li>
<p><strong>Focus on Transparency:</strong> The personalization afforded by Super-Aggregators means their advertising is simply not comparable to anything that has come before: television commercials, radio jingles, newspaper ads, all are publicly disseminated and thus can be tracked (the one possible exception is direct mail, which, unsurprisingly, has been the home of the foulest sort of political advertising in particular). Digital ads, on the other hand, can be shown to a designated audience without anyone else knowing. It is worth debating whether this level of secrecy should be allowed in general; it seems without question, <a href="https://stratechery.com/2017/amazons-second-headquarters-amazons-internal-primitives-facebook-and-political-ads/">in my mind</a>, that it should not be allowed for political ads. Of course, that begs the question of what is a political ad, which again points towards regulation (which, per point one, is preferable to the unaccountable Google and Facebook deciding).</p>
</li>
<li>
<p><strong>Remember the Benefits of Zero Transaction Costs:</strong> The biggest beneficiaries of zero transaction costs on the super-aggregators are not traditional advertisers, whether that be companies like CPG conglomerates or presidential campaigns. Both have the resources to advertise anywhere and everywhere, and indeed, often find that the fine-tooth targeting on super-aggregators isn’t worth the effort required. The folks that do benefit, though, are those that wouldn’t have a voice otherwise: startups and niche offerings, both in terms of business and politics. Google and Facebook have opened the field to far more entrants, and while that means there are more folks with bad intentions, there are also a whole lot more folks with ideas that were shut out by the significant transaction costs inherent in pre-Internet platforms.</p>
</li>
</ul>
<p>There’s one final consideration that should apply to regulation, broadly: given that Google and Facebook are already well-established with businesses that serve users, suppliers, and advertisers in a virtuous cycle, it is unlikely that regulation of any kind will have meaningful effects on their bottom lines. Indeed, I expect Google and Facebook to be mostly cooperative with whatever regulation comes from these recent revelations.</p>
<p>Rather, the companies that will be hurt are those seeking to knock Google and Facebook off their perch; given that they are not yet super-aggregators, they will not have the feedback loops in place to overcome overly prescriptive regulation such that they can seriously challenge Google and Facebook.</p>
<p>For example, consider the much-touted <a href="http://www.opentext.com/campaigns/99challenges/comply-with-regulations-wp?utm_source=google&amp;utm_medium=ppc&amp;utm_campaign=99-gdpr&amp;utm_content=ppc-99-gdpr-text&amp;elqcampaignid=27115&amp;gclid=EAIaIQobChMIrrbT892u1gIVSgoqCh3zTweKEAAYASAAEgLaifD_BwE">General Data Protection Regulation (GDPR)</a> set to take effect in the European Union next year. There is lot of excitement about how this regulation will limit Google and Facebook in particular, by, for example, limiting the use of personal data and enforcing data portability (and not just a PDF of your data — services will be required to build API access for easy export).</p>
<p>The reality, though, is that given that Google and Facebook make most of their money on their own sites, they will be hurt far less than competitive ad networks that work across multiple sites; that means that even more digital advertising money — which will continue to grow, regardless of regulation — will flow to Google and Facebook. Similarly, given that the data portability provisions explicitly exclude your social network — exporting your friends requires explicit approval from your friends — it will be that much harder to bootstrap a competitor.</p>
<p>This is the reality of regulation: as much as the largest incumbents may moan and groan, they are, in nearly all cases, the biggest beneficiaries. To be sure, that doesn’t mean regulation isn’t appropriate — it should be far more obvious to everyone that Russians were purchasing election-related ads on Facebook — but rather that it be expressly designed to limit the worst abuses and enable meaningful competitors, even if they accept payment in Russian Rubles.</p>
<ol><li>For what it’s worth, Stratechery has never actually taken out a Facebook ad, or any ad for that matter</li><li>Yes, I’m writing about Aggregation Theory again; I explain why I do so often <a href="https://stratechery.com/2017/the-platform-paradox-voice-assistants-vulnerable-facebook-and-russian-ads/">here</a> </li><li>Presuming his tweet was not as cynical as it very well might have been</li></ol>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://codewithoutrules.com/2017/09/18/when-startups-pay-less/">Join our startup, we&#39;ll cut your pay by 40%!</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://codewithoutrules.com/atom.xml">Code Without Rules</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">practices</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Mon Sep 18 2017 05:00:00 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>Have you ever thought to yourself, “I need to get paid far far less than I’m worth?”
Me neither.
And yet some companies not only pay less, they’re proud of it.
Allow me to explain—</p>



<p>I recently encountered a job posting from one such startup.
My usual response would be to roll my eyeballs and move on, but this particular posting was so egregious that had I done so I would’ve ended up looking at the back of my skull.</p>

<p>So in an effort to avoid the pain of over-rolled eyeballs, and more importantly to help <em>you</em> avoid the pain of working for this kind of company, let me share the key sentence from the job posting:</p>

<blockquote>
<p>“It’s not unusual to see some team members in the office late into the evening; many of us routinely work and study 70+ hours a week.”</p>
</blockquote>

<p>In this post I will work through the implications of that sentence.
I made sure not to drink anything while writing it, because if I had I’d be spitting my drink out every time I reread that sentence.
The short version is that should you join such a company, you’d be working for people who are:</p>

<ul>
<li>Exploiting you by massively underpaying you.</li>
<li>Destroying your productivity.</li>
<li>Awful at project management.</li>
</ul>

Cutting your salary by 40%

<p>Let’s start with your salary.
The standard workweek in the US is 40 hours a week.
If you’re going to be working 70 hours a week that means you’re working 75% more hours than usual.
<strong>Or, to put it another way, the company is offering to pay you 40% less than market rate for your time.</strong></p>

<p>Instead of hiring more engineers, they’re trying to get their engineers to do far more for the same amount of money.
This is exploitation, and there’s no reason you should put up with it.</p>

<p>It’s not that hard to find companies where you can work a normal 40 hour workweek.
I’ve done so at the past five companies I’ve worked at, ranging from tiny startups to Google.
Sometimes you need to push back, it’s true, but it’s certainly possible.
And even if you can’t find such a job, there are many more companies where you can work 45 hours, or 50 hours.
Even an awful workweek of 60 hours is better than 70.</p>

<h3>When programming is your hobby</h3>

<p>Now, it may be that you love programming so much that you’re thinking, “I’d be coding 70 hours a week anyway, why not do it at work?”
As I’ll mention below, I don’t think working 70 hours a week is going to produce much, but even if it did you still shouldn’t do it on your employer’s behalf.</p>

<p>Let’s imagine you’re coding 70 hours a week.
You could work 70 hours for your employer, getting paid nothing extra for your time, or you could stick to 40 hours and use those remaining 30 hours to:</p>

<ul>
<li>Work on a personal project, just for fun: you could learn new skills that <em>you</em> choose, or build something frivolous because <em>you</em> enjoy it.</li>
<li>Work on an open source project, helping others as well.</li>
<li>Take on consulting work, getting paid more.</li>
<li>Start your own startup, so you get a more significant upside from success.</li>
</ul>

<p>And you’d also have some optional slack time, which is useful when life gets in the way of programming.</p>

“Work not just smart, but also hard”

<p>Encouraging 70 hour workweeks is an extraordinary level of exploitation, but sadly it’s also a rather common form of stupidity.
The problem is encapsulated in another statement from the job posting:</p>

<blockquote>
<p>“[We] work not just smart, but also hard.”</p>
</blockquote>

<p>If your starting point is exploitation, if you’re setting out to extract as much work as possible from your employees, you lose sight of the <em>purpose</em> of work.
Work has no inherent value: what matters is the results.
The problems solved, the value created, this is what you’re trying to maximize.</p>

<p>And is turns out there’s <a href="http://www.igda.org/?page=crunchsixlessons">decades of research</a> showing that consistently working more than 40 hours a week results in <em>less output</em>.
But presumably the people running this startup don’t believe that, or they wouldn’t be pushing for it.
And maybe you don’t believe that either.
But even if we assume 70 hours of work produce 75% more output than 40 hours of work, it’s still a fundamentally bad idea for the company.</p>

<p><strong>When an organization tries to maximize inputs, rather than outputs, the result is a whole series of bad judgments.</strong>
Hiring, for example, as you can see from this job ad.
A junior programmer working 70 hours a week will produce far less valuable output than an experienced programmer working 40 hours a week.
But a company that wants to maximize exploitation, to maximize work, will write job ads that ensure the latter will never apply.</p>

<h3>Emergencies: when long hours <em>are</em> necessary.</h3>

<p>Beyond reduced output, and beyond a confused hiring policy, encouraging long hours also implies a lack of project management skills.
Long work hours are both a cause and a symptom of this particular failure.</p>

<p>70 hours a week means 7 days a week, from 9AM to 7PM.
That doesn’t leave much slack time for life, and it also leaves no slack time for the project.
Sooner or later every project has an emergency.
If a production server crashes, someone is going to have to bring it back up.
And more broadly, extra work comes up: a customer asks for more features, or a seemingly simple task turns out to be far more difficult than expected.</p>

<p>To help deal with these situations you need some advance planning.
Scheduling everything down to the minute won’t help, and pushing everyone to work at the absolute limit won’t help.
The problem is <em>unexpected</em> work, after all.
What you need is planned slack time, time that hasn’t been budgeted, that’s available for all the inevitable unexpected problems.</p>

<p><strong>But a manager that is pushing you to work 70 hours a week isn’t a manager who plans ahead for unexpected work.</strong>
No, this is a manager who solves problem by telling you to work harder and longer.
So when the unexpected happens, when an emergency happens, your manager will be saying “who coulda knowed? ¯\_(ツ)_/¯” and before you know it you’re working 80 hours a week.</p>

<p>Maybe that will fix things.
But I doubt it.
More plausibly you’ll eventually burn out and quit, taking your business knowledge with you.</p>

“Strong willingness to help junior engineers”

<p>The job posting that led to this post also suggested that a “strong willingness to help junior engineers” would be helpful, though not required.
So here’s my advice to all you junior engineers out there: avoid companies that want you to work crazy hours.</p>

<ol>
<li>It’s bad for you.</li>
<li>It’s bad for the company.</li>
<li>And you don’t want to work for a manager who isn’t competent enough to realize what’s bad for the company.</li>
</ol>

<p>And if you are stuck working for such a company, you might want to read my book, <em><a href="/saneworkweek/">The Programmer’s Guide to a Sane Workweek</a></em>.</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://eklitzke.org/resurrecting-the-sram-automatix">Resurrecting the SRAM Automatix</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://eklitzke.org/atom.xml?type=blog">Evan Klitzke</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">compsci</span>
              <span class="tag">unix</span>
              <span class="tag">python</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Mon Sep 18 2017 01:00:00 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            Two years ago I wrote about my <a href="/mission-bicycles-automatix">SRAM Automatix hub
bicycle</a>. I’ve owned a lot of bikes, but this
particular bike has a special place in my heart. It’s the bike I’ve had the most
fun riding, and the ...
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://www.nngroup.com/articles/mobile-tables/">Mobile Tables: Comparisons and Other Data Tables</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://www.nngroup.com/feed/rss/">Nielsen Norman Group</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">ux</span>
              <span class="tag">company</span>
            </div>
            <span class="subtitle is-7">Sun Sep 17 2017 17:00:00 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p><strong>Summary:</strong> Locking headers and allowing users to select a subset of data according to their needs make large data tables usable on mobile devices. </p><hr /><br /><p> Displaying big data on a small screen is a daunting challenge. How do we make a large amount of data fit on a small screen? Other than limiting the number of rows or columns of data, what other options are available to display tables on small screens? What does a usable table look like on mobile?</p><p> (The general problem of having more data than fits on the screen is not specific to mobile: information-visualization researchers have struggled for years with the topic of showing data that has more columns (or rows) that can fit on the user’s screen — be it a large monitor, an array of monitors, or a small mobile display. However, <strong>  the smaller the screen, the more likely that we’ll get into trouble </strong> with any given dataset and will need to design carefully to minimize usability problems.)</p><p> In our article on <a href="https://www.nngroup.com/articles/comparison-tables/">  comparison tables </a> , we covered key elements in presenting data, such as the need for <strong>  consistency of content </strong> and <strong>  presenting meaningful attributes </strong> to users. Both are equally, if not even more important for mobile tables, due to the small amount of data visible at one time. You must <strong>  first create a usable table for a large screen </strong> before translating it to a small one. The need to make a table work on a smaller screen may be a good reason, excuse, or impetus to reevaluate the content and attributes in your table, regardless of screen size, and to improve content for all users.</p><br /><br /><a href="/articles/mobile-tables/">Read Full Article</a>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://www.nngroup.com/articles/ux-research-goals-to-scenarios/">From Research Goals to Usability-Testing Scenarios: A 7-Step Method</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://www.nngroup.com/feed/rss/">Nielsen Norman Group</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">ux</span>
              <span class="tag">company</span>
            </div>
            <span class="subtitle is-7">Sun Sep 17 2017 17:00:00 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p><strong>Summary:</strong> Developing goals for a usability study, deciding what to test, and crafting user scenarios can be challenging. This method makes the process straightforward.</p><hr /><br /><p> For some usability studies, there may be an obvious and limited set of interface elements that should be tested. In many cases, however, the system is complex, new to the researcher, or the list of candidate features for testing is long. This article shows a 7-step method that helps everyone focus on the most important elements to test first, while also prioritizing everything of concern for later research.</p> The 7 Steps<p> Our <a href="https://www.nngroup.com/articles/usability-test-checklist/">  checklist for planning usability studies </a> describes the larger process.</p><p> It’s important to involve stakeholders in these planning steps. After conducting user studies, researchers can sometimes encounter resistance to the findings: someone says the researcher <a href="https://www.nngroup.com/articles/recruiting-test-participants-for-usability-studies/">  recruited the wrong participants </a> or tested the wrong elements. Going through part of the test-planning process as a team ensures that everyone understands and supports the research goals, priorities, and methods.</p><br /><br /><a href="/articles/ux-research-goals-to-scenarios/">Read Full Article</a>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://jvns.ca/blog/2017/09/17/how-i-spent-my-time-at-the-recurse-center/">How I spent my time at the Recurse Center</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://jvns.ca/atom.xml">Julia Evans</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">practices</span>
              <span class="tag">linux</span>
              <span class="tag">networking</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Sun Sep 17 2017 13:43:22 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>I went to the Recurse Center a while ago now (4 years ago) but I’m interested in the topic of “how
do I accomplish a lot in a short amount of time” again so I wanted to revisit how I spent my time
there because I learned a lot. I don’t know if this will be interesting to anyone but me but I often
think about things by blogging, so here you go :)</p>

<p>The <a href="https://www.recurse.com/">Recurse Center</a> is a place where you go for 12 weeks and spend all
your time working on getting better at programming. To learn more about it I recommend the <a href="https://www.recurse.com/manual">user’s manual</a>.</p>

<p>The days are numbered 1-46 because when I was there Monday-Thursday were the only “mandatory” days.
So 12 weeks is 48 days, minus 2 for some reason.</p>

<p>Here’s how I spent those 46 days (in order). (I know this because I <a href="https://jvns.ca/categories/hackerschool/">wrote a blog post every day I was there</a>). Some of this is probably <a href="https://en.wikipedia.org/wiki/Retroactive_continuity">retconned</a> at least a little bit :)</p>

<ul>
<li><strong>Day 1-3</strong>: Work on a tiny shell in bash (<a href="https://github.com/jvns/_dash/blob/master/dash.c">here’s the source</a>)</li>
<li>meanwhile: Learn what a shell does. Learn some basics of the Linux kernel does. Tom teaches me about netcat.
Netcat is awesome, wow.</li>
<li><strong>Day 4-6</strong>: Learn what a Linux kernel module is. Write a couple of beginner kernel modules.</li>
<li><strong>Day 7</strong>: Consider learning Clojure / writing a bittorrent client in Clojure. Write a clojure echo
server &amp; decide against continuing down that path</li>
<li><strong>Day 8-10</strong>: Pair with Allison on her bytecode interpreter, go to a talk on Julia &amp; open source, feel
confused about what I’m going to do next</li>
<li><strong>Day 11-16</strong>: Implement gunzip in Julia and demo it (<a href="https://jvns.ca/blog/2013/10/24/day-16-gzip-plus-poetry-equals-awesome/">gzip + poetry = awesome</a>). <a href="https://github.com/jvns/gzip.jl">https://github.com/jvns/gzip.jl</a>.</li>
<li><strong>Day 17-18</strong>: Read some of <a href="https://www.nostarch.com/hacking2.htm">Hacking : The Art of Exploitation</a>. (which is a GREAT BOOK). Do some experimentation with buffer overflows &amp; ARP cache poisoning.</li>
<li><strong>Day 19-20</strong>: Get interested in networking. Talk to Jari &amp; Brian about networking. Implement
traceroute in Python</li>
<li><strong>Day 21-27</strong>: Work on a TCP stack in Python. (<a href="https://github.com/jvns/teeceepee">https://github.com/jvns/teeceepee</a>)</li>
<li><strong>Day 28-29</strong>: Philip Guo is a resident. Inspired to make <a href="https://visualize-your-git.herokuapp.com/">https://visualize-your-git.herokuapp.com/</a>.
(web projects were against my personal rules for being at the recurse center but I did it anyway)</li>
<li><strong>Day 30-33</strong>: Have fun with making music with Clojure. Write a webserver with Lyndsey that lets
a crowd of people play music on your computer (<a href="https://github.com/jvns/magical-orchestra">https://github.com/jvns/magical-orchestra</a>)</li>
<li><strong>Day 34-46</strong>: Work on writing a tiny operating system in Rust (<a href="https://github.com/jvns/puddle">https://github.com/jvns/puddle</a>)</li>
</ul>

<p>Overall:</p>

<ul>
<li>13 days on writing a mini Rust OS (500 lines of Rust). <a href="https://github.com/jvns/puddle">https://github.com/jvns/puddle</a></li>
<li>9 days on learning about networking &amp; writing a TCP stack in Python (200 lines of Python + tests). <a href="https://github.com/jvns/teeceepee">https://github.com/jvns/teeceepee</a></li>
<li>6 days on gzip in Julia (360 lines of Julia). <a href="https://github.com/jvns/gzip.jl">https://github.com/jvns/gzip.jl</a></li>
<li>3 days on writing a shell in C (250 lines of C, with Daphne). <a href="https://github.com/jvns/_dash">https://github.com/jvns/_dash</a></li>
<li>3 days writing fun kernel modules (200 lines of C). <a href="https://github.com/jvns/kernel-module-fun">https://github.com/jvns/kernel-module-fun</a></li>
<li>2 days on a git workflow visualization tool (150 lines of Python). <a href="https://github.com/jvns/git-workflow">https://github.com/jvns/git-workflow</a></li>
<li>5 days on Clojure &amp; having fun making music in Clojure (250 lines of Clojure, with Lyndsey). <a href="https://github.com/jvns/magical-orchestra">https://github.com/jvns/magical-orchestra</a></li>
<li>2 days reading a hacking book and experimenting with buffer overflows / ARP cache poisoning. (no code)</li>
<li>3 days not working on any specific project</li>
</ul>

<p>adds up to a Recurse Center batch!</p>

<p>To be clear I don’t think that it’s <strong>necessary</strong> to spend all your time working on a
Specific Project. My partner went to RC and spent most of his time not working on any specific
project and still got a lot of out of it. People spend their time at RC in totally different ways
and that’s okay!</p>

<p>some observations:</p>

<ul>
<li>I wrote maybe 50-100 lines of code a day on average.</li>
<li>I didn’t like it when I wasn’t working on a “project”.</li>
<li>I was pretty comfortable with web development / machine learning basics before RC, so I completely
avoided working on those things. I focused on stuff I thought seemed hard/scary (networking/security/operating
systems/writing a shell/compression). Clojure / the git visualization tool were exceptions to this, those I just thought were fun.</li>
<li>The stuff I learned about at RC 4 years ago is still a lot of the same stuff I’m excited about
today.</li>
<li>There were a lot of things I worked on for only 2-3 days (like “write kernel modules”). Some of
those things I learned a LOT from.</li>
<li>I spent almost all of my time (all except 5 days) working on projects I could demo and talk about
easily. But nothing I made was really that polished or anything.</li>
<li>I was pretty concerned with getting a job after, a big part of why I
blogged about what I was learning was that I wanted to get a cool job after the Recurse Center and
so my blog was my “media strategy”. I spent 1-2 hours a day writing (which was a lot) but I did
get a cool job after.</li>
<li>on the other hand I was lucky that I didn’t really <em>need</em> to get a job immediately after RC, I
could have easily afforded to spend a few months job hunting. So I had space to focus on
learning/programming.</li>
<li>all the people at RC were really amazing (other recursers / residents / facilitators). For example
Lindsey encouraged me to work on writing an OS in Rust and that turned out to be a great idea. And
the facilitators were always extremely enthusiastic/positive  about helping me debug weird problems I had.</li>
<li>Today I have a really positive attitude about debugging (what are we going to learn TODAY?!). I
think I learned that from the RC facilitators &lt;3 (like I’d ask Allison “hi can you help me debug
this weird thing” and she would be SO HELPFUL)</li>
</ul>

<p>I think this media strategy approach (“everything I do has to be a cool thing I can demo and write
about”) thing is pretty weird and I don’t know that I recommend it. It worked for me though, I think
it maybe helped keep me focused/motivated. I think “blog every day” isn’t actually an approach that
works for most people though :).</p>

<h3>RC is awesome</h3>

<p>looking back I think some of the most important things I learned at RC were:</p>

<ul>
<li>debugging is fun and interesting</li>
<li>systems/networking/linux are extremely cool and not really that hard to get started with even if
you only know a little bit of C. Like Tom told me about netcat on Day 2 and I am still VERY
EXCITED about networking :)</li>
<li>I learned some about unknown-unknowns – things that I didn’t know existed and wouldn’t really
have thought to ask about (strace! netcat! system calls!)</li>
<li>writing about what I’m learning is really fun</li>
</ul>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://www.fewbutripe.com/swift/functional/programming/2017/09/17/announcing-point-free.html">Announcing Point-Free</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="http://www.fewbutripe.com/feed.xml">Few, but ripe...</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">compsci</span>
              <span class="tag">web</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Sun Sep 17 2017 01:00:00 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <blockquote>
  <p>A soon-to-be launching <a href="https://www.pointfree.co/">weekly video series</a> exploring Swift and functional programming!</p>
</blockquote>

<p>For the past 3 years, since Swift’s early days, I have been a strong proponent of adopting the techniques of functional programming to improve the understandability and maintainability of Swift code. I co-organize the <a href="http://www.funswiftconf.com/">Functional Swift Conference</a>, I write extensively on my <a href="http://www.fewbutripe.com/">site</a>, give <a href="http://www.fewbutripe.com/talks/">talks</a> whenever I can, and <a href="https://kickstarter.engineering/open-sourcing-our-android-and-ios-apps-6891be909fcd">open-sourced</a> the iOS and Android code bases at Kickstarter to show how these techniques can be used in a real-life code base. However, functional programming often comes across as foreign and intimidating, and so there’s always more work that can be done to make it approachable!</p>

<p>So, with my former colleague from Kickstarter <a href="http://www.stephencelis.com/">Stephen Celis</a>, we are launching <a href="https://www.pointfree.co/">Point-Free</a>, a weekly video series exploring Swift and functional programming.</p>

<p></p>

<p>We learned quite a bit while making this site! From the beginning we knew we wanted to make the site using server-side Swift, and we wanted to approach it in a functional way. We wanted the server to just be a pure function that takes a request and outputs a response. We wanted all the side-effects to be pushed to the boundaries of the application, and then be interpreted in an understandable and testable manner. And we wanted to embrace views as <a href="http://www.fewbutripe.com/swift/html/dsl/2017/06/29/composable-html-views-in-swift.html">composable, stateless functions</a>.</p>

<p>Turns out, if you accomplish all of the above, all types of fun stuff starts popping out. First, you get to easily write tests that traverse the full stack of the application and make assertions on every little thing that happened along the way. Then, because the server is just a pure function, you can easily load it up in a Swift Playground and pipe requests through, including <code>POST</code> requests!</p>

<p></p>

<p>And finally, we developed an all-purpose <a href="https://www.github.com/pointfreeco/swift-snapshot-testing">snapshot testing library</a> so that we could take full snapshots of HTML text <em>and</em> screenshots of pages at different browser sizes.</p>

<p>We are so excited with our findings that we decided to open-source the whole collection of libraries we made that aid in developing server-side Swift. We are also open-sourcing the full source code of the site itself! You can find all of the repositories on the <a href="https://github.com/pointfreeco">Point-Free GitHub organization</a>, but here are some of the interesting things you will find:</p>

<ul>
  <li><a href="https://github.com/pointfreeco/pointfreeco">github/pointfreeco</a>: The source to the full www.pointfree.co site! It includes all of the routing and server middleware, HTML views, CSS styling and business logic.</li>
  <li><a href="https://github.com/pointfreeco/pointfreeco-server">github/pointfreeco-server</a>: A barebones Kitura application that delegates all of the request-to-response lifecycle responsibilities to pointfreeco.</li>
  <li><a href="https://github.com/pointfreeco/swift-web">github/swift-web</a>: A collection of types and functions that aid in server-side developing, including HTML/CSS creation and rendering, request routing and server middleware.</li>
  <li><a href="https://github.com/pointfreeco/swift-prelude">github/swift-prelude</a>: Offers a standard library for experimental functional programming in Swift.</li>
  <li><a href="https://github.com/pointfreeco/swift-snapshot-testing">github/swift-snapshot-testing</a>: A full-featured snapshot testing library to capture a data structure as text, image or anything, and make assertions against reference data.</li>
</ul>

<p><br /></p>

<hr />
<p><br /></p>

<p>So, this is what I’ve been up to since <a href="https://twitter.com/mbrandonw/status/874683027464622081">leaving Kickstarter</a> a few months ago! We hope to have our first episodes live by the end of this year. Please consider <a href="https://www.pointfree.co/">signing up</a> to show us how much interest is out there for such a thing, and to be notified when we launch!</p>

<p>Also, if any of this interests you and you want to collaborate on some work together, I’m available <a href="http://fewbutripe.com/hire-me/">for hire</a>.</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://ponyfoo.com/articles/tiny-story-about-complexity">A Tiny Story about Systems Complexity</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://feeds.feedburner.com/ponyfoo">Pony Foo</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">javascript</span>
              <span class="tag">web</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Sun Sep 17 2017 00:10:01 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <div><p>A human decides to open a new tab in their favorite web browser and they then google for <a href="https://www.google.com/search?q=cat+in+a+pickle+gifs&amp;tbm=isch" target="_blank"><em>“cat in a pickle gifs”</em></a>. What happens next will shock you!</p><p>The browser allocates a new process through a system call to the operating system, which shifts some bits around on the physical hardware that lies inside the human’s computer. Before the HTTP request hits the network, we need to hit DNS servers, engaging in the elaborate process of casting <code>google.com</code> into an IP address. The browser then checks whether there’s a ServiceWorker installed, and assuming there isn’t one the request finally takes the default route of querying Google’s servers for the phrase <em>“cat in a pickle gifs”</em>.</p> <p>Naturally, Google receives this request at one of the front-end edges of its public network, in charge of balancing the load and routing requests to healthy back-end services. The query goes through a variety of analyzers that attempt to break it down to its semantic roots, stripping the query down to its essential keywords in an attempt to better match relevant results.</p> <p>The search engine figures out the 10 most relevant results for <em>“cat pickle gif”</em> out of billions of pages in its index – which was of course primed by a different system that’s also part of the whole – and at the same time, Google pulls down a highly targeted piece of relevant advertisement about <em>cat gifs</em> that matches what they believe is the demographic the human making the query belongs to, thanks to a sophisticated ad network, figures out whether the user is authenticated with Google through an HTTP header session cookie and the search results page starts being constructed and streamed to the human, who now appears impatient and fidgety.</p><p>As the first few bits of HTML being streaming down the wire, the search engine produces its results and hands them back to the front-end servers, which includes it in the HTML stream that’s sent back to the human. The web browser has been working hard at this too, parsing the incomplete pieces of HTML that have been streaming down the wire as best it could, even daring to launch other admirably and equally-mind-boggling requests for HTTP resources presumed to be JavaScript, CSS, font, and image files as the HTML continues to stream down the wire. The first few chunks of HTML are converted into a DOM tree, and the browser would finally be able to begin rendering bits and pieces of the page on the screen, if it weren’t because it’s still waiting on those equally-mind-boggling CSS and font requests.</p> <p>As the CSS stylesheets and fonts are transmitted, the browser begins modeling the CSSOM and getting a more complete picture of how to turn the HTML and CSS plain text chunks provided by Google servers into a graphical representation that the human finds pleasant. Browser extensions get a chance to meddle with the content, removing the highly targeted piece of relevant advertisement about cat gifs before I even realize Google hoped I wouldn’t block ads this time around.</p> <p>A few seconds have passed by since I first decided to search for <em>cat in a pickle gifs</em>. Needless to say, thousands of others brought similarly inane requests. To the same systems. During this time.</p> <p>Not only does this example demonstrate the marvelous machinery and infrastructure that fuels even our most flippant daily computing experiences, but it also illustrates how abundantly hopeless it is to make sense of a system as a whole, let alone its comprehensive state at any given point in time. After all, where do we draw the boundaries? Within the code we wrote? The code that powers our customer’s computers? Their hardware? The code that powers our servers? Its hardware? The internet as a whole? The power grid?</p> <p>The overall state of a system has little to do with our ability to comprehend parts of that same system. Our focus in reducing state-based entropy must then lie in the individual aspects of the system. It’s for this reason that breaking apart large pieces of code is so effective. We’re reducing the amount of state local to each given aspect of the system, and that’s the kind of state that’s worth taking care of, since it’s what we can keep in our heads and make sense of.</p></div>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://www.fourmilab.ch/fourmilog/archives/2017-09/001712.html">Tom Swift and His Electric Runabout updated, EPUB added</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://www.fourmilab.ch/fourmilog/atom_10.xml">Fourmilog - Fourmilab Change Log</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Sat Sep 16 2017 00:34:41 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            All 25 of the public domain Tom Swift novels have been posted in the <a href="/etexts/www/appleton/" target="Fourmilog_Aux">Tom Swift and His Pocket Library</a> collection. I am now returning to the earlier novels, upgrading them to use the more modern typography of those I've done in recent years. The fifth novel in the series, Tom Swift and His Electric Runabout, has now been updated. Several typographical errors in the original edition have been corrected, Unicode text entities are used for special characters such as single and double quotes and dashes, and the HTML version is now XHTML 1.0 Strict.

<p>

An <a href="https://en.wikipedia.org/wiki/EPUB" target="Fourmilog_Aux">EPUB</a> edition of this novel is now available which may be downloaded to compatible reader devices; the details of how to do this differ from device to device—please consult the documentation for your reader for details.

</p><p>

Tom Swift's electric car, described in this 1910 novel, compares quite favourably with those on the market more than a century later.  Top speed was one hundred miles an hour, and in chapter four Tom says of range on a battery charge, “Well, if I can make it do three hundred miles I'll be satisfied, but I'm going to try for four hundred.”  But in case the battery runs down and there's no charging station nearby, Tom has a trick up his sleeve even Elon Musk can't exploit.  Asked by his father, “Suppose you're not near a charging station?”, Tom replies, “…I'm going to have it fixed so I can take current from any trolley line, as well as from a regular charging station.  My battery will be capable of being recharged very quickly, or, in case of need, I can take out the old cells and put in new ones.”

</p><p>

In 1910, <a href="https://en.wikipedia.org/wiki/Interurban" target="Fourmilog_Aux">interurban trolley</a> lines were ubiquitous in the eastern United States, so the intrepid motorist, seeing the charge gauge creeping down toward zero, need only divert to the nearest trolley line, hook up to the rail and overhead wire or third rail, and before long everything would be just tickey-boo.  No thief, the young Swift remarks, “ ‘I'm going to pay for the current I use,’ explained the young inventor.  ‘I have a meter which tells how much I take.’ ”</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://papers-we-love.github.io/2017/video/wes-chow-off-the-record-communication/">Wes Chow on Off-the-Record Communication, or, Why Not To Use PGP</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="http://paperswelove.org/feed.xml">Papers We Love</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">compsci</span>
              <span class="tag">community</span>
            </div>
            <span class="subtitle is-7">Fri Sep 15 2017 18:00:09 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            

<p></p>

New York - July 19, 2017

<hr />

<ul>
  <li><strong>Meetup:</strong> <a href="https://www.meetup.com/papers-we-love/events/241525902/">https://www.meetup.com/papers-we-love/events/241525902/</a></li>
  <li><strong>Paper:</strong> <a href="https://otr.cypherpunks.ca/otr-wpes.pdf">Off-the-Record Communication, or, Why Not To Use PGP</a></li>
  <li><strong>Slides:</strong> <a href="https://speakerdeck.com/paperswelove/wes-chow-on-off-the-record-communication-or-why-not-to-use-pgp">Wes Chow on Off-the-Record Communication, or, Why Not To Use PGP</a></li>
  <li><strong>Audio:</strong> <a href="https://www.mixcloud.com/paperswelove/wes-chow-on-off-the-record-communication-or-why-not-to-use-pgp/">Wes Chow on Off-the-Record Communication, or, Why Not To Use PGP</a></li>
</ul>

<p><strong>Description</strong></p>

<p>Your intrepid reporter goes to a private location and meets with a key source who wishes to remain anonymous and off the record. The reporter understands that all information she learns from the source must be validated elsewhere and not directly quoted (private), that the source is who he says he is (authenticated), and that should their conversation become public they could both plausibly deny having said any of the recorded words (repudiable). How do we construct a digital version of an IRL meeting?</p>

<p>Nikita Borisov, Ian Goldberg, and Eric Brewer devise a communication protocol in Off-the-Record Communication, or Why Not To Use PGP that provides all of the above mentioned properties, as well as forward-secrecy (breaking the encryption on one message doesn’t give an attacker keys to past or future messages). Wes will review the OTR protocol and its clever collection of strong and purposefully weak cryptographic techniques that form the basis of the <a href="https://whispersystems.org/">Signal</a> private messaging app.</p>

<p><strong>Bio</strong></p>

<p>Wes Chow (<a href="https://twitter.com/weschow">@weschow</a>) has a B.S. in Electrical Engineering &amp; Computer Science from UC Berkeley. He spent eight years building technical infrastructure for high frequency trading shops. One day, he stared into his dark soul and realized he needed to move into the startup light. Thus S7 Labs sprang into being, and he led teams that built Storybox, a Seedcamp NY finalist, and Songza Radio, subsumed by Google Music. He's now at Chartbeat serving out his term as CTO.</p>

<p><strong>Audio</strong></p>



<p><strong>Slides</strong></p>



<hr />

<p>
<a href="https://www.twosigma.com/"></a> The <strong>New York Chapter</strong> would like to thank <a href="https://www.twosigma.com">TwoSigma</a> for helping to make this meetup possible.
</p>

<hr />
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://feedproxy.google.com/~r/TroyHunt/~3/J7KGW08UEPY/">Weekly update 52</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://feeds.feedburner.com/TroyHunt">Troy Hunt</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">security</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Fri Sep 15 2017 08:44:40 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p><a href="http://www.symantec.com/ready-secure-go/it-smb/?om_ext_cid=ws_ad_TROY_DBC-Bulldog_IT-SMB_Encrypt"><strong>Presently sponsored by:</strong> Get a security solution that will keep your website up and running—and keep you sleeping soundly: Symantec Website Security. Learn how</a></p><div><p>Hey, it's weekly update 52! That's <em>almost</em> a year's worth of weekly videos, next week will actually be that anniversary (ok, it's a day short, but close) and by that time I'll be over in Utah doing the <a href="https://www.pluralsight.com/event-details/2017/live-2017">Pluralsight Live</a> thing. I'm especially looking forward to this event, there's a huge amount of organisation gone into it and I think it'll be a really slick show.</p>
<p>This week - Equifax. Wow. It's such a mess on so many levels and as I say in the Security Sense column, trust is now a massive problem not just because of the breach itself, but because of how they've subsequently handled it. Apple's new toys and Face ID is another really interesting topic this week, primarily because of the security implications of biometrics in general and their new implementation in particular. Enjoy this week, next week will be something new altogether I'm sure.</p>
<p><a href="https://itunes.apple.com/au/podcast/troy-hunts-weekly-update-podcast/id1176454699">iTunes podcast</a> | <a href="https://goo.gl/app/playmusic?ibi=com.google.PlayMusic&amp;isi=691797987&amp;ius=googleplaymusic&amp;link=https://play.google.com/music/m/If3tw7npymckucxq4q76762ncny?t%3DTroy_Hunt%27s_Weekly_Update_Podcast">Google Play Music podcast</a> | <a href="http://www.omnycontent.com/d/playlist/1439345f-6152-486d-a9c2-a6bf0067f2b7/3ba9af7f-3bfb-48fd-aae7-a6bf00689c10/fde26e49-9fb8-457d-8f16-a6bf00696676/podcast.rss">RSS podcast</a></p>

References
<ol>
<li><a href="http://windowsitpro.com/security/security-sense-trust-problem-equifax">Equifax has some serious problems...</a> (yeah, I know you know, but the trust angle is really important beyond just the technology itself)</li>
<li><a href="https://www.troyhunt.com/face-id-touch-id-pins-no-id-and-pragmatic-security/">Face ID is coming and it will change the world!</a> (ok, maybe not, but it's really interesting to look at how it compares to Touch ID, PINs and no ID as it relates to security)</li>
<li><a href="https://terbiumlabs.com/matchlight.html">Matchlight by Terbium Labs is sponsoring me this week</a> (another repeat sponsor, thanks guys!)</li>
</ol>
</div>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://8thlight.com/blog/sarah-sunday/2017/09/15/aws-dos-and-donts.html">AWS Do&#39;s and Don&#39;ts</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://8thlight.com/blog/feed/atom.xml">8th Light Blog</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">practices</span>
              <span class="tag">principles</span>
              <span class="tag">company</span>
            </div>
            <span class="subtitle is-7">Fri Sep 15 2017 06:00:00 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>So.</p>

<p>You’re <a href="https://8thlight.com/solutions/cloud-migration-aws/">migrating to the cloud</a>.</p>

<p>Simple, right? Everything is basically the same. You've got a server with an operating system of your choice. Nothing’s really different. Just launch an EC2 instance on AWS, do everything you’ve always done, and <strong>boom</strong>. Done. You’re certifiably <em>in the cloud</em>.</p>

<p>Yeah. No. That’s not how it works.</p>

<p>Using EC2 for everything—database, load balancers, Redis, and so on—isn’t using AWS or the cloud effectively. You’re just paying for servers that happen to be hosted by Amazon. By using the most basic AWS cloud offering, you’re limiting your potential and neglecting the power of AWS and the cloud.</p>

<p>By all means, you can work it like that. But it’s not necessarily going to be cost-effective, nor is it the most efficient way of using AWS.</p>

<p>So you may ask, how should I be using AWS, if not in the way described above?</p>

<p>Well, it’s very simple.</p>

<p><strong>Actually use AWS for all it's worth.</strong></p>

<p>Some concrete examples:</p>

<ul>
<li><p><strong>DO:</strong> Use RDS instead of putting a database on an EC2 instance. Amazon tells you to do that themselves!
</p></li>
<li><p><strong>DON'T:</strong> Make your security groups open to the world.</p></li>
<li><p><strong>DO:</strong> Open only certain ports to certain groups or IP addresses in your security groups.</p></li>
<li><p><strong>DON'T:</strong> Use a programmatic load balancer when you could use an Elastic Load Balancer.</p></li>
<li><p><strong>DO:</strong> Use <a href="https://aws.amazon.com/certificate-manager/">Amazon Certificate Manager</a> to make websites and applications more secure over HTTPS. It’s free! Just put in the domain(s) you want a cert for and Amazon will send a verification email. Click the link and then associate the cert with the Elastic Load Balancer or Elastic Beanstalk stack, among other available AWS services, of your choice, and then you’re good to go.</p></li>
<li><p><strong>DO:</strong> Enable <a href="http://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html">CloudWatch alarms</a> for your resources.</p></li>
<li><p><strong>DON'T:</strong> Assume AWS will tell you when things break.</p></li>
<li><p><strong>DO:</strong> Make backups! (In general). There are a variety of ways to take backups of your resources in AWS. For example, RDS can automatically create snapshots for you, you can manually create EBS snapshots on the web console, and you can schedule EBS snapshots <a href="http://docs.aws.amazon.com/AmazonCloudWatch/latest/events/TakeScheduledSnapshot.html">using Cloudwatch</a>.</p></li>
<li><p><strong>DO:</strong> Use names for resources! Can you imagine how hard it would be finding something if it isn’t named properly? Don’t imagine it. <strong>Avoid naming things vaguely or not naming them at all</strong>!</p></li>
<li><p><strong>DON'T:</strong> Rely on IP addresses on servers. If you need a static IP address for a server, assign an <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html">Elastic IP</a> to it. Otherwise, if you stop and start the server, the IP address will change.</p></li>
<li><p><strong>DO:</strong> Look into Amazon’s own AMIs for your servers. They aren’t as costly as some other AMIs provided by third parties, and they also come bundled with basic software.</p></li>
<li><p><strong>DON'T:</strong> Be complacent. AMIs can be taken off the marketplace, and if you want to spin up a copy server, you may have to try another AMI, which might be problematic. Make sure that your AMIs are all still available.</p></li>
<li><p><strong>DO</strong>: Alternatively, research if making your own AMI is right for your software stack. Instead of installing software to be used by your app every time you spin up an instance, bake it into a custom AMI to save deploy time and to not run into issues with AMIs being taken off the marketplace.</p></li>
<li><p><strong>DON'T:</strong> Spend time developing deployment scripts for simple applications when you could just use <a href="https://aws.amazon.com/elasticbeanstalk/">Elastic Beanstalk</a> instead.</p></li>
<li><p><strong>DO:</strong> If you can't use ElasticBeanstalk, still invest in some form of deployment scripts and configuration management! Yes, you can just copy a server’s data and make a new one, but if you want to scale or be nimble in dealing with outages or whatnot, having a deployment framework in place is essential.</p></li>
<li><p><strong>DO:</strong> Make your servers disposable. You should be able to spin up new ones and replace old ones within a short period of time. If a server has hardware maintenance at a period of time, and you can’t handle that time frame, you need to be able to adapt and put something else in place.</p></li>
</ul>

<p>The list can go on and on and on.</p>

<p>Some of it is just good DevOps practice, which Amazon doesn’t do for you (excluding some aspects of Elastic Beanstalk). Put Continuous Integration in place, document, enable logging and alarms, and have secure applications. That's what you should be doing anyway. AWS doesn't change that.</p>

<p>The rest of it is just being savvy about AWS resources. Comparing cost and choosing which options are best for your business and application. Basically, being a smart consumer. AWS is one super store of options, and not all of them are created equal. Be a discerning buyer. Look at a wide selection of options instead of going to what feels familiar. Different can be an improvement and, importantly, cheaper.</p>

<p>AWS has so much potential to change a business. But it doesn't just come on a silver platter named Elastic Compute Cloud. You have to <em>work</em> to integrate it into your app ecosphere. Only then can you really be in the cloud.</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://prestonbyrne.com/2017/09/14/prog-rock-friday/">Prog Rock Friday</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://prestonbyrne.com/feed/">Preston Byrne</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">legal</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Fri Sep 15 2017 05:16:33 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            There is an inexpressibly sad story re: the prog rock mega-group Yes in the news this week. I won’t go into here, save to say it prompted me to look back at Steve Howe’s guitar work for adolescent Yes in the 1970s. Please find below a track from Yessongs, Yes’ first live album, released in 1973 (although recorded in 1972). The song is Yours is No Disgrace (from the Yes Album). I also include a video recording of the group performing the same song for your viewing enjoyment. What we hear and see is Howe just beginning to wield the full might...
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://bradfrost.com/blog/link/is-there-any-value-in-people-who-cannot-write-javascript/">Is there any value in people who cannot write JavaScript?</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="http://feeds.feedburner.com/brad-frosts-blog">Brad Frost</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">design</span>
              <span class="tag">web</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Thu Sep 14 2017 22:02:09 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>This is a great post from Mandy Michael, and touches on some of the things I talked about in my post about <a href="http://bradfrost.com/blog/post/full-stack-developers/">full-stack developers</a>.</p>
<blockquote><p>What I am very concerned about is that many still don’t see value in being skilled in CSS &amp; HTML. This attitude is something I just don’t understand. All of us working together provide value in our industry. HTML &amp; CSS are very important pieces of this puzzle, and I (perhaps naively) thought we had evolved to a point where we were starting to appreciate the challenges each of us face in our different areas of expertise. I guess I was wrong because this attitude is still clearly still prevalent.</p></blockquote>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://www.scottaaronson.com/blog/?p=3445">My Big Numbers talk at Festivaletteratura</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://www.scottaaronson.com/blog/?feed=rss2">Shtetl-Optimized</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">quantum</span>
              <span class="tag">compsci</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Thu Sep 14 2017 11:11:14 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>Last weekend, I gave a talk on big numbers, as well as a Q&amp;A about quantum computing, at <a href="http://www.festivaletteratura.it/en">Festivaletteratura</a>: one of the main European literary festivals, held every year in beautiful and historic <a href="https://en.wikipedia.org/wiki/Mantua">Mantua</a>, Italy.  (For those who didn’t know, as I didn’t: this is the city where Virgil was born, and where Romeo gets banished in <em>Romeo and Juliet</em>.  Its layout hasn’t substantially changed since the Middle Ages.)</p>
<p>I don’t know how much big numbers or quantum computing have to do with literature, but I relished the challenge of explaining these things to an audience that was not merely “popular” but humanisitically rather than scientifically inclined.  In this case, there was not only a math barrier, but <em>also</em> a language barrier, as the festival was mostly in Italian and only some of the attendees knew English, to varying degrees.  The quantum computing session was live-translated into Italian (the challenge faced by the translator in not mangling this material provided a lot of free humor), but the big numbers talk wasn’t.  What’s more, the talk was held outdoors, on the steps of a cathedral, with tons of background noise, including a bell that loudly chimed halfway through the talk.  So if my own words weren’t simple and clear, forget it.</p>
<p>Anyway, in the rest of this post, I’ll share a writeup of my big numbers talk.  The talk has substantial overlap with my “classic” <a href="https://www.scottaaronson.com/writings/bignumbers.html">Who Can Name The Bigger Number?</a> essay from 1999.  While I don’t mean to supersede or displace that essay, the truth is that I think and write somewhat differently than I did as a teenager (whuda thunk?), and I wanted to give Scott2017 a crack at material that Scott1999 has been over already.  If nothing else, the new version is more up-to-date and less self-indulgent, and it includes points (for example, the relation between ordinal generalizations of the Busy Beaver function and the axioms of set theory) that I didn’t understand back in 1999.</p>
<p>For regular readers of this blog, I don’t know how much will be new here.  But if you’re one of those people who keeps introducing themselves at social events by saying “I really love your blog, Scott, even though I don’t understand anything that’s in it”—something that’s always a bit awkward for me, because, uh, thanks, I guess, but what am I supposed to say next?—then <strong>this lecture is for you</strong>.  I hope you’ll read it and understand it.</p>
<p>Thanks so much to Festivaletteratura organizer Matteo Polettini for inviting me, and to Fabrizio Illuminati for moderating the Q&amp;A.  I had a wonderful time in Mantua, although I confess there’s something about being Italian that I don’t understand.  Namely: how do you derive any pleasure from international travel, if anywhere you go, the pizza, pasta, bread, cheese, ice cream, coffee, architecture, scenery, historical sights, and <em>pretty much everything else</em> all fall short of what you’re used to?</p>
<hr />
<p><strong>Big Numbers</strong></p>
<p>by Scott Aaronson<br />
Sept. 9, 2017</p>
<p>My four-year-old daughter sometimes comes to me and says something like: “daddy, I think I <em>finally</em> figured out what the biggest number is!  Is it a million million million million million million million million thousand thousand thousand hundred hundred hundred hundred twenty eighty ninety eighty thirty a million?”</p>
<p>So I reply, “I’m not even sure exactly what number you named—but whatever it is, why not that number plus one?”</p>
<p>“Oh yeah,” she says.  “So is <em>that</em> the biggest number?”</p>
<p>Of course there’s no biggest number, but it’s natural to wonder what are the biggest numbers we can name in a reasonable amount of time.  Can I have two volunteers from the audience—ideally, two kids who like math?</p>
<p>[Two kids eventually come up.  I draw a line down the middle of the blackboard, and place one kid on each side of it, each with a piece of chalk.]</p>
<p>So the game is, you each have ten seconds to write down the biggest number you can.  You can’t write anything like “the other person’s number plus 1,” and you also can’t write infinity—it has to be finite.  But other than that, you can write basically anything you want, as long as I’m able to understand exactly what number you’ve named.  [These instructions are translated into Italian for the kids.]</p>
<p>Are you ready?  On your mark, get set, GO!</p>
<p>[The kid on the left writes something like: 999999999</p>
<p>While the kid on the right writes something like: 11111111111111111</p>
<p>Looking at these, I comment:]</p>
<p>9 is bigger than 1, but 1 is a bit faster to write, and as you can see that makes the difference here!  OK, let’s give our volunteers a round of applause.</p>
<p>[I didn’t plant the kids, but if I had, I couldn’t have designed a better jumping-off point.]</p>
<hr />
<p>I’ve been fascinated by how to name huge numbers since I was a kid myself.  When I was a teenager, I even wrote an essay on the subject, called <a href="https://www.scottaaronson.com/writings/bignumbers.html">Who Can Name the Bigger Number?</a>  That essay might <em>still</em> get more views than any of the research I’ve done in all the years since!  I don’t know whether to be happy or sad about that.</p>
<p>I think the reason the essay remains so popular, is that it shows up on Google whenever someone types something like “what is the biggest number?”  Some of you might know that Google itself was named after the huge number called a <a href="https://en.wikipedia.org/wiki/Googol">googol</a>: 10100, or 1 followed by a hundred zeroes.</p>
<p>Of course, a googol isn’t even close to the biggest number we can name.  For starters, there’s a googolplex, which is 1 followed by a googol zeroes.  Then there’s a googolplexplex, which is 1 followed by a googolplex zeroes, and a googolplexplexplex, and so on.  But one of the most basic lessons you’ll learn in this talk is that, when it comes to naming big numbers, whenever you find yourself just repeating the same operation over and over and over, it’s time to step back, and look for something new to do that transcends everything you were doing previously.  (Applications to everyday life left as exercises for the listener.)</p>
<p>One of the first people to think about systems for naming huge numbers was Archimedes, who was Greek but lived in what’s now Italy (specifically Syracuse, Sicily) in the 200s BC.  Archimedes wrote a sort of pop-science article—possibly history’s <em>first</em> pop-science article—called <a href="http://euclid.trentu.ca/math/sb/3810H/Fall-2009/The-Sand-Reckoner.pdf">The Sand-Reckoner</a>.  In this remarkable piece, which was addressed to the King of Syracuse, Archimedes sets out to calculate an upper bound on the number of grains of sand needed to fill the entire universe, or at least the universe as known in antiquity.  He thereby seeks to refute people who use “the number of sand grains” as a shorthand for uncountability and unknowability.</p>
<p>Of course, Archimedes was just guessing about the size of the universe, though he <em>did</em> use the best astronomy available in his time—namely, the work of Aristarchus, who anticipated Copernicus.  Besides estimates for the size of the universe and of a sand grain, the other thing Archimedes needed was a way to name arbitrarily large numbers.  Since he didn’t have Arabic numerals or scientific notation, his system was basically just to compose the word “myriad” (which means 10,000) into bigger and bigger chunks: a “myriad myriad” gets its own name, a “myriad myriad myriad” gets another, and so on.  Using this system, Archimedes estimated that ~1063 sand grains would suffice to fill the universe.  Ancient Hindu mathematicians were able to name similarly large numbers using similar notations.  In some sense, the next really fundamental advances in naming big numbers wouldn’t occur until the 20th century.</p>
<p>We’ll come to those advances, but before we do, I’d like to discuss another question that motivated Archimedes’ essay: namely, what are the biggest numbers <em>relevant to the physical world</em>?</p>
<p>For starters, how many atoms are in a human body?  Anyone have a guess?  About 1028.  (If you remember from high-school chemistry that a “mole” is 6×1023, this is not hard to ballpark.)</p>
<p>How many stars are in our galaxy?  Estimates vary, but let’s say a few hundred billion.</p>
<p>How many stars are in the entire observable universe?  Something like 1023.</p>
<p>How many <em>subatomic particles</em> are in the observable universe?  No one knows for sure—for one thing, because we don’t know what the dark matter is made of—but 1090 is a reasonable estimate.</p>
<p>Some of you might be wondering: but for all anyone knows, couldn’t the universe be infinite?  Couldn’t it have <em>infinitely</em> many stars and particles?  The answer to that is interesting: indeed, no one knows whether space goes on forever or curves back on itself, like the surface of the earth.  But because of the dark energy, discovered in 1998, it seems likely that even if space is infinite, we can only ever see a finite part of it.  The dark energy is a force that pushes the galaxies apart.  The further away they are from us, the faster they’re receding—with galaxies far enough away from us receding <em>faster than light</em>.</p>
<p>Right now, we can see the light from galaxies that are up to about 45 billion light-years away.  (Why 45 billion light-years, you ask, if the universe itself is “only” 13.6 billion years old?  Well, when the galaxies emitted the light, they were a lot closer to us than they are now!  The universe expanded in the meantime.)  If, as seems likely, the dark energy has the form of a cosmological constant, then there’s a somewhat further horizon, such that it’s not just that the galaxies beyond that can’t be seen by us right now—it’s that they can <em>never</em> be seen.</p>
<hr />
<p>In practice, many big numbers come from the phenomenon of exponential growth.  Here’s a graph showing the three functions n, n2, and 2n:</p>
<p><a href="https://www.scottaaronson.com/expgrowth.gif"></a></p>
<p>The difference is, n and even n2 grow in a more-or-less manageable way, but 2n just shoots up off the screen.  The shooting-up has real-life consequences—indeed, more important consequences than just about any other mathematical fact one can think of.</p>
<p>The current human population is about 7.5 billion (when I was a kid, it was more like 5 billion).  Right now, the population is doubling about once every 64 years.  If it continues to double at that rate, and humans don’t colonize other worlds, then you can calculate that, less than 3000 years from now, the entire earth, all the way down to the core, will be made of human flesh.  I hope the people use deodorant!</p>
<p>Nuclear chain reactions are a second example of exponential growth: one uranium or plutonium nucleus fissions and emits neutrons that cause, let’s say, two other nuclei to fission, which then cause <em>four</em> nuclei to fission, then 8, 16, 32, and so on, until boom, you’ve got your nuclear weapon (or your nuclear reactor, if you do something to slow the process down).  A third example is compound interest, as with your bank account, or for that matter an entire country’s GDP.  A fourth example is Moore’s Law, which is the thing that said that the number of components in a microprocessor doubled every 18 months (with other metrics, like memory, processing speed, etc., on similar exponential trajectories).  Here at Festivaletteratura, there’s a “Hack Space,” where you can see state-of-the-art Olivetti personal computers from around 1980: huge desk-sized machines with maybe 16K of usable RAM.  Moore’s Law is the thing that took us from those (and the even bigger, weaker computers before them) to the smartphone that’s in your pocket.</p>
<p>However, a general rule is that <em>any time we encounter exponential growth in our observed universe, it can’t last for long</em>.  It <em>will</em> stop, if not before then when it runs out of whatever resource it needs to continue: for example, food or land in the case of people, fuel in the case of a nuclear reaction.  OK, but what about Moore’s Law: what physical constraint will stop <em>it</em>?</p>
<p>By some definitions, Moore’s Law has <em>already</em> stopped: computers aren’t getting that much faster in terms of clock speed; they’re mostly just getting more and more parallel, with more and more cores on a chip.  And it’s easy to see why: the speed of light is finite, which means the speed of a computer will always be limited by the size of its components.  And transistors are now just 15 nanometers across; a couple orders of magnitude smaller and you’ll be dealing with individual atoms.  And unless we leap really far into science fiction, it’s hard to imagine building a transistor smaller than one atom across!</p>
<p>OK, but what if we <em>do</em> leap really far into science fiction?  Forget about engineering difficulties: is there any fundamental principle of <em>physics</em> that prevents us from making components smaller and smaller, and thereby making our computers faster and faster, without limit?</p>
<p>While no one has tested this directly, it appears from current physics that there <em>is</em> a fundamental limit to speed, and that it’s about 1043 operations per second, or one operation per Planck time.  Likewise, it appears that there’s a fundamental limit to the density with which information can be stored, and that it’s about 1069 bits per square meter, or one bit per Planck area. (Surprisingly, the latter limit scales only with the surface area of a region, not with its volume.)</p>
<p>What would happen if you tried to build a faster computer than that, or a denser hard drive?  The answer is: cycling through that many different states per second, or storing that many bits, would involve concentrating so much <em>energy</em> in so small a region, that the region would exceed what’s called its Schwarzschild radius.  If you don’t know what that means, it’s just a fancy way of saying that your computer would collapse to a black hole.  I’ve always liked that as Nature’s way of telling you not to do something!</p>
<p>Note that, on the modern view, a black hole <em>itself</em> is not only the densest possible object allowed by physics, but also the most efficient possible hard drive, storing ~1069 bits per square meter of its event horizon—though the bits are not so easy to retrieve! It’s also, in a certain sense, the fastest possible computer, since it really <i>does</i> cycle through 1043 states per second—though it might not be computing anything that anyone would care about.</p>
<p>We can also combine these fundamental limits on computer speed and storage capacity, with the limits that I mentioned earlier on the size of the observable universe, which come from the cosmological constant.  If we do so, we get an upper bound of ~10122 on the number of bits that can ever be involved in <em>any</em> computation in our world, no matter how large: if we tried to do a bigger computation than that, the far parts of it would be receding away from us faster than the speed of light.  In some sense, this 10122 is the most fundamental number that sets the scale of our universe: on the current conception of physics, everything you’ve ever seen or done, or will see or will do, can be represented by a sequence of at most 10122 ones and zeroes.</p>
<hr />
<p>Having said that, in math, computer science, and many other fields (including physics itself), many of us meet bigger numbers than 10122 dozens of times before breakfast! How so? Mostly because we choose to ask, not about the number of <i>things that are</i>, but about the number of possible <i>ways they could be</i>—not about the size of ordinary 3-dimensional space, but the sizes of abstract spaces of possible configurations. And the latter are subject to exponential growth, continuing way beyond 10122.</p>
<p>As an example, let’s ask: how many different novels could possibly be written (say, at most 400 pages long, with a normal-size font, yadda yadda)? Well, we could get a lower bound on the number just by walking around here at Festivaletteratura, but the number that <i>could</i> be written certainly far exceeds the number that have been written or ever will be. This was the subject of Jorge Luis Borges’ famous story <a href="http://www.arts.ucsb.edu/faculty/reese/classes/artistsbooks/The%20Library%20of%20Babel.pdf">The Library of Babel</a>, which imagined an immense library containing every book that could possibly be written up to a certain length. Of course, the vast majority of the books are filled with meaningless nonsense, but among their number one can find all the great works of literature, books predicting the future of humanity in perfect detail, books predicting the future except with a single error, etc. etc. etc.</p>
<p>To get more quantitative, let’s simply ask: how many different ways are there to fill the <em>first page</em> of a novel?  Let’s go ahead and assume that the page is filled with intelligible (or at least grammatical) English text, rather than arbitrary sequences of symbols, at a standard font size and page size.  In that case, using standard estimates for the entropy (i.e., compressibility) of English, I estimated this morning that there are maybe ~10700 possibilities.  So, forget about the rest of the novel: there are astronomically more possible <em>first pages</em> than could fit in the observable universe!</p>
<p>We could likewise ask: how many chess games could be played?  I’ve seen estimates from 1040 up to 10120, depending on whether we count only “sensible” games or also “absurd” ones (though in all cases, with a limit on the length of the game as might occur in a real competition). For Go, by contrast, which is played on a larger board (19×19 rather than 8×8) the estimates for the number of possible games seem to start at 10800 and only increase from there. This difference in magnitudes has <i>something</i> to do with why Go is a “harder” game than chess, why computers were able to beat the world chess champion already in 1997, but the world Go champion not until last year.</p>
<p>Or we could ask: given a thousand cities, how many routes are there for a salesman that visit each city exactly once? We write the answer as 1000!, pronounced “1000 factorial,” which just means 1000×999×998×…×2×1: there are 1000 choices for the first city, then 999 for the second city, 998 for the third, and so on.  This number is about 4×102567.  So again, more possible routes than atoms in the visible universe, yadda yadda.</p>
<p>But suppose the salesman is interested only in the <em>shortest</em> route that visits each city, given the distance between every city and every other.  We could then ask: to find that shortest route, would a computer need to search exhaustively through all 1000! possibilities—or, maybe not all 1000!, maybe it could be a bit more clever than that, but at any rate, a number that grew exponentially with the number of cities n?  Or could there be an algorithm that zeroed in on the shortest route dramatically faster: say, using a number of steps that grew only linearly or quadratically with the number of cities?</p>
<p>This, modulo a few details, is one of the most famous unsolved problems in all of math and science.  You may have heard of it; it’s called P versus NP.  P (Polynomial-Time) is the class of problems that an ordinary digital computer can solve in a “reasonable” amount of time, where we define “reasonable” to mean, growing at most like the size of the problem (for example, the number of cities) raised to some fixed power.  NP (Nondeterministic Polynomial-Time) is the class for which a computer can at least <em>recognize</em> a solution in polynomial-time.  If P=NP, it would mean that for every combinatorial problem of this sort, for which a computer could recognize a valid solution—Sudoku puzzles, scheduling airline flights, fitting boxes into the trunk of a car, etc. etc.—there would be an algorithm that cut through the combinatorial explosion of possible solutions, and zeroed in on the best one.  If P≠NP, it would mean that at least some problems of this kind required astronomical time, regardless of how cleverly we programmed our computers.</p>
<p>Most of us believe that P≠NP—indeed, I like to say that if we were physicists, we would’ve simply declared P≠NP a “law of nature,” and given ourselves Nobel Prizes for the discovery of the law!  And if it turned out that P=NP, we’d just give ourselves more Nobel Prizes for the law’s overthrow.  But because we’re mathematicians and computer scientists, we call it a “conjecture.”</p>
<p>Another famous example of an NP problem is: I give you (say) a 2000-digit number, and I ask you to find its prime factors.  Multiplying two thousand-digit numbers is easy, at least for a computer, but factoring the product back into primes <em>seems</em> astronomically hard—at least, with our present-day computers running any known algorithm.  Why does anyone care?  Well, you might know that, any time you order something online—in fact, every time you see a little padlock icon in your web browser—your personal information, like (say) your credit card number, is being protected by a cryptographic code that depends on the belief that factoring huge numbers is hard, or a few closely-related beliefs.  If P=NP, then those beliefs would be false, and indeed <em>all</em> cryptography that depends on hard math problems would be breakable in “reasonable” amounts of time.</p>
<p>In the special case of factoring, though—and of the other number theory problems that underlie modern cryptography—it wouldn’t even take anything as shocking as P=NP for them to fall.  Actually, that provides a good segue into another case where exponentials, and numbers vastly larger than 10122, regularly arise in the real world: quantum mechanics.</p>
<p>Some of you might have heard that quantum mechanics is complicated or hard.  But I can let you in on a secret, which is that it’s incredibly simple once you take the physics out of it!  Indeed, I think of quantum mechanics as not exactly even “physics,” but more like an operating system that the rest of physics runs on as application programs.  It’s a certain generalization of the rules of probability.  In one sentence, the central thing quantum mechanics says is that, to fully describe a physical system, you have to assign a number called an “amplitude” to every <em>possible</em> configuration that the system could be found in.  These amplitudes are used to calculate the probabilities that the system will be found in one configuration or another if you look at it.  But the amplitudes aren’t themselves probabilities: rather than just going from 0 to 1, they can be positive or negative or even complex numbers.</p>
<p>For us, the key point is that, if we have a system with (say) a thousand interacting particles, then the rules of quantum mechanics say we need at least 21000 amplitudes to describe it—which is way more than we could write down on pieces of paper filling the entire observable universe!  In some sense, chemists and physicists knew about this immensity since 1926.  But they knew it mainly as a practical problem: if you’re trying to simulate quantum mechanics on a conventional computer, then as far as we know, the resources needed to do so increase exponentially with the number of particles being simulated.  Only in the 1980s did a few physicists, such as Richard Feynman and David Deutsch, suggest “turning the lemon into lemonade,” and building computers that <em>themselves</em> would exploit the exponential growth of amplitudes.  Supposing we built such a computer, what would it be good for?  At the time, the only obvious application was simulating quantum mechanics itself!  And that’s probably <em>still</em> the most important application today.</p>
<p>In 1994, though, a guy named Peter Shor made a discovery that dramatically increased the level of interest in quantum computers.  That discovery was that a quantum computer, if built, could factor an n-digit number using a number of steps that grows only like about n2, rather than exponentially with n.  The upshot is that, if and when practical quantum computers are built, they’ll be able to break almost all the cryptography that’s currently used to secure the Internet.</p>
<p>(Right now, only small quantum computers have been built; the record for using Shor’s algorithm is still to factor 21 into 3×7 with high statistical confidence!  But Google is planning within the next year or so to build a chip with 49 quantum bits, or qubits, and other groups around the world are pursuing parallel efforts.  Almost certainly, 49 qubits still won’t be enough to do anything <em>useful</em>, including codebreaking, but it might be enough to do something <em>classically hard</em>, in the sense of taking at least ~249 or 563 trillion steps to simulate classically.)</p>
<p>I should stress, though, that for <em>other</em> NP problems—including breaking various other cryptographic codes, and solving the Traveling Salesman Problem, Sudoku, and the other combinatorial problems mentioned earlier—we don’t know any quantum algorithm analogous to Shor’s factoring algorithm.  For these problems, we generally think that a quantum computer could solve them in roughly the <em>square root</em> of the number of steps that would be needed classically, because of another famous quantum algorithm called Grover’s algorithm.  But getting an <em>exponential</em> quantum speedup for these problems would, at the least, require an additional breakthrough.  No one has proved that such a breakthrough in quantum algorithms is impossible: indeed, no one has proved that it’s impossible even for <em>classical</em> algorithms; that’s the P vs. NP question!  But most of us regard it as unlikely.</p>
<p>If we’re right, then the upshot is that quantum computers are not magic bullets: they might yield dramatic speedups for certain special problems (like factoring), but they won’t tame the curse of exponentiality, cut through to the optimal solution, every time we encounter a Library-of-Babel-like profusion of possibilities.  For (say) the Traveling Salesman Problem with a thousand cities, even a quantum computer—which is the most powerful kind of computer rooted in known laws of physics—might, for all we know, take longer than the age of the universe to find the shortest route.</p>
<hr />
<p>The truth is, though, the biggest numbers that show up in math are <em>way</em> bigger than anything we’ve discussed until now: bigger than 10122, or even</p>
<p>$$ 2^{10^{122}}, $$</p>
<p>which is a rough estimate for the number of quantum-mechanical amplitudes needed to describe our observable universe.</p>
<p>For starters, there’s <a href="https://en.wikipedia.org/wiki/Skewes%27_number">Skewes’ number</a>, which the mathematician G. H. Hardy once called “the largest number which has ever served any definite purpose in mathematics.”  Let π(x) be the number of prime numbers up to x: for example, π(10)=4, since we have 2, 3, 5, and 7.  Then there’s a certain estimate for π(x) called li(x).  It’s known that li(x) overestimates π(x) for an enormous range of x’s (up to trillions and beyond)—but then at some point, it crosses over and starts underestimating π(x) (then overestimates again, then underestimates, and so on).  Skewes’ number is an upper bound on the location of the first such crossover point.  In 1955, Skewes proved that the first crossover must happen before</p>
<p>$$ x = 10^{10^{10^{964}}}. $$</p>
<p>Note that this bound has since been substantially improved, to 1.4×10316.  But no matter: there are numbers vastly bigger even than Skewes’ original estimate, which have since shown up in <a href="https://en.wikipedia.org/wiki/Ramsey_theory">Ramsey theory</a> and other parts of logic and combinatorics to take Skewes’ number’s place.</p>
<p>Alas, I won’t have time here to delve into specific (beautiful) examples of such numbers, such as <a href="https://en.wikipedia.org/wiki/Graham%27s_number">Graham’s number</a>.  So in lieu of that, let me just tell you about the sorts of processes, going far beyond exponentiation, that tend to yield such numbers.</p>
<p>The starting point is to remember a sequence of operations we all learn about in elementary school, and then ask why the sequence suddenly and inexplicably stops.</p>
<p>As long as we’re only talking about positive integers, “multiplication” just means “repeated addition.”  For example, 5×3 means 5 added to itself 3 times, or 5+5+5.</p>
<p>Likewise, “exponentiation” just means “repeated multiplication.”  For example, 53 means 5×5×5.</p>
<p>But what’s repeated exponentiation?  For that we introduce a new operation, which we call <em>tetration</em>, and write like so: 35 means 5 raised to itself 3 times, or</p>
<p>$$ ^{3} 5 = 5^{5^5} = 5^{3125} \approx 1.9 \times 10^{2184}. $$</p>
<p>But we can keep going. Let x <em>pentated</em> to the y, or xPy, mean x tetrated to itself y times.  Let x <em>sextated</em> to the y, or xSy, mean x pentated to itself y times, and so on.</p>
<p>Then we can define the <a href="https://en.wikipedia.org/wiki/Ackermann_function">Ackermann function</a>, invented by the mathematician Wilhelm Ackermann in 1928, which cuts across <em>all</em> these operations to get more rapid growth than we could with any one of them alone.  In terms of the operations above, we can give a slightly nonstandard, but perfectly serviceable, definition of the Ackermann function as follows:</p>
<p>A(1) is 1+1=2.</p>
<p>A(2) is 2×2=4.</p>
<p>A(3) is 3 to the 3rd power, or 33=27.</p>
<p>Not very impressive so far!  But wait…</p>
<p>A(4) is 4 tetrated to the 4, or</p>
<p>$$ ^{4}4 = 4^{4^{4^4}} = 4^{4^{256}} = BIG $$</p>
<p>A(5) is 5 pentated to the 5, which I won’t even <em>try</em> to simplify.  A(6) is 6 sextated to the 6.  And so on.</p>
<p>More than just a curiosity, the Ackermann function actually shows up sometimes in math and theoretical computer science.  For example, the <em>inverse</em> Ackermann function—a function α such that α(A(n))=n, which therefore grows as slowly as the Ackermann function grows quickly, and which is at most 4 for any n that would ever arise in the physical universe—sometimes appears in the running times of real-world algorithms.</p>
<p>In the meantime, though, the Ackermann function also has a more immediate application.  Next time you find yourself in a biggest-number contest, like the one with which we opened this talk, you can just write A(1000), or even A(A(1000)) (after specifying that A means the Ackermann function above).  You’ll win—<em>period</em>—unless your opponent has also heard of something Ackermann-like or beyond.</p>
<hr />
<p>OK, but Ackermann is very far from the end of the story.  If we want to go incomprehensibly beyond it, the starting point is the so-called <a href="https://en.wikipedia.org/wiki/Berry_paradox">“Berry Paradox”</a>, which was first described by Bertrand Russell, though he said he learned it from a librarian named Berry.  The Berry Paradox asks us to imagine leaping past exponentials, the Ackermann function, and every other particular system for naming huge numbers.  Instead, why not just go straight for a single gambit that seems to beat everything else:</p>
<p><strong>The biggest number that can be specified using a hundred English words or fewer</strong></p>
<p>Why is this called a paradox?  Well, do any of you see the problem here?</p>
<p>Right: if the above made sense, then we could just as well have written</p>
<p><strong>Twice the biggest number that can be specified using a hundred English words or fewer</strong></p>
<p>But <em>we just specified that number</em>—one that, by definition, takes more than a hundred words to specify—using far fewer than a hundred words!  Whoa.  What gives?</p>
<p>Most logicians would say the resolution of this paradox is simply that the concept of “specifying a number with English words” isn’t precisely defined, so phrases like the ones above don’t actually name definite numbers.  And how do we know that the concept isn’t precisely defined?  Why, because if it was, then it would lead to paradoxes like the Berry Paradox!</p>
<p>So if we want to escape the jaws of logical contradiction, then in this gambit, we ought to replace English by a clear, logical language: one that can be used to specify numbers in a completely unambiguous way.  Like … oh, I know!  Why not write:</p>
<p><strong>The biggest number that can be specified using a computer program that’s at most 1000 bytes long</strong></p>
<p>To make this work, there are just two issues we need to get out of the way.  First, what does it mean to “specify” a number using a computer program?  There are different things it could mean, but for concreteness, let’s say a computer program specifies a number N if, when you run it (with no input), the program runs for exactly N steps and then stops.  A program that runs forever doesn’t specify any number.</p>
<p>The second issue is, which programming language do we have in mind: BASIC? C? Python?  The answer is that it won’t much matter!  The <a href="https://en.wikipedia.org/wiki/Church%E2%80%93Turing_thesis">Church-Turing Thesis</a>, one of the foundational ideas of computer science, implies that every “reasonable” programming language can emulate every other one.  So the story here can be repeated with just about any programming language of your choice.  For concreteness, though, we’ll pick one of the first and simplest programming languages, namely “Turing machine”—the language invented by Alan Turing all the way back in 1936!</p>
<p>In the Turing machine language, we imagine a one-dimensional tape divided into squares, extending infinitely in both directions, and with all squares initially containing a “0.”  There’s also a tape head with n “internal states,” moving back and forth on the tape.  Each internal state contains an instruction, and the only allowed instructions are: write a “0” in the current square, write a “1” in the current square, move one square left on the tape, move one square right on the tape, jump to a different internal state, halt, and do any of the previous conditional on whether the current square contains a “0” or a “1.”</p>
<p>Using Turing machines, in 1962 the mathematician Tibor Radó invented the so-called <a href="https://en.wikipedia.org/wiki/Busy_beaver">Busy Beaver function</a>, or BB(n), which allowed naming <em>by far</em> the largest numbers anyone had yet named.  BB(n) is defined as follows: consider all Turing machines with n internal states.  Some of those machines run forever, when started on an all-0 input tape.  Discard them.  Among the ones that eventually halt, there must be some machine that runs for a maximum number of steps before halting.  However many steps that is, that’s what we call BB(n), the nth Busy Beaver number.</p>
<p>The first few values of the Busy Beaver function have actually been calculated, so let’s see them.</p>
<p>BB(1) is 1.  For a 1-state Turing machine on an all-0 tape, the choices are limited: either you halt in the very first step, or else you run forever.</p>
<p>BB(2) is 6, as isn’t <em>too</em> hard to verify by trying things out with pen and paper.</p>
<p>BB(3) is 21: that determination was already a research paper.</p>
<p>BB(4) is 107 (another research paper).</p>
<p>Much like with the Ackermann function, not very impressive yet!  But wait:</p>
<p>BB(5) is not yet known, but it’s known to be at least 47,176,870.</p>
<p>BB(6) is at least 7.4×1036,534.</p>
<p>BB(7) is at least</p>
<p>$$ 10^{10^{10^{10^{18,000,000}}}}. $$</p>
<p>Clearly we’re dealing with a monster here, but can we understand just how terrifying of a monster?  Well, call a sequence f(1), f(2), … <em>computable</em>, if there’s some computer program that takes n as input, runs for a finite time, then halts with f(n) as its output.  To illustrate, f(n)=n2, f(n)=2n, and even the Ackermann function that we saw before are all computable.</p>
<p>But I claim that the Busy Beaver function grows faster than <em>any</em> computable function.  Since this talk should have at least <em>some</em> math in it, let’s see a proof of that claim.</p>
<p>Maybe the nicest way to see it is this: suppose, to the contrary, that there were a computable function f that grew at least as fast as the Busy Beaver function.  Then by using that f, we could take the Berry Paradox from before, and turn it into an <em>actual</em> contradiction in mathematics!  So for example, suppose the program to compute f were a thousand bytes long.  Then we could write another program, not much longer than a thousand bytes, to run for (say) 2×f(1000000) steps: that program would just need to include a subroutine for f, plus a little extra code to feed that subroutine the input 1000000, and then to run for 2×f(1000000) steps.  But by assumption, f(1000000) is at least the maximum number of steps that any program up to a million bytes long can run for—even though we just wrote a program, less than a million bytes long, that ran for more steps!  This gives us our contradiction.  The only possible conclusion is that the function f, and the program to compute it, couldn’t have existed in the first place.</p>
<p>(As an alternative, rather than arguing by contradiction, one could simply start with any computable function f, and then build programs that compute f(n) for various “hardwired” values of n, in order to show that BB(n) must grow at least as rapidly as f(n).  Or, for yet a third proof, one can argue that, if any upper bound on the BB function were computable, then one could use that to solve the <a href="https://en.wikipedia.org/wiki/Halting_problem">halting problem</a>, which Turing famously showed to be uncomputable in 1936.)</p>
<p>In some sense, it’s not so surprising that the BB function should grow uncomputably quickly—because if it <em>were</em> computable, then huge swathes of mathematical truth would be laid bare to us.  For example, suppose we wanted to know the truth or falsehood of the Goldbach Conjecture, which says that every even number 4 or greater can be written as a sum of two prime numbers.  Then we’d just need to write a program that checked each even number one by one, and halted if and only if it found one that <em>wasn’t</em> a sum of two primes.  Suppose that program corresponded to a Turing machine with N states.  Then by definition, if it halted at all, it would have to halt after at most BB(N) steps.  But that means that, if we <em>knew</em> BB(N)—or even any upper bound on BB(N)—then we could find out whether our program halts, by simply running it for the requisite number of steps and seeing.  In that way we’d learn the truth or falsehood of Goldbach’s Conjecture—and similarly for the Riemann Hypothesis, and every other famous unproved mathematical conjecture (there are a lot of them) that can be phrased in terms of a computer program never halting.</p>
<p>(Here, admittedly, I’m using “we could find” in an <em>extremely</em> theoretical sense.  Even if someone handed you an N-state Turing machine that ran for BB(N) steps, the number BB(N) would be so hyper-mega-astronomical that, in practice, you could probably never distinguish the machine from one that simply ran forever.  So the aforementioned “strategy” for proving Goldbach’s Conjecture, or the Riemann Hypothesis would probably never yield fruit before the heat death of the universe, even though <em>in principle</em> it would reduce the task to a “mere finite calculation.”)</p>
<p>OK, you wanna know something else wild about the Busy Beaver function?  In 2015, my former student Adam Yedidia and I <a href="https://www.scottaaronson.com/blog/?p=2725">wrote a paper</a> where we proved that BB(8000)—i.e., the 8000th Busy Beaver number—<em>can’t be determined</em> using the usual axioms for mathematics, which are called Zermelo-Fraenkel (ZF) set theory.  Nor can B(8001) or any larger Busy Beaver number.</p>
<p>To be sure, BB(8000) <em>has</em> some definite value: there are finitely many 8000-state Turing machines, and each one either halts or runs forever, and among the ones that halt, there’s <em>some</em> maximum number of steps that any of them runs for.  What we showed is that math, if it limits itself to the currently-accepted axioms, can never prove the value of BB(8000), even in principle.</p>
<p>The way we did that was by explicitly constructing an 8000-state Turing machine, which (in effect) enumerates all the consequences of the ZF axioms one after the next, and halts if and only if it ever finds a contradiction—that is, a proof of 0=1.  Presumably set theory is actually consistent, and therefore our program runs forever.  But if you <em>proved</em> the program ran forever, you’d also be proving the consistency of set theory.  And has anyone heard of any obstacle to doing that?  Of course, Gödel’s Incompleteness Theorem!  Because of Gödel, if set theory is consistent (well, technically, also arithmetically sound), then it can’t prove our program either halts or runs forever.  But that means set theory can’t determine BB(8000) either—because if it could do <em>that</em>, then it could also determine the behavior of our program.</p>
<p>To be clear, it was long understood that there’s <em>some</em> computer program that halts if and only if set theory is inconsistent—and therefore, that the axioms of set theory can determine at most k values of the Busy Beaver function, for <em>some</em> positive integer k.  “All” Adam and I did was to prove the first explicit upper bound, k≤8000, which required a lot of optimizations and software engineering to get the number of states down to something reasonable (our initial estimate was more like k≤1,000,000).  More recently, Stefan O’Rear has improved our bound—most recently, he says, to k≤1000, meaning that, at least by the lights of ZF set theory, fewer than a thousand values of the BB function can ever be known.</p>
<p>Meanwhile, let me remind you that, at present, only four values of the function <em>are</em> known!  Could the value of BB(100) already be independent of set theory?  What about BB(10)?  BB(5)?  Just how early in the sequence do you leap off into Platonic hyperspace?  I don’t know the answer to that question but would love to.</p>
<hr />
<p>Ah, you ask, but is there any number sequence that grows so fast, it blows <em>even the Busy Beavers</em> out of the water?  There is!</p>
<p>Imagine a magic box into which you could feed in any positive integer n, and it would instantly spit out BB(n), the nth Busy Beaver number.  Computer scientists call such a box an “oracle.”  Even though the BB function is uncomputable, it still makes mathematical sense to imagine a Turing machine that’s enhanced by the magical ability to access a BB oracle any time it wants: call this a “super Turing machine.”  Then let SBB(n), or the nth super Busy Beaver number, be the maximum number of steps that any n-state <em>super</em> Turing machine makes before halting, if given no input.</p>
<p>By simply repeating the reasoning for the ordinary BB function, one can show that, not only does SBB(n) grow faster than any computable function, it grows faster than <em>any function computable by super Turing machines</em> (for example, BB(n), BB(BB(n)), etc).</p>
<p>Let a super duper Turing machine be a Turing machine with access to an oracle for the super Busy Beaver numbers.  Then you can use super duper Turing machines to define a super duper Busy Beaver function, which you can use in turn to define super duper pooper Turing machines, and so on!</p>
<p>Let “level-1 BB” be the ordinary BB function, let “level-2 BB” be the super BB function, let “level 3 BB” be the super duper BB function, and so on.  Then clearly we can go to “level-k BB,” for any positive integer k.</p>
<p>But we need not stop even there!  We can then go to level-ω BB.  What’s ω?  Mathematicians would say it’s the “first infinite ordinal”—the ordinals being a system where you can pass from any set of numbers you can possibly name (even an infinite set), to the next number larger than all of them.  More concretely, the level-ω Busy Beaver function is simply the Busy Beaver function for Turing machines that are able, whenever they want, to call an oracle to compute the level-k Busy Beaver function, <em>for any positive integer k of their choice</em>.</p>
<p>But why stop there?  We can then go to level-(ω+1) BB, which is just the Busy Beaver function for Turing machines that are able to call the level-ω Busy Beaver function as an oracle.  And thence to level-(ω+2) BB, level-(ω+3) BB, etc., defined analogously.  But then we can transcend that entire sequence and go to level-2ω BB, which involves Turing machines that can call level-(ω+k) BB as an oracle for any positive integer k.  In the same way, we can pass to level-3ω BB, level-4ω BB, etc., until we transcend that entire sequence and pass to level-ω2 BB, which can call <em>any</em> of the previous ones as oracles.  Then we have level-ω3 BB, level-ω4 BB, etc., until we transcend <em>that</em> whole sequence with level-ωω BB.  But we’re still not done!  For why not pass to level</p>
<p>$$ \omega^{\omega^{\omega}} $$,</p>
<p>level</p>
<p>$$ \omega^{\omega^{\omega^{\omega}}} $$,</p>
<p>etc., until we reach level</p>
<p>$$ \left. \omega^{\omega^{\omega^{.^{.^{.}}}}}\right\} _{\omega\text{ times}} $$?</p>
<p>(This last ordinal is also called ε0.)  And mathematicians know how to keep going even to way, way bigger ordinals than ε0, which give rise to ever more rapidly-growing Busy Beaver sequences.  Ordinals achieve something that on its face seems paradoxical, which is to systematize the concept of transcendence.</p>
<p>So then just how far can you push this?  Alas, ultimately the answer depends on which axioms you assume for mathematics.  The issue is this: once you get to sufficiently enormous ordinals, you need some systematic way to <em>specify</em> them, say by using computer programs.  But then the question becomes which ordinals you can “prove to exist,” by giving a computer program together with a proof that the program does what it’s supposed to do.  The more powerful the axiom system, the bigger the ordinals you can prove to exist in this way—but every axiom system will run out of gas at some point, only to be transcended, in Gödelian fashion, by a yet more powerful system that can name yet larger ordinals.</p>
<p>So for example, if we use Peano arithmetic—invented by the Italian mathematician <a href="https://en.wikipedia.org/wiki/Giuseppe_Peano">Giuseppe Peano</a>—then Gentzen proved in the 1930s that we can name any ordinals below ε0, but not ε0 itself or anything beyond it.  If we use ZF set theory, then we can name vastly bigger ordinals, but once again we’ll eventually run out of steam.</p>
<p>(Technical remark: some people have claimed that we can transcend this entire process by passing from first-order to second-order logic.  But I fundamentally disagree, because with second-order logic, <em>which number you’ve named</em> could depend on the model of set theory, and therefore be impossible to pin down.  With the ordinal Busy Beaver numbers, by contrast, the number you’ve named might be breathtakingly hopeless ever to compute—but provided the notations have been fixed, and the ordinals you refer to actually exist, at least we know there <em>is</em> a unique positive integer that you’re talking about.)</p>
<p>Anyway, the upshot of all of this is that, if you try to hold a name-the-biggest-number contest between two actual professionals who are trying to win, it will (alas) degenerate into an argument about the axioms of set theory.  For the stronger the set theory you’re allowed to assume consistent, the bigger the ordinals you can name, therefore the faster-growing the BB functions you can define, therefore the bigger the actual numbers.</p>
<p>So, yes, in the end the biggest-number contest just becomes another Gödelian morass, but one can get surprisingly far before that happens.</p>
<hr />
<p>In the meantime, our universe seems to limit us to at most 10122 choices that could ever be made, or experiences that could ever be had, by any one observer.  Or fewer, if you believe that you won’t live until the heat death of the universe in some post-Singularity computer cloud, but for at most about 102 years.  In the meantime, the survival of the human race might hinge on people’s ability to understand much smaller numbers than 10122: for example, a billion, a trillion, and other numbers that characterize the exponential growth of our civilization and the limits that we’re now running up against.</p>
<p>On a happier note, though, if our goal is to make math engaging to young people, or to build bridges between the quantitative and literary worlds, the way this festival is doing, it seems to me that it wouldn’t hurt to let people know about the vastness that’s out there.  Thanks for your attention.</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://feedproxy.google.com/~r/TroyHunt/~3/IHrHVlwNwRg/">Face ID, Touch ID, No ID, PINs and Pragmatic Security</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://feeds.feedburner.com/TroyHunt">Troy Hunt</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">security</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Thu Sep 14 2017 09:45:41 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p><a href="http://www.symantec.com/ready-secure-go/it-smb/?om_ext_cid=ws_ad_TROY_DBC-Bulldog_IT-SMB_Encrypt"><strong>Presently sponsored by:</strong> Get a security solution that will keep your website up and running—and keep you sleeping soundly: Symantec Website Security. Learn how</a></p><div><p>I was wondering recently after poring through yet another data breach how many people actually use multi-step verification. I mean here we have a construct where even if the attacker has the victim's credentials, they're rendered useless once challenged for the authenticator code or SMS which is subsequently set. I went out looking for figures and found the following on Dropbox:</p>
<blockquote><p>&quot;less than 1% of the Dropbox user base is taking advantage of the company’s two-factor authentication feature&quot;: <a href="https://t.co/AdbYwWGb7t">https://t.co/AdbYwWGb7t</a></p>— Troy Hunt (@troyhunt) <a href="https://twitter.com/troyhunt/status/738620838304317441">June 3, 2016</a></blockquote>

<p>Less than 1%. That's alarming. It's alarming not just because the number is so low, but because Dropbox holds such valuable information for so many people. Not only that, but their multi-step implementation is very low-friction - you generally only ever see it when setting up a new machine for the first time.</p>
<p>But here's the problem with multi-step verification: it's a perfect example of where security is friction. No matter how easy you make it, it's something you have to do <em>in addition</em> to the thing you normally do, namely entering a username and password. That's precisely the same problem with getting people to put PINs on their phone and as a result, there's a huge number of devices out there left wide open. How many? It's hard to tell because there's no easy way of collecting those stats. I found one survey from 2014 which said <a href="https://www.elie.net/blog/survey-most-people-dont-lock-their-android-phones-but-should">52% of people have absolutely nothing protecting their phone</a>. Another in 2016 said <a href="http://www.androidauthority.com/psa-use-a-lockscreen-password-668689/">the number is more like 34%</a>. Keep searching and you'll find more stats of wildly varying values but the simple fact remains that there are a <em>huge</em> number of people out there with no protection on the device at all.</p>
<p>And this brings us to Face ID. I'm writing this the day after the iPhone X launch and obviously this pattern of biometric login is now going to be a major part of the Apple security strategy:</p>
<p></p>
<p>Of course, this now brings with it the whole biometrics discussion and to some extent, it's similar to the one we had when Touch ID launched in 2013 with the iPhone 5S. Obviously there are differences, but many of the issues are also the same. Either way, in my mind both pose fantastic upsides for 99.x% of people and I shared that thought accordingly:</p>
<blockquote><p>Face ID: for 99.x% of people, their &quot;threat actors&quot; are people who steal their phone at a bar. For everyone else, don't use biometrics.</p>— Troy Hunt (@troyhunt) <a href="https://twitter.com/troyhunt/status/907701016161787904">September 12, 2017</a></blockquote>

<p>Not everyone agreed though and there were some responses I honestly didn't see coming. I want to outline some of the issues with each and why per the title of this post, &quot;pragmatic security&quot; is really important here.</p>
No ID
<p>Let's start here because it's the obvious one. Missing PINs on phones provides zero protection from any adversary that gets their hands on it; the kids, a pickpocket or law enforcement - it's free rein for all. Free reign over photos and videos, free reign over messages and email and free reign to communicate with anyone else under the identity of the device owner. I'm stating things here that may seem obvious to you, but clearly the risks haven't yet hit home for many people.</p>
<p>A lack of PIN has also proved very useful for <em>remote</em> attackers. Back in 2014 I wrote about <a href="https://www.troyhunt.com/the-mechanics-of-icloud-hack-and-how/">the &quot;Oleg Pliss&quot; situation</a> where unprotected devices were being remotely locked and ransomed. This was only possible when the device didn't have a PIN, a fact the attacker abused by then placing their own on it after gaining access to the victim's online Apple account.</p>
<p>But we can't talk about devices not having any authentication without talking about <em>why</em> and that almost always comes down to friction. The Dropbox multi-step verification situation described above where an additional security control is imposed. So let's move on and start talking about that friction, let's talk about PINs.</p>
PIN
<p>The first point I'll make here as I begin talking about the 3 main security constructs available is that they're all <em>differently</em> secure. This is not a case of one is &quot;secure&quot; and another is &quot;insecure&quot; in any sort of absolute way and anyone referring to them in this fashion is missing a very important part of the narrative. With that said, let's look at the pros and cons involved here.</p>
<p>Obviously, the big pro of a PIN is familiarity (that and not having the problems mentioned above, of course). If you can remember a number, you can set a PIN and hey, we're all good at remembering numbers, right? Like that same one that people use everywhere... which is obviously a con. And this is a perfect example of the human element we so frequently neglect when having this discussion: people take shortcuts with PINs. They reuse them. They follow basic patterns. They disclose them to other people - how many people's kids know the PIN that unlocks their phone? And before you answer &quot;not mine&quot;, you're (probably) not normal people by virtue of being interested enough in your security to be here reading this post in the first place!</p>
<p>But PINs are enormously popular and even when you <em>do</em> use the biometric options we're about to get into, you're still going to need one on your phone anyway. For example, every time you hard-reboot an iPhone with Touch ID you need to enter the PIN. When you do, depending on the environment you're in you may be a bit inclined to do so like this:</p>
<p></p>
<p>This is Edward Snowden typing his password in whilst under a blanket in the <a href="https://en.wikipedia.org/wiki/Citizenfour">Citizenfour documentary</a>. He's doing everything he can to ensure there's no way his password can be observed by others because this is precisely the problem with passwords - anyone who has yours can use it (again, this is why multi-step verification is so important). Now you may argue that Snowden's threat profile is such that he <em>needs</em> to take such measures and you're right - I can see exactly why he'd do this. But this also means recognising that different people have different threat profiles and whilst Ed was worried about the likes of the NSA specifically targeting him as a high-value individual, you are (almost certainly) not.</p>
<p>We've all been warned about the risk of shoulder surfing at one time or another and it's pretty common to find it represented in corporate training programs <a href="https://www.slideshare.net/akshaysurve53/shoulder-surfing-resistant-graphical-and-image-based-login-system">in a similar fashion to this</a>:</p>
<p></p>
<p>Except as it relates to PINs on phones, the problem is much worse. Firstly, it's worse because it's a PIN that's a mere 4 or 6 digits (you could always go alphanumeric on an iPhone but that'll be a near-zero percentage of people) and there's only 10 characters to choose from so observing and remembering them isn't hard. Secondly, mobile devices are obviously used in, well, mobile locations so you're unlocking them on the train, in the shops and in a whole raft of locations where people can directly observe you. When using Apple Pay is a perfect example: you're standing in a queue with people in front of you and people behind you waiting for you to pay for your shopping and that's not a great environment to be entering a secret by pressing a small number of big buttons on a publicly observable screen.</p>
<p>And then there's all the really niche attacks against PINS, for example <a href="https://www.theatlantic.com/technology/archive/2017/03/hot-hands-smartphones/519069/">using thermal imaging to detect the locations the screen was tapped</a>. Now that's by no means trivial, but some of criticisms being levelled at biometrics are also by no means trivial so let's keep it an even playing field. Even entering your PIN in an open space away from people presents a risk in the presence of the burgeoning number of surveillance cameras that are present.</p>
<p>But there's one thing in particular PINs are resilient to which biometrics are not: <a href="https://www.theatlantic.com/technology/archive/2016/05/iphone-fingerprint-search-warrant/480861/">the police in the US can force you to unlock your phone using your fingerprint</a>. I'm not sure how this differs in the rest of the world, but it was regularly highlighted to me during yesterday's discussions. Now there are obvious privacy issues with this - <em>big ones</em> - but getting back to the personal threat actors issue, this is something the individual needs to think about and consider whether it's a significant enough risk to them to rule out biometrics. If you're an activist or political dissident or indeed an outright criminal, this may pose a problem. For everyone else, you're starting to approach infinitely small likelihoods. I heard an argument yesterday that, for example, a lady who was filming a bloke being shot by the police could have then been forced to unlock her phone by fingerprint so the cops can erase the evidence. But think this through for a moment...</p>
<blockquote><p>So the risk here is being shot while recording it to local storage then cops unlocking phone with biometrics and illegally erasing evidence?</p>— Troy Hunt (@troyhunt) <a href="https://twitter.com/troyhunt/status/907735651759513600">September 12, 2017</a></blockquote>

<p>There will always be attack vectors like this. <em>Always.</em> The question someone has to ask when choosing between biometrics or PIN is how threatened they personally feel by these situations. And then they make their own security decision of their own free volition.</p>
<p>Let's move on to the biometric alternatives.</p>
Touch ID
<p>Given we've now had 4 years of Touch ID (and of course many more years of fingerprint auth in general), we've got a pretty good sense the threat landscape. Even 15 years ago, <a href="https://www.theregister.co.uk/2002/05/16/gummi_bears_defeat_fingerprint_sensors/">researchers were circumventing biometric logins</a>. In that particular case, the guy simply lifted a fingerprint from a glass then enhanced it with a cyanoacrylate adhesive, photographed it, took it into Photoshop and cleaned up the picture, printed it onto a transparency sheet, grabbed a photo-sensitive printed circuit board then etched the printed fingerprint from the transparency sheet into the copper on the board before moulding a gelatine finger onto it hence inheriting the fingerprint. Easy!</p>
<p>There have been many other examples of auth bypass since that time, including against Apple's Touch ID and indeed some of them have been simpler too. Like PINs, it's not foolproof and what people are doing is trading various security and usability attributes between the PIN and biometric options. A PIN has to be known to unlock the device whilst a fingerprint could be forged, but then a PIN may have been observed or readily guessed (and certainly there are brute force protections to limit this) whilst biometric login can be used in plain sight. They're <em>differently</em> secure and they protect from different threat actors. It's <em>extremely</em> unlikely that the guy who steals your phone off a bar is going to be able to do this, much more likely that a nation state actor that sees a high value in a target's phone will.</p>
<p>One of the arguments I heard against Touch ID yesterday is that an &quot;attacker&quot; could cause a sleeping or unconscious person to unlock their device by placing the owner's finger on the home button. I've quoted the word &quot;attacker&quot; because one such situation occurred last year when <a href="http://www.complex.com/life/2016/12/kid-buys-250-dollars-pokemon-items-with-moms-thumbprint">a six year old used her sleeping mother's fingerprint to buy $250 worth of Pokemon</a>. Now I've got a 5-year-old and a 7-year-old so I reckon I'm qualified enough to make a few comments on the matter (plus the whole thing about me thinking a lot about security!)</p>
<p>Firstly, whilst the hacker inside of me is thinking &quot;that kid is pretty smart&quot;, the parent inside of me is thinking &quot;that kids needs a proverbial kick up the arse&quot;. There's nothing unusual about kids using parent's phones for all manner of things and we've probably all given an unlocked phone to one of our own children for them to play a game, watch a video or even just talk on the phone. Touch ID, PIN or nothing at all, if a kid abuses their parent's trust in this way then there's a very different discussion to be had than the one about how sophisticated a threat actor needs to be in order to circumvent it.</p>
<p>Be that as it may, there are certainly circumstances where biometric login poses a risk that PINs don't and the unconscious one is a perfect example. Now in my own personal threat model, being unconscious whilst someone steals my phone <em>and</em> forces me to unlock it is not exactly high up on the list of things that keep me awake at night. But if I was a binge drinker and prone to the odd bender, Touch ID may simply not be a good model for me. Then again, if the victim is getting paralytic drunk and the attacker wants access to an unlocked phone then there are many other simple social engineering tricks to make that happen. In fact, in the attacker's world, the victim having a PIN may well be preferable because it could be observed on unlock and then used to modify security settings that are otherwise unavailable with mere access to fingerprints.</p>
<p>One of the neat features coming in iOS 11 when it hits next week <a href="https://www.wired.com/story/apples-ios-11-will-make-it-even-harder-for-cops-to-extract-your-data/">is the ability to rapidly disable Touch ID</a>:</p>
<blockquote>
<p>Tap the phone's home button five times, and it will launch a new lockscreen with options to make an emergency call or offer up the owner's emergency medical information. But that S.O.S. mode also silently disables TouchID, requiring a passcode to unlock the phone. That feature could be used to prevent someone from using the owner's finger to unlock their phone while they're sleeping or otherwise incapacitated, for instance.</p>
</blockquote>
<p>What this means is that were you find yourself in a higher risk situation with only Touch ID enabled (i.e. you've been pulled over by the police and are concerned about them compelling you to biometrically unlock your phone), there's a speedy option to disable it. But that's obviously no good if you don't have time to enable it so if you're going to hold up a liquor store and it's possible the cops may come bursting in at any time, it could be tough luck (also, don't hold up liquor stores!)</p>
<p>Another new feature helps further strengthen the security model:</p>
<blockquote>
<p>in iOS 11, iPhones will not only require a tap to trust a new computer, but the phone's passcode, too. That means even if forensic analysts do seize a phone while it's unlocked or use its owner's finger to unlock it, they still need a passcode to offload its data to a program where it can be analyzed wholesale.</p>
</blockquote>
<p>I particularly like this because it adds protection to <em>all</em> unlocked devices where the PIN is not known. If you're compelled to biometrically unlock the device, that won't allow the data to be siphoned off via tethering it. Yes, it could still be accessed directly on the device, but that's a damn sight better then unfettered direct access to storage.</p>
<p>So pros and cons and indeed that's the whole theme of this post. Let's move on to the new thing.</p>
Face ID
<p>I watched the keynote today and was obviously particularly interested in how Face ID was positioned so let me share the key bits here. Keep in mind that I obviously haven't played with this and will purely be going by Apple's own info here.</p>
<p>Firstly, this is not a case of &quot;if the camera sees something that looks like the owner's face the device unlocks&quot;. Here's why:</p>
<p></p>
<p>Infrared camera + flood illuminator + proximity sensor + ambient light sensor + camera + dot projector = Face ID. Each of these plays a different role and you can see how, for example, something like infrared could be used to discern the difference between a human head and a photo.</p>
<p>In Apple's demo, they talk about the flood illuminator being used to detect the face (including in the dark):</p>
<p></p>
<p>This is followed up by the infrared camera taking an image:</p>
<p></p>
<p>The dot projector then pumps out 30k invisible dots:</p>
<p></p>
<p>The IR image and the dot pattern then get &quot;pushed though neural networks to create a mathematical model of your face&quot; which is then compared to the stored one created at setup. Now of course we don't know how much of this is fancy Apple speak versus reality and I'm <em>very</em> keen to see the phone get into the hands of creative security people, but you can't help but think that the breadth of sensors available for visual verification trumps those required for touch alone.</p>
<p>Apple is obviously conscious of comparisons between the two biometric implementations and they shared some stats on the likelihood of each returning a false positive. Firstly, Touch ID:</p>
<p></p>
<p>So what they're saying here is that you've got a 1 in 50k chance of someone else's print unlocking your phone. From a pure chance perspective, those are pretty good odds but I'm not sure that's the best metric to use (more on that in a moment).</p>
<p>Here's how Face ID compares:</p>
<p></p>
<p>One in a million. There's literally a saying that's &quot;one in a million&quot; which symbolises the extremely remote likelihood of something happening! The 20x figure over Touch ID is significant but it doesn't seem like the right number to be focusing on. The <em>right</em> number would be the one that illustrates not the likelihood of random people gaining access, but rather the likelihood of an adversary tricking the biometrics via artificial means such as the gummi bears and PCBs. But that's not the sort of thing we're going to know until people start attempting just that.</p>
<p>Be that as it may, Apple claims that Face ID is resilient to both photos and masks:</p>
<p></p>
<p></p>
<p>And with all those sensors, it's certainly believable that there's enough inputs to discern with a high degree of confidence what is a legitimate face versus a fake one. Having said that, even Apple themselves acknowledged that certain threats remain:</p>
<p></p>
<p>Laughs were had, jokes were made but the underlying message was that Face ID isn't foolproof. Just like Touch ID. And PINs.</p>
<p>Thinking back to Touch ID for a moment, one of the risk flagged there was a kid holding a sleeping adult's finger on the sensor or indeed someone doing the same with an unconscious iPhone owner. Face ID seems to solve this problem:</p>
<blockquote>
<p>If your eyes are closed or you're looking away, it's not going to unlock</p>
</blockquote>
<p>Which makes a lot of sense - given the processing power to actually observe and interpret eye movements in the split second within which you expect this to work, this would be a really neat failsafe. Apple highlights this as &quot;attention awareness&quot; when they wrap up the Face ID portion of the presentation:</p>
<p></p>
<p>Moving on to something different, when I shared 99.x% tweet earlier on, a thread emerged about abusive spouses. Now if I'm honest, I didn't see that angle coming and it made me curious - what <em>is</em> the angle? I mean how does Face ID pose a greater threat to victims of domestic violence than the previous auth models? There's the risk of being physically compelled to unlock the phone, but of course Touch ID poses the same risk. One would also imagine that in such a situation, an abusive husband may demand a PIN in the same intimidating fashion in which they demand a finger is placed on the sensor or the front facing camera is pointed at the face (and appropriate eye movement is made). It's hard to imagine there are many legitimate scenarios where an iPhone X is present, is only using Face ID, the owner is an abused woman and the man is able to compel her to unlock the device in a way that wasn't previously possible. The only tangible thing I could take away from that conversation is that many people won't understand the respective strengths and weaknesses of each authentication method which, of course, is true for anyone regardless of their relationship status. (Folks who understand both domestic violence and the role of technology in that environment, do please comment if I'm missing something.)</p>
<p>The broader issue here is trusting those you surround yourself with in the home. In the same way that I trust my kids and my wife not to hold my finger to my phone while I'm sleeping, I trust them not to abuse my PC if I walk away from it whilst unlocked and yes, one would reasonably expect to be able to do that in their own home. The PC sits there next to my wallet with cash in it and the keys to the cars parked out the front. When you can no longer trust those in your immediate vicinity within the sanctity of your own home, you have a much bigger set of problems:</p>
<blockquote><p>My ex broke into my phone by holding it against me while asleep<br /><br />She also broke a table over my head so I'm not sure I'm disproving Troy <a href="https://t.co/RELrlLozhN">https://t.co/RELrlLozhN</a></p>— Alexander Payne (@myrrlyn) <a href="https://twitter.com/myrrlyn/status/907736938752258048">September 12, 2017</a></blockquote>

<p>Having absorbed all the info and given Face ID some deeper thought, I stand by that 99.x% tweet until proven wrong. I just can't make good, rational arguments against it without letting go of the pragmatism which acknowledges all the factors I've mentioned above.</p>
Summary
<p>What we have to keep in mind here is just how low the security bar is still set for so many people. Probably not for you being someone interested in reading this sort of material in the first place, but for the billions of &quot;normals&quot; out there now using mobile devices. Touch ID and Face ID are so frictionless that they remove the usability barrier PINs post. There's a good reason Apple consistently shows biometric authentication in all their demos - because it's just such a slick experience.</p>
<p>The majority of negative commentary I'm seeing about Face ID in particular amounts to &quot;facial recognition is bad&quot; and that's it. Some of those responses seem to be based on the assumption that it introduces a privacy risk in the same way as facial tracking in, say, the local supermarket would. But that's not the case here; the data is stored in the iPhone's secure enclave and never leaves the device:</p>
<p></p>
<p>More than anything though, we need to remember that Face ID introduces another security model with its own upsides and downsides on both security and usability. It's not &quot;less secure than a PIN&quot;, it's differently secure and the trick now is in individuals choosing the auth model that's right for them.</p>
<p>I'll order an iPhone X when they hit the store next month and I'll be giving Face ID a red hot go. I'll also be watching closely as smart security folks around the world try to break it :)</p>
<p><strong>Edit 1:</strong> <a href="https://techcrunch.com/2017/09/15/interview-apples-craig-federighi-answers-some-burning-questions-about-face-id/">TechCrunch has published a great interview with Craig Federighi</a> that answers many of the questions that have been raised in this blog post and in the subsequent comments. Highly recommended reading!</p>
<p><strong>Edit 2:</strong> Someone pointed me at a great video from last year's WWDC titled <a href="https://developer.apple.com/videos/play/wwdc2016/705/">How iOS Security Really Works</a>. At the 12:45 mark, they share info on Touch ID including the following three slides:</p>
<p></p>
<p></p>
<p></p>
<p>I've shared these here because they're enormously important with regards to the narrative about the frequency with which people unlock their devices and the value proposition of reducing friction. The numbers speak for themselves.</p>
</div>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://bradfrost.com/blog/post/full-stack-developers/">Full-Stack Developers</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="http://feeds.feedburner.com/brad-frosts-blog">Brad Frost</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">design</span>
              <span class="tag">web</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Wed Sep 13 2017 18:52:06 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>In my experience consulting with and workshopping with companies of all shapes and sizes, I always ask “who owns the frontend code here?” I help web teams create design systems and collaborate more effectively, so it’s important for me to get a solid understanding of who tackles frontend code, where they sit, which department they’re a part of, and where they fit in the team’s workflow.</p>
<p>On occasion, I’ll have the following conversation when I ask where the frontend people are:</p>
<blockquote><p>“Who are the frontend developers?”</p>
<p>“Oh, we only hire full-stack developers here.”</p>
<p>“OK, so those full-stack developers craft all the UI code?”</p>
<p>“Yep”</p>
<p>“Ok, so talk me through some of the decisions you make about frontend strategy and architecture.”</p>
<p>“Uhhhh…yeah.”</p></blockquote>
<p>In my experience,<strong> “full-stack developers” always translates to “programmers who can do frontend code because they have to and it’s ‘easy’.”</strong> It’s never the other way around. The term “full-stack developer” implies that a developer is equally adept at both frontend code and backend code, but I’ve never in my personal experience witnessed anyone who truly fits that description.</p>
<p>Are there people out there that feel equally comfortable designing a flexible, performant, accessible card component UI, and also wiring up the API that will feed content into that card component? I have no doubts. But is that skillset a common occurrence? I doubt it.</p>
<p>I’ve discussed in <a href="http://bradfrost.com/blog/post/frontend-design/">other posts</a> and <a href="http://atomicdesign.bradfrost.com/chapter-4/#development-is-design">in my book</a> about how the role of frontend design is still being defined, and organizationally there’s still a lot of confusion about what to do with frontend designers and where they fit into the process. The divide between design and engineering is often<strong> designers = people producing static design artifacts</strong> and <strong>development = people who code</strong>. Frontend design often gets bucketed in with engineering because it’s technically code. So then UI code becomes just another task for already-busy engineers to tackle, and because HTML/CSS aren’t programming languages it’s seen as “easy” work, not as much time and attention gets committed to it. Of course, the entire final product suffers as a result.</p>
<p>Frontend design is a critical part of the design and development process. Creating UIs that are responsive, performant, accessible, compatible, flexible, extensible, and resilient is tough work. That’s why it’s a good idea to treat frontend as a first-class citizen in your organization and dedicate some people that can really own frontend code.  Frontend designers can straddle the line between design and development, serving as <a href="http://bradfrost.com/blog/post/job-title-its-complicated/">mortar</a> that can hold the team together.</p>
<p><a href="http://bradfrost.com/blog/post/frontend-design/"></a></p>
<p>Now I understand that smaller companies often hire developers that run the gamut out of necessity. There simply aren’t enough resources to hire both backend and frontend developers. In my first jobs at small companies, I wrote the frontend code, got the SSL certificates working, handled the CMS integration, and fixed the router when the internet crapped out. While I did what was necessary and I benefitted from the experience, I’d never admit I was equally skilled at all of those things (especially the router fixing stuff).</p>
<p>Large organizations have the ability to hire specialists, which is why I get so confused why so many companies proudly declare they only hire full-stack developers. Having team members that can own the frontend experience is a good thing. Having team members that can own all things backend is a good thing. Having everyone work together to create successful products is a good thing.</p>
<p>I don’t think “full-stack developer” is necessarily a bad word, and there’s nothing wrong for individual developers to define their role in that way. But organizationally, when I encounter “we only hire full-stack developers” orgs I’ve only seen issues with their process and UI code.</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://stratechery.com/2017/the-lessons-and-questions-of-the-iphone-x-and-the-iphone-8/">The Lessons and Questions of the iPhone X and the iPhone 8</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="http://stratechery.com/feed/">Stratechery</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">business</span>
              <span class="tag">economics</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Wed Sep 13 2017 16:13:05 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>It’s tempting — and easy — to be cynical about the richest company in the world beginning its annual unveiling of new products with what effectively amounted to a promotional video for a building custom-built at enormous expense for said unveiling, set to the soundtrack of John Lennon singing “All You Need is Love”.<a href="https://stratechery.com/2017/the-lessons-and-questions-of-the-iphone-x-and-the-iphone-8/#footnote_0_2731">1</a></p>
<p><a href="https://stratechery.com/wp-content/uploads/2017/09/IMG_0007.png"></a></p>
<div><em><br />
There’s nothing you can do that can’t be done<br />
Nothing you can sing that can’t be sung<br />
Nothing you can say but you can learn how to play the game<br />
It’s easy<p></p>
<p>Nothing you can make that can’t be made<br />
No one you can save that can’t be saved<br />
Nothing you can do but you can learn how to be you in time<br />
It’s easy<br />
</p></em></div>
<p>In fact, the song was perfect; the temptation to be cynical is right there in the first verse, with the observation that by virtue of doing or singing or making you are operating in the bounds of what is merely possible, no more. And yet, the second verse holds forth salvation: find yourself, and find fulfillment. After all, you are the only one that can accomplish that precise task.</p>
<p><a href="https://www.apple.com/apple-events/september-2017/"></a></p>
<p>After the song finished, with the stage saying nothing more than “Welcome to the Steve Jobs Theater,” a recording of the late Apple founder and two-time CEO made the same point:</p>
<blockquote><p>
  There’s lots of ways to be as a person. And some people express their deep appreciation in different ways. But one of the ways that I believe people express their appreciation to the rest of humanity is to make something wonderful and put it out there. And you never meet the people, you never shake their hands, you never hear their story or tell yours, but somehow, in the act of making something with a great deal of care and love, something is transmitted there. And it’s a way of expressing to the rest of our species our deep appreciation. So we need to be true to who we are, and remember what’s really important to us. That’s what is going to keep Apple Apple, if we keep us us.
</p></blockquote>
<p>I don’t know when Jobs said those words, but one of the most compelling examples of what he meant was, by definition, yet to come. My mind immediately went to the days after Jobs passed away in October 2011:</p>
<p><a href="https://www.flickr.com/photos/astrozombie/6239747376/in/photolist-avokvj-atrn7h-augVE2-avRq5y-atfWz8-avkenw-av1Ahi-av1AhP-av1Ahr-av1AhH-auMaXs-av4ksy-auMn21-aw6YNk-av1CdB-augLTp-auh2kX-auh6hp-augJvM-aujy5A-aujRb7-aujCr5-rK3dbh-avZb8T-avZb8X-2oyvb-p1Ww6W-p3H5UT-p1WvXu-p4jBAB-6mhSKe-aujUUQ-aujruJ-aujqhf-aujSVY-auh3E4-augTuR-aujD4E-aujvEb-augWzt-aujusA-aujBPS-aujtTm-aujth3-auh7xc-augH9D-aujTvJ-aujwMS-auhf8V-aujpFN"></a></p>
<p>Spontaneously, all over the world, makeshift memorials, usually around Apple Stores, sprang up to honor someone whom, to paraphrase Jobs, they never met, whose hand they never shook, who never heard their story — and frankly, had they met Jobs, they very well might have regretted the experience!</p>
<p>That, though, was Jobs’ point: the stories of his mistreatment of those closest to him, both professional and deeply personal, reveal the man’s weaknesses; I see no need and have no desire to whitewash them. The company he built and the products that engendered such a deep emotional attachment in their owners, though, captured his strengths — and Apple’s customers felt his appreciation. You might call it love.</p>
<p>To return to Lennon’s words, Jobs, particularly in his second stint at Apple, had learned how to be himself: less designer than editor-in-chief, Jobs not only drove those he worked with to create “with great deal of care”, he also set Apple on a path towards being its best self. That, famously, means the integration of hardware and software, but at least in the case of the iPhone, the pertinent integration goes down to the silicon.</p>
<p>To that end, the products Apple unveiled at the new Steve Jobs Theater could not have been more appropriate: a cellular watch significantly smaller than competitors with comparable battery life, a new iPhone 8 improved in virtually every dimension, and, of course, the iPhone X, with nearly every new feature dependent on that integration.<a href="https://stratechery.com/2017/the-lessons-and-questions-of-the-iphone-x-and-the-iphone-8/#footnote_1_2731">2</a></p>
<h4>About That Notch</h4>
<p>Apple clearly decided to not minimize the notch, the black cut-out at the top of the iPhone X that houses an array of sensors and cameras. If anything, the company went out of its way to emphasize it, including playing video such that the notch obscured what was being shown (that is actually an optional view; by default video is letter-boxed such that it avoids the notch).</p>
<p><a href="https://www.apple.com/apple-events/september-2017/"></a></p>
<p>I think the emphasis on the notch served another purpose, though: it is, in its own way, something that only Apple can do.<a href="https://stratechery.com/2017/the-lessons-and-questions-of-the-iphone-x-and-the-iphone-8/#footnote_2_2731">3</a> First, the operating system needed to be modified to work around the notch. Only Apple has sufficient control of the entire stack that they can pull off such a radical overhaul in software to accommodate the change in hardware. Second, applications will need to be reworked to look their best; thousands of developers are hard at work today doing just that, because the iOS ecosystem is so valuable.</p>
<p>Moving beyond the notch, Apple is also demonstrating its power over users; using an iPhone X is going to be significantly different than any other previous iPhone. Everything has changed, from unlocking the phone to invoking Siri to exiting apps to multi-tasking to Apple Pay. And yet there is little doubt that millions will do just that (and, naturally, insist that the new way is obviously better).</p>
<p>What is and remains so brilliant about the iPhone specifically and Apple’s business broadly is how everything is aligned around Apple being the Apple Jobs envisioned: a company that shows its “appreciation to the rest of humanity [by making] something wonderful and put[ting] it out there.” By making the <a href="https://stratechery.com/2014/best/">best</a> products Apple earns loyal customers willing to pay a premium; loyal customers give Apple both freedom to make large scale changes and also a point of leverage against partners like <a href="https://stratechery.com/2013/why-do-carriers-subsidize-the-iphone/">carriers</a> and developers. And then, the resultant profits lets Apple buy the small companies and do the R&amp;D to create the next set of products.</p>
<p>This has been the story of the iPhone: for ten years every single model has been a meaningful jump over the previous one, giving Apple a stranglehold on the top of the market. There was no further segmentation needed: the smartphone market was growing around the world, and Apple was taking the premium part. Indeed, the company’s one misstep — 2013’s iPhone 5C — came from a misguided attempt to go downmarket.</p>
<h4>The iPhone 5C</h4>
<p>“Misstep” is perhaps a bit harsh. What we know about the iPhone 5C is this: in 2013 Apple was under tremendous external pressure, not just in the press but especially on Wall Street<a href="https://stratechery.com/2017/the-lessons-and-questions-of-the-iphone-x-and-the-iphone-8/#footnote_3_2731">4</a>, to produce a lower-cost iPhone. Most analysts were convinced the company had not just saturated the high end but was in imminent danger of being disrupted by cheaper good-enough Android phones<a href="https://stratechery.com/2017/the-lessons-and-questions-of-the-iphone-x-and-the-iphone-8/#footnote_4_2731">5</a>, and speculation was rampant that Apple would release a new iPhone at a significantly lower price point.</p>
<p>Apple went the other way; <a href="https://stratechery.com/2013/two-minutes-fifty-six-seconds/">in one of my favorite Apple keynotes</a>, the company stuck to the high end. The iPhone 5C was cheaper than the 5S, announced on the same day, but only by $100; it was effectively a replacement for the iPhone 5 in terms of Apple’s previous practice of selling previous iPhones at a lower price.</p>
<p>Still, I for one thought it would sell very well; all indications are that Apple agreed, but it quickly became apparent that customers overwhelmingly preferred the iPhone 5S. Apple struggled to keep the latter in stock, having produced far too many 5Cs, and the model was quietly discontinued two years later. I <a href="https://stratechery.com/2015/the-iphone-6s-the-end-of-the-iphone-5c-the-iphone-upgrade-program/">wrote at the time</a>:</p>
<blockquote><p>
  The problem with the 5C, though, is that it wasn’t an iPhone. Well, technically it was — it was made by Apple, after all — but particularly in Asia, and especially in China, an iPhone is about more than even the hardware and software that Apple is so proud of integrating. It is the device to own for <a href="https://stratechery.com/2015/daily-update-apple-china-and-the-upper-middle-class-apples-earnings-apple-pay-coming-to-best-buy/">emerging upper middle class consumers</a>, and what is brilliant about the sell-old-flagship-iPhones strategy is that it allows the cheaper iPhones to punch above their weight: after all, that iPhone 5S you pull out of your pocket and casually place on the table may be brand new today (because you can only afford $450), or you may simply have not yet replaced the iPhone you bought at full price when it came out. Regardless, you have a flagship; the 5C, on the other hand, was from day one not the flagship, and quite obvious about it (one is reminded of Jony Ive calling it “unapologetically plastic”). To buy a 5C was to show you couldn’t afford a better one.
</p></blockquote>
<p>The 5C’s failure, such that it was, showed that the iPhone had three distinct markets:</p>
<ul>
<li>Customers who wanted the best possible phone. They bought the 5S.</li>
<li>Customers who wanted the prestige of owning the highest-status phone on the market. Heavily concentrated in China, they bought the gold 5S.</li>
<li>Customers who aspire to owning a top-of-the-line iPhone, but couldn’t afford one. They bought the 4S instead of the 5C.</li>
</ul>
<p>What was missing was the cost-conscious customer; the truth is that if price is the priority an Android phone will always win. By 2013 even the cheapest phones were “good enough”; only people who cared about owning an iPhone would pay more,<a href="https://stratechery.com/2017/the-lessons-and-questions-of-the-iphone-x-and-the-iphone-8/#footnote_5_2731">6</a> and if they were going to pay more of course they wanted the best, or at least a phone that gave off the prestige of having been the best at some point in time.</p>
<h4>More Lessons Learned</h4>
<p>A year later Apple (finally) released two iPhones with significantly larger screens: the iPhones 6 and 6 Plus. The response was incredible: iPhone sales jumped a staggering 45% year-over-year. That, though, made the iPhone 6S a much tougher sale. It became clear that <a href="https://stratechery.com/2016/doubting-the-iphone-revisted-what-has-changed-on-being-bearish/">Apple had pulled forward</a> some number of upgraders to the iPhone 6, even as other customers held onto their good-enough phones for longer.</p>
<p>The iPhone 7 cemented this view: growth returned inline with models that <a href="https://stratechery.com/2017/apples-china-problem/">presumed that the iPhone 6 pulled forward upgrades</a> in an increasingly saturated market; Apple was no longer benefiting from overall smartphone growth, but the company also wasn’t losing customers to Android — if anything, it was gaining them.</p>
<p>The one exception was China. As I noted in <a href="https://stratechery.com/2017/apples-china-problem/">Apple’s China Problem</a>, the iPhone was growing all over the world but shrinking in China, and I blamed WeChat:</p>
<blockquote><p>
  WeChat works the same on iOS as it does on Android. That, by extension, means that for the day-to-day lives of Chinese there is no penalty to switching away from an iPhone. Unsurprisingly, in stark contrast to the rest of the world, <a href="https://mp.weixin.qq.com/s?__biz=MjM5ODEyOTAyMA==&amp;mid=2661906770&amp;idx=2&amp;sn=73662bbef300beda9237d30adaf374fa&amp;chksm=bd92a5418ae52c57eb1c238a34b9f8a6418993d3384ec331758b4ae102351a4e04d2ed878336&amp;mpshare=1&amp;scene=1&amp;srcid=0504Q1KjS8LTXMPois1oL3yP&amp;key=c84baf55e7d27452d7d25e8d4216f3083fd387d19f1c1a67737c24d1dd9b45dae04bc0e728d03eada93643c901672625aa3e51158cc3bfff1934908b5c0229bee273701f7cf7b5d3c55a6dfefacb6747&amp;ascene=0&amp;uin=MTY1ODQwOTc0MA%3D%3D&amp;devicetype=iMac+MacBookAir6%2C2+OSX+OSX+10.11.6+build(15G1421)&amp;version=12020610&amp;nettype=WIFI&amp;fontScale=100&amp;pass_ticket=txq8pNJOZPQCgr2USCGKoo4Q6ywoGnIyfwIIr0PTnlFp6hwHmmCF%2FvRWzEnCnVY2">according to a report earlier this year</a> only 50% of iPhone users who bought another phone in 2016 stayed with Apple:</p>
<p>  </p>
<p>  This is still better than the competition, but compared to the <a href="http://www.businessinsider.com/iphone-users-abandon-loyalty-to-apple-2016-11">80%+ retention rate Apple enjoys in the rest of the world</a>, it is shockingly low, and the result is that the iPhone has slid down China’s sales rankings: iPhone sales were <a href="https://www.bloomberg.com/news/articles/2017-02-06/oppo-huawei-widen-lead-in-china-as-apple-shipments-plummet">only 9.6% of the market last year</a>, behind local Chinese brands like Oppo, Huawei and Vivo. All of those companies sold high-end phones of their own; the issue isn’t that Apple was too expensive, it’s that the iPhone 6S and 7 were simply too boring.
</p></blockquote>
<p>There was one more lesson learned from the iPhone 7: for the first time Apple raised prices. Specifically, the iPhone 7 Plus was $769, $20 more than the iPhone 6S Plus at launch; the iPhone 7 pricing was identical to the iPhone 6S ($650). Theoretically this should have curbed demand for the 7 Plus, but the opposite happened: Apple sold <em>more</em> 7 Pluses relative to the 7 than they did 6S Pluses relative to the 6S. To be clear, I don’t think they sold more <em>because</em> of the price change; rather, consumer preferences continued to move towards bigger phones and, at least for an iPhone buyer, price simply isn’t the top priority.</p>
<h4>Apple’s New iPhone Strategy</h4>
<p>Forgive the long-winded history of the iPhone, but I think it is critical to understand Apple’s thinking when it comes to this year’s announcements; I think all of the lessons I referenced above influenced this lineup:</p>
<p></p>
<p>Start at the top. The iPhone X sells to two of the markets I identified above:</p>
<ul>
<li>Customers who want the best possible phone</li>
<li>Customers who want the prestige of owning the highest-status phone on the market</li>
</ul>
<p>Note that both of these markets are relatively price-insensitive; to that end, $999 (or, more realistically, $1149 for the 256 GB model), isn’t really an obstacle. For the latter market, it’s arguably a positive.</p>
<p>The iPhone 8 (and 8 Plus), meanwhile, serves the slow and steady markets that bought the iPhone 7: previous iPhone owners upgrading and Android switchers. Critically, the iPhone 8 also serves those folks who aspire to an iPhone. No, they can’t afford an iPhone 8, but the iPhone 6S they can afford looks almost exactly the same, and in a few years the iPhone 8 will still be viewed as a once-flagship (the SE, meanwhile, <a href="https://stratechery.com/2016/andy-grove-and-the-iphone-se/">deliberately apes the shape of the once-flagship 5S</a>). Oh, and by the way, Apple is raising the price on the 8 as well: if price isn’t the chief factor, how far can you go?</p>
<h4>Apple’s Risk</h4>
<p>That said, I think Apple is taking a pretty significant risk with the iPhone 8 in particular: we know the company can succeed by selling the “best” phone, but the one example we have of building a less-than-best phone was underwhelming; to that end, how many iPhone buyers will forgo the 8 to wait for the X?</p>
<p>In some respects this is a good problem to have — customers wanting to give you more money for a more expensive phone — but the fact the iPhone X is not launching until November suggests it is well behind in production, which further suggests supply will be limited for some time to come. It is quite possible that Apple’s fiscal Q12018 results will be depressed by limited supply.</p>
<p>Of course this is a short-term problem; I do expect the iPhone X to be a massive hit in China in particular. Indeed, I wouldn’t be surprised if most of the early iPhone X supply were earmarked for the country. My argument about WeChat’s effect on Apple is that it elevates the importance of fresh hardware designs over iOS when it comes to iPhone sales; iPhone X is as fresh as it gets.</p>
<hr />
<p>There’s one more verse in Lennon’s song:</p>
<div><em><br />
Nothing you can know that isn’t known<br />
Nothing you can see that isn’t shown<br />
Nowhere you can be that isn’t where you’re meant to be<br />
It’s easy
</em></div>
<p></p>
<p>I have to disagree: I don’t know if Apple can segment the iPhone market; what has been shown is that they can’t, that the iPhone can only be the best, nothing less.</p>
<p>That is why I find this launch so fascinating, and will be watching the upcoming quarter’s results so closely: Jobs built Apple to be the best, and the company has succeeded by being exactly that. Does that foreclose the possibility of also being really good, and the gains from market segmentation that follow?</p>
<ol><li>Unfortunately, the <a href="https://www.apple.com/apple-events/september-2017/">video of the event</a> no longer includes the opening with The Beatles song; you’ll have to take my word for it</li><li>I will cover all aspects of Apple’s keynote in tomorrow’s Daily Update</li><li>Other phones, like the <a href="https://www.essential.com">Essential Phone</a>, do have a much smaller cutout; the operating system isn’t re-worked to the degree the iOS is for the iPhone X, though, nor will developers put in special work</li><li>The stock had fallen to $55.79 in April 2013, nearly half the price of a year earlier</li><li>In a fortuitous coincidence, I started Stratechery in 2013, and I got a lot of early traction by arguing that Apple was fine</li><li>I’m generalizing here; some customers genuinely prefer Android and primarily bought Samsung; Apple dominated the high end though</li></ol>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://www.sandimetz.com/blog/2017/9/13/breaking-up-the-behemoth">Breaking Up the Behemoth</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://www.sandimetz.com/blog?format=RSS">Sandi Metz</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">practices</span>
              <span class="tag">architecture</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Wed Sep 13 2017 12:00:00 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>Good Morning,</p>

<p>I've been thinking about how applications evolve, and what we might do if we're unhappy with the results.  Three apparently unrelated ideas have been percolating in my head.  In this newsletter I'll introduce each one and connect them together, in hopes that understanding these connections will help us understand our apps.</p>

<p>These thoughts are very definitely my opinion, justified only by past experience.  YMMV, but it'll give you something to think about.  :-) Expect lots of pictures.  Imagine me waving my arms and drawing on the whiteboard.</p>

<p>The first idea is <a href="https://martinfowler.com/" target="\_blank">Martin Fowler's</a> <a href="https://martinfowler.com/bliki/DesignStaminaHypothesis.html"> Design Stamina Hypothesis</a>.</p>

<h3>#1: Design Stamina Hypothesis</h3>

<p>Fowler illustrates this idea with the following graph.</p>

<p><a href="https://martinfowler.com/bliki/DesignStaminaHypothesis.html" target="\_blank">
  
</a> <br />
<strong><em>Figure 1</em></strong></p>

<p>The vertical axis above represents cumulative functionality.  The higher the line, the more that got done. The horizontal axis represents time passing.  The further to the right, the later in time.</p>

<p>Two different lines are plotted.  The orange line illustrates how much functionality you will produce by any point in time by investing in design from day one.  The blue line illustrates the outcome if you defer serious design.  Notice that the blue line rises fastest early on, but that the orange line eventually overtakes it.  </p>

<p>The Design Stamina Hypothesis suggests that early on in a project you'll get more done if you don't bother too much with design, but a point will come when you'll be better off if you invest some energy into it.</p>

<p>The next idea is about the difference between procedural and object-oriented code.</p>

<h3>#2: Procedural vs Object-Oriented Code</h3>

<p>The section compares procedural and OO code in terms of changeability and understandability.  The following graph explores the trade-offs between the two.</p>

<p> <br />
<strong><em>Figure 2</em></strong></p>

<p>Changeability is plotted on the vertical axis above.  Code that is easier to change goes at the bottom.  Code that is harder to change, at the top.</p>

<p>Understandability is represented by the horizontal axis.  Easier to understand code goes to the left, harder to understand code, to the right.</p>

<p>A simple procedure is merely a list of steps.  Simple procedures are easy to understand and easy to change, and so fall into the lower left part of the graph above.  This is the most cost-efficient place to be.</p>

<p>For some problems, simple, non-conditional, non-duplicated procedural code is the best solution.  What could be cheaper?  Write the code and run.</p>

<p>Over time, however, the situation might change.  A new feature request might force the addition of conditional logic, or the duplication of parts of the solution in other places, leading to Figure 3 below.</p>

<p> <br />
<strong><em>Figure 3</em></strong></p>

<p>Above, the cost-effective procedure has morphed into a complicated, condition-laden, duplicative morass of code that's hard to understand or change.</p>

<p>Simple procedures are cheap.  Complicated procedures are expensive.  The only compliment you can pay a complicated procedure (and this is really scraping the bottom of the barrel) is to say that at least all of the #$%@! code is in one place.  However, proximity alone is not enough to justify this complexity.  There are more cost-effective ways to arrange code.</p>

<p>The next graph adds object-oriented code to the mix. Notice that the OO solution is a little more costly than a simple procedure but far less costly than a complex one.</p>

<p> <br />
<strong><em>Figure 4</em></strong></p>

<p>In object-oriented solutions, small, interchangeable objects collaborate by sending messages.  Messages afford seams which allow you to replace existing objects with new ones that play the same role.  Message sending makes it easy to change behavior by swapping in new parts.  </p>

<p>Another consequence of message sending is that it obscures the <em>details</em> of what happens as a result.  From the senders' point of view, a message represents only an intention.  The receiver of the message supplies the implementation, which is hidden from the sender.  Messages bestow effortless <em>local</em> substitutability at the cost of ignorance of <em>distant</em> implementation.  </p>

<p>Relative to complex procedures, OO is easier to understand and change.  Relative to  <em>simple</em> procedures, OO can be as easy to change, but might well be harder to understand <em>as a whole</em>.</p>

<p>So, OO isn't a slam-dunk, hands-down winner.  It depends on the complexity of your problem and the longevity of your application.</p>

<p>Speaking of longevity, let's move on to the final idea, churn.</p>

<h3>#3: Churn and Complexity</h3>

<p><a href="https://michaelfeathers.silvrback.com/" target="_blank">Michael Feathers'</a> <a href="https://www.stickyminds.com/article/getting-empirical-about-refactoring" target="_blank">Getting Empirical about Refactoring</a> article introduces the idea of <em>churn</em>.  Churn is a measure of how often a file changes.  Files that change more have higher churn.  </p>

<p>Churn is interesting in isolation, but it's even more useful to consider churn alongside complexity.  Feathers' article includes the following chart, to which I've added the curved green line.</p>

<p><a href="https://www.stickyminds.com/article/getting-empirical-about-refactoring" target="_blank">
  
</a> <br />
<strong><em>Figure 6</em></strong></p>

<p>Above, churn is on the horizontal axis.  Code complexity is on the vertical.  </p>

<p>Complicated code that rarely changes appears in the upper left quadrant of this graph.  We abhor complication, but if the code never changes, it's not costing us money.  Pretend the code is a cabinet overstuffed with teetering Tupperware: just quietly press the door closed and sneak away.  Ignore code in this quadrant until it starts to churn.</p>

<p>Simple code that changes a lot falls into the lower right quadrant.  If code is simple enough (think <em>configuration file</em>) it will be cheap to change.  Change this code as often as necessary, as long as it remains simple.</p>

<p>The lower left quadrant contains things that aren't very complicated and that don't change much, so this code is already cost-effective and can also be ignored.</p>

<p>The green line curves from upper left, through lower left, and into lower right.  This illustrates the curve around which we'd like the code in our apps to cluster.  Notice that this line does <em>not</em> enter the top right quadrant.</p>

<p>The top right quadrant reflects complicated code that changes often.  By definition, code like this will be hard to understand and difficult to change.  We'd prefer this quadrant to be empty, so code that slithers into it should be refactored right back out of it.</p>

<p>Now that we have these ideas in common, I can lean on them to explain a way in which applications go wrong.</p>

<h3>Code Evolves Towards a Predictable Kind Of Mess</h3>

<p>There's a kind of code mess I see repeatedly, where'er I travel.  Courtesy of <a href="https://codeclimate.com/" target="_blank">Code Climate</a>, here are a few graphs that expose its symptoms in several projects (as of Sept 7, 2017).</p>

<p><a href="https://codeclimate.com/github/angular/angular.js/trends/churn" target="_blank">
  
</a> <br />
<strong><em>Figure 7</em></strong></p>

<p><a href="https://codeclimate.com/github/discourse/discourse/trends/churn" target="_blank">
  
</a> <br />
<strong><em>Figure 8</em></strong></p>

<p><a href="https://codeclimate.com/github/gitlabhq/gitlabhq/trends/churn" target="_blank">
  
</a> <br />
<strong><em>Figure 9</em></strong></p>

<p>The <em>Churn vs. quality</em> charts above are Code Climate's variant of Michael Feathers' <em>File Churn vs. Complexity</em> idea.  Notice that on each of these charts the points cluster around a curve similar to the green line in Figure 6.  This is good.  It is commendable that in these applications most of the complex code changes little, and most changes are to simple code.</p>

<p>However, each of these graphs also contains an unwanted outlier that resides in the top right quadrant.  I'm not familiar with the source code for these apps, but sight unseen I feel confident making a few predictions about the outlying classes.  I suspect that they:</p>

<ol>
<li>are larger than most other classes,</li>
<li>are laden with conditionals, and</li>
<li>represent core concepts in the domain</li>
</ol>

<p><em>You can verify this, if you care to, by clicking on each graph above to open the corresponding page in Code Climate.  Once there, click on the dot of the outlier to link to the underlying code.  As I've already confessed, I don't really know these apps so what do I know...but based on name, size, complexity and churn, I can't help but believe I'm correct.  If I'm wrong, or if the code has changed by the time you look, just ignore the example and continue to trust the principle. :-)</em></p>

<p>Many applications express this pattern.  Much of their code is fairly understandable and reasonably easy to change.  However, they also contain one or two large, complex, and constantly churning classes that represent extremely important ideas in their domain.  </p>

<p>Everyone hates working on these outlier classes.  To touch them is to break them.  The tests don't provide safety.  And no attempted cure helps.  Despite best efforts, these classes continue to grow in size and complexity.  They're headed from bad to worse.</p>

<p>How does this happen?</p>

<p>We can use a combination of the first three ideas to explain the problem, and understanding the problem offers hope of preempting it.</p>

<p>I posit the following:</p>

<ol>
<li><p>If you do design too early, you'll waste your efforts. <br />
(<em>Figure 1 - The orange line early in time</em>)</p></li>
<li><p>If you never do design, your code will become a painful mess. <br />
(<em>Figure 1 - The blue line late in time</em>)</p></li>
<li><p>A time will come when investing in design will save you money. <br />
(<em>Figure 1 - Where the lines cross</em>)</p></li>
<li><p>Simple procedures require little design and are cheap to maintain. <br />
(<em>Figure 2, Figure 1: The blue line early</em>)</p></li>
<li><p>Procedures become more complex over time, and more expensive to maintain. <br />
(<em>Figure 3, Figure 1: The blue line late in time</em>)</p></li>
<li><p>Object-oriented code is more cost-effective than complex procedural code. <br />
(<em>Figure 4</em>)</p></li>
<li><p>The procedures <em>that are most important to your domain</em> change more than those that are incidental to your domain.  </p></li>
<li><p>The procedures <em>that are important to your domain</em> increase in complexity faster than other code.  </p></li>
<li><p>It's difficult to be aware of the exact moment when your application crosses the design payoff line. <br />
(<em>Figure 1</em>)</p></li>
<li><p>You become aware that you have passed the design payoff line because velocity slows and suffering increases.
(<em>Figure 1 - blue line beyond the design payoff line</em>)</p></li>
<li><p>The most important code will be the most out-of-control by the time you realize you've passed the design payoff line.</p></li>
<li><p>Moderately complex procedures are easy to convert to OO.</p></li>
<li><p>Extremely complex procedures are more difficult to convert to OO.</p></li>
<li><p>Your attempts to convert moderately complicated procedures to OO generally <strong>succeed</strong>.</p></li>
<li><p>Your attempts to convert extremely complicated procedures to OO often <strong>fail</strong>.</p></li>
</ol>

<p>This is how we end up with applications where many small, efficient classes coexist alongside one costly, massive, condition-filled behemoth.  A series of small, innocent changes turned the application's most important code into a class so complex that no one could fix it.  The problem becomes visible at item 15 above, but its roots lie in item 8, where tiny bits of complexity got added, repeatedly, until the logic passed the point of no return.</p>

<p>Sticking with procedures too long is just as bad as doing design too soon.  If important classes in your domain change often, get bigger every time they change, and are accumulating conditionals, stop adding to them right now. Use every new request as an opportunity to do a bit of design.  Implement the change using small, well-designed classes that collaborate with the existing object.</p>

<p>A 5,000 line class exerts a gravitational pull that makes makes it hard to imagine creating a set of 10 line helper classes to meet a new requirement. Make new classes anyway.  The way to get the outliers back on the green line where they belong is to resist putting more code in objects that are already too large.  Make small objects, and over time, the big ones will disappear.</p>

<p>Best, <br />
Sandi</p>

<h3>News:</h3>

<h5>Public POOD course in October in beautiful North Carolina</h5>

<p>I'm teaching a public <a href="https://www.sandimetz.com/courses/" target="\_blank">Practical Object-Oriented Design course</a> in Durham, NC on Oct 25-27, 2017, which we're fondly referring to as <a href="https://www.sandimetz.com/poodnc-2017/" target="\_blank">POODNC</a>.  This is your chance to spend three days with like-minded peers, changing the way you think about objects.  Your applications will never be the same.</p>

<p>A few <a href="https://ti.to/torqueforge/poodnc-october-2017" target="\_blank">tickets</a> remain.   <a href="https://ti.to/torqueforge/poodnc-october-2017" target="\_blank">Get yours</a> before they're gone!</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://blog.mecheye.net/2017/09/i-dont-know-who-the-web-audio-api-is-designed-for/">I don’t know who the Web Audio API is designed for</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="http://blog.mecheye.net/feed/">Clean Rinse</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">web</span>
              <span class="tag">linux</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Wed Sep 13 2017 07:44:23 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>WebGL is, all things considered, a pretty decent API. It’s not a great API, but that’s just because OpenGL is also not a great API. It gives you raw access to the GPU and is pretty low-level. For those intimidated by something so low-level, there are quite a few higher-level engines like <a href="https://threejs.org/">three.js</a> and <a href="https://docs.unity3d.com/Manual/webgl-building.html">Unity</a> which are easier to work with. It’s a good API with a tremendous amount of power, and it’s the best portable abstraction we have for a good way to work with the GPU on the web.</p>
<p>HTML5 Canvas is, all things considered, a pretty decent API. It has plenty of warts: <a href="https://lists.w3.org/Archives/Public/public-whatwg-archive/2014May/0164.html">lack of colorspace</a>, you can’t directly draw DOM elements to a canvas without <a href="https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API/Drawing_DOM_objects_into_a_canvas">awkwardly porting it to an SVG</a>, blurs are strangely hidden from the user into a “shadows” API, and a few other things. But it’s honestly a good abstraction for drawing 2D shapes.</p>
<p><a href="https://webaudio.github.io/web-audio-api/">Web Audio</a>, conversely, is an API I do not understand. The scope of Web Audio is hopelessly huge, with features I can’t imagine anybody using, core abstractions that are hopelessly expensive, and basic functionality basically missing. To quote the specification itself: “It is a goal of this specification to include the capabilities found in modern game audio engines as well as some of the mixing, processing, and filtering tasks that are found in modern desktop audio production applications.”</p>
<p>I can’t imagine any game engine or music production app that would want to use <em>any</em> of the advanced features of Web Audio. Something like the <a href="https://webaudio.github.io/web-audio-api/#the-dynamicscompressornode-interface">DynamicsCompressorNode</a> is practically a joke: basic features from a real compressor are basically missing, and the behavior that is there is underspecified such that I can’t even trust it to sound correct between browsers. More than likely, such filters would be written using asm.js or WebAssembly, or ran as Web Workers due to the rather stateless, input/output nature of DSPs. Math and tight loops like this aren’t hard, and they aren’t rocket science. It’s the only way to ensure correct behavior.</p>
<p>For people that do want to do such things: compute our audio samples and then play it back, well, the APIs make it near impossible to do it in any performant way.</p>
<p>For those new to audio programming, with a traditional sound API, you have a buffer full of samples. The hardware speaker runs through these samples. When the API thinks it is about to run out, it goes to the program and asks for more. This is normally done through a data structure called a “<a href="https://en.wikipedia.org/wiki/Circular_buffer">ring buffer</a>” where we have the speakers “chase” the samples the app is writing into the buffer. The gap between the “read pointer” and the “write pointer” speakers is important: too small and the speakers will run out if the system is overloaded, causing crackles and other artifacts, and too high and there’s a noticeable lag in the audio.</p>
<p>There’s also some details like how many of these samples we have per second, or the “sample rate”. These days, there are two commonly used sample rates: 48000Hz, in use by most systems these days, and 44100Hz, which, while a bit of a strange number, rose in popularity due to its use in <a href="https://en.wikipedia.org/wiki/Compact_Disc_Digital_Audio">CD Audio</a> (why 44100Hz for CDDA? Because Sony, one of the organizations involved with the CD, cribbed CDDA from an earlier digital audio project it had lying around, the <a href="https://en.wikipedia.org/wiki/U-matic#Digital_audio">U-matic</a> tape). It’s common to see the operating system have to convert to a different sample rate, or “resample” audio, at runtime.</p>
<p>Here’s an example of a theoretical, non-Web Audio API, to compute and play a <a href="https://www.youtube.com/watch?v=gPBgWWZhcA4">440Hz sine wave</a>.</p>
<pre>
const frequency = 440; // 440Hz A note.
 // 1 channel (mono), 44100Hz sample rate
const stream = window.audio.newStream(1, 44100);
stream.onfillsamples = function(samples) {
    // The stream needs more samples!
    const startTime = stream.currentTime; // Time in seconds.
    for (var i = 0; i &lt; samples.length; i++) {
        const t = startTime + (i / stream.sampleRate);
        // samples is an Int16Array
        samples[i] = Math.sin(t * frequency) * 0x7FFF;
    }
};
stream.play();
</pre>
<p>The above, however, is nearly impossible in the Web Audio API. Here is the closest equivalent I can make.</p>
<pre>
const frequency = 440;
const ctx = new AudioContext();
// Buffer size of 4096, 0 input channels, 1 output channel.
const scriptProcessorNode = ctx.createScriptProcessorNode(4096, 0, 1);
scriptProcessorNode.onaudioprocess = function(event) {
    const startTime = ctx.currentTime;
    const samples = event.outputBuffer.getChannelData(0);
    for (var i = 0; i &lt; 4096; i++) {
        const t = startTime + (i / ctx.sampleRate);
        // samples is a Float32Array
        samples[i] = Math.sin(t * frequency);
    }
};
// Route it to the main output.
scriptProcessorNode.connect(ctx.destination);
</pre>
<p>Seems similar enough, but there are some important distinctions. First, well, this is deprecated. Yep. <a href="https://developer.mozilla.org/en-US/docs/Web/API/ScriptProcessorNode">ScriptProcessorNode has been deprecated</a> in favor of Audio Workers since 2014. Audio Workers, by the way, don’t exist. Before they were ever implemented in any browser, they were replaced by the <a href="https://webaudio.github.io/web-audio-api/#AudioWorklet">AudioWorklet</a> API, which doesn’t have any implementation in browsers.</p>
<p>Second, the sample rate is global for the entire context. There is no way to get the browser to resample dynamically generated audio. Despite the browser requiring having fast resample code in C++, this isn’t exposed to the user of ScriptProcessorNode. The sample rate of an AudioContext isn’t defined to be 44100Hz or 48000Hz either, by the way. It’s dependent on not just the browser, but also the operating system and hardware of the device. <a href="https://bugs.webkit.org/show_bug.cgi?id=154538">Connecting to Bluetooth headphones</a> can cause the sample rate of an AudioContext to change, without warning.</p>
<p>So ScriptProcessorNode is a no go. There is, however, an API that lets us provide a differently sampled buffer and have the Web Audio API play it. This, however, isn’t a “pull” approach where the browser fetches samples every once in a while, it’s instead a “push” approach where we play a new buffer of audio every so often. This is known as BufferSourceNode, and it’s what emscripten’s SDL port uses to play audio. (they used to use ScriptProcessorNode but then <a href="https://github.com/kripken/emscripten/commit/9440047e4b99425032874d870683ca4a3c833e35">removed it because it didn’t work good, consistently</a>)</p>
<p>Let’s try using BufferSourceNode to play our sine wave:</p>
<pre>
const frequency = 440;
const ctx = new AudioContext();
let playTime = ctx.currentTime;
function pumpAudio() {
    // The rough idea here is that we buffer audio roughly a
    // second ahead of schedule and rely on AudioContext's
    // internal timekeeping to keep it gapless. playTime is
    // the time in seconds that our stream is currently
    // buffered to.

    // Buffer up audio for roughly a second in advance.
    while (playTime - ctx.currentTime &lt; 1) {
        // 1 channel, buffer size of 4096, at
        // a 48KHz sampling rate.
        const buffer = ctx.createBuffer(1, 4096, 48000);
        const samples = buffer.getChannelData(0);
        for (let i = 0; i &lt; 4096; i++) {
            const t = playTime + Math.sin(i / 48000);
            samples[i] = Math.sin(t * frequency);
        }

        // Play the buffer at some time in the future.
        const bsn = ctx.createBufferSource();
        bsn.buffer = buffer;
        bsn.connect(ctx.destination);
        // When a buffer is done playing, try to queue up
        // some more audio.
        bsn.onended = function() {
            pumpAudio();
        };
        bsn.start(playTime);
        // Advance our expected time.
        // (samples) / (samples per second) = seconds
        playTime += 4096 / 48000;
    }
}
pumpAudio();
</pre>
<p>There’s a few… unfortunate things here. First, we’re basically relying on floating point timekeeping in seconds to keep our playback times consistent and gapless. There is no way to reset an AudioContext’s currentTime short of constructing a new one, so if someone wanted to build a professional Digital Audio Workstation that was alive for days, precision loss from floating point would become a big issue.</p>
<p>Second, and this was also an issue with ScriptProcessorNode, the samples array is full of floats. This is a minor point, but forcing everybody to work with floats is going to be slow. <a href="https://people.xiph.org/~xiphmont/demo/neil-young.html">16 bits is enough for everybody</a> and for an output format it’s more than enough. Integer Arithmetic Units are <em>very</em> fast workers and there’s no huge reason to shun them out of the equation. You can <em>always</em> have code convert from a float to an int16 for the final output, but once something’s in a float, it’s going to be slow forever.</p>
<p>Third, and most importantly, we’re allocating two new objects per audio sample! Each buffer is roughly <a href="https://www.google.com/search?q=%284096+%2F+48000%29+seconds+in+milliseconds">85 milliseconds long</a>, so every 85 milliseconds we are allocating two new GC’d objects. This could be mitigated if we could use an existing, large ArrayBuffer that we slice, but we can’t provide our own ArrayBuffer: createBuffer creates one for us, for each channel we request. You might imagine you can createBuffer with a very large size and play only small slices in the BufferSourceNode, but there’s no way to slice an AudioBuffer object, nor is there any way to specify an offset into the corresponding with a AudioBufferSourceNode.</p>
<p>You might imagine the best solution is to simply keep a pool of BufferSourceNode objects and recycle them after they are finished playing, but BufferSourceNode is designed to be a one-time-use-only, fire-and-forget API. The documentation <a href="https://developer.mozilla.org/en-US/docs/Web/API/AudioBufferSourceNode">helpfully states</a> that they are “cheap to create” and they “will automatically be garbage-collected at an appropriate time”.</p>
<p>I know I’m fighting an uphill battle here, but a GC is not what we need during realtime audio playback.</p>
<p>Keeping a pool of AudioBuffers seems to work, though in <a href="http://magcius.github.io/spc.js/spc.html">my own test app</a> I still see slow growth to 12MB over time before a major GC wipes, according to the Chrome profiler.</p>
<p>What makes this so much more ironic is that a very similar API was proposed by Mozilla, called the <a href="https://wiki.mozilla.org/Audio_Data_API">Audio Data API</a>. It’s three functions: setup(), currentSampleOffset(), and writeAudio(). It’s still a push API, not a pull API, but it’s very simple to use, supports resampling at runtime, doesn’t require you to break things up into GC’d buffers, and doesn’t have any.</p>
<p>Specifications and libraries can’t be created in a vacuum. If we instead got the simplest possible interface out there and let people play with it, and then took some of the more slow bits people were implementing in JavaScript (resampling, FFT) and put them in C++, I’m sure we’d see a lot more growth and usage than what we do today. And we’d have actual users for this API, and real-world feedback from users using it in production. But instead, the biggest user of Web Audio right now appears to be emscripten, who obviously won’t care much for any of the graph routing nonsense, and already attempts to work around the horrible APIs themselves.</p>
<p>Can the ridiculous overeagerness of Web Audio be reversed? Can we bring back a simple “play audio” API and bring back the performance gains once we see what happens in the wild? I don’t know, I’m not on these committees, I don’t even work in web development other than fooling around on nights and weekends, and I certainly don’t have the time or patience to follow something like this through.</p>
<p>But I would really, really like to see it happen.</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://8thlight.com/blog/ray-hightower/2017/09/13/how-to-grow-a-user-group-remix.html">How to Grow a User Group, the Remix</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://8thlight.com/blog/feed/atom.xml">8th Light Blog</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">practices</span>
              <span class="tag">principles</span>
              <span class="tag">company</span>
            </div>
            <span class="subtitle is-7">Wed Sep 13 2017 06:00:00 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>Last week, I had the honor of <a href="/blog/ray-hightower/2017/09/07/chicagoruby-new-leadership.html">passing the ChicagoRuby leadership baton</a> to one of my 8th Light colleagues, Nicole Carpenter. Much can pile up in ten years of leadership, so she and I have spent the last few weeks managing handoff details, including introductions to key contact people, transferring social media accounts, finding videos, and answering questions.</p>

<h3>Why?</h3>

<p>The most important question that I can answer for Nicole and her team is <em>why?</em> Answering that question means sharing the thought process behind my past decisions for the group. Answering <em>why?</em> will help them to determine whether one of my old decisions makes sense in a future context vs a different decision by the new team. Old decisions don’t always hold up over time.</p>

<p>This is an updated version of the <a href="http://rayhightower.com/blog/2014/05/30/how-to-grow-a-user-group/">How to Grow a User Group</a> post from 2014. ChicagoRuby has evolved since then: We have 4,200 members (up from 2,700), new meeting locations, a larger team of organizers, and a deeper relationship with 8th Light. The growth continues. My hope: That the growth we've experienced since that original post will provide useful information to anyone building a community around their interests.</p>

<p>And now… here’s How to Grow a User Group, the Remix.</p>

<h3>Quick Summary</h3>

<p>ChicagoRuby is successful because our team does a few things really well. We practice consistency, teamwork, iteration, and learning from mistakes. We have maintained our focus since 2007. And we’ve made a few mistakes, revealed here.</p>

<h3>Be Consistent With Meetings</h3>

<p>Consistency is difficult in the beginning, especially on that night when only three people show up for a meeting. I have hosted a 3-person ChicagoRuby meeting before. Consistency is hard, and it is also the most important factor in user group success.</p>

<p>People trust consistency. Consistent meetings grow groups.</p>

<p>Members of ChicagoRuby know that our largest meeting happens on the first Tuesday of every month downtown. Our hack nights and workshops are posted on the <a href="https://www.meetup.com/ChicagoRuby/events/">ChicagoRuby calendar</a> months ahead of time. Members can plan their schedules far in advance because the ChicagoRuby calendar is consistent. Consistency builds trust.</p>

<p>ChicagoRuby cancelled a meeting in January 2011 due to a major snow storm in Chicago. We were forced to cancel because the property manager shut the entire building down. We’re a little bit stubborn about consistency. And that’s how we build trust.</p>

<p>Yes, there will be times when the organizer is too exhausted to run an upcoming meeting. That’s why it’s important to share the work by building a team.</p>

<h3>Build the Team</h3>

<p>As of this writing, ChicagoRuby has 14 organizers. We believe in sharing the work amongst multiple people. The group is stronger with several brains working in concert. Working as a team enables us to benefit from each other’s strengths. Some organizers have strong design skills, others are strong developers. And some are good at asking members for help.</p>

<h3>Ask for Help</h3>

<blockquote>
<p>I guarantee you that any favor he asks of you, you will offer to do before he requests it.<br /> <br />~Tom Hagen, Consigliere</p>
</blockquote>

<p>One way to ask for help: Encourage members to help in their area of enthusiasm. And if they’re really effective, ask them to join the organizer team. </p>

<p>For example, <a href="https://twitter.com/amazingspeciali">Ola Giwa</a> is a data scientist active at <a href="http://www.thebluelacuna.com">BLUE Lacuna</a> on Chicago’s South Side. Ola is passionate about sharing her data science and programming knowledge with others. So we asked her to make it happen as a member of the organizer team. Today, Ola leads ChicagoRuby's work at BLUE.</p>

<p>Members who have ideas for improvement tend to be strong leaders. The whole group benefits when we get out of their way and let them lead.</p>

<p>Take a look at the list of <a href="http://www.chicagoruby.org/organizers/">ChicagoRuby Organizers</a>, past and present. Many of our former organizers remain active in an emeritus role. Every mind helps to make the group stronger.</p>

<p>In addition to the core group of ChicagoRuby organizers, a few people deserve special mention. <a href="http://twitter.com/zolk">Kevin Zolkiewicz</a> managed every WindyCityRails from 2008 through 2016. This year, <a href="https://www.meetup.com/ChicagoRuby/members/214943981/">Breanna Reader</a> assumed Kevin’s management responsibilities while <a href="https://twitter.com/jmkozy">Jordan Koczot</a> helps with logistics. Kevin remains as an advisor. The team grows stronger when responsibilities are shared. Everybody wins.</p>

<h3>Automate Everything</h3>

<p>Managing a database of 4,200+ members could be drudgery. Fortunately, <a href="http://meetup.com/chicagoruby">Meetup.com</a> makes the process easy. Meetup handles RSVPs, membership additions and deletions, and reminders. Sometimes, the 2-week automated reminder from Meetup.com reminds the organizers to get a speaker for the next meeting!</p>

<p>Early in our history, someone suggested that ChicagoRuby should not use Meetup.com because it was written in PHP, and we’re a Ruby group. But we see things differently. ChicagoRuby is a Ruby group that uses the best tool for the job, regardless of language. Meetup.com has proven itself useful since 2007, and it’s still going strong.</p>

<h3>Choose a Short Name</h3>

<p>In order to grow, a group has to attract new members. People looking for a group to join are likely to start with a search engine. Search engines adore simplicity.</p>

<p>Our group was originally called The Chicago Area Ruby on Rails Meetup Group. Accurate, and a mouthful. We discovered that a simple domain name, <a href="http://chicagoruby.org">ChicagoRuby.org</a>, was available. So we grabbed it, along with the <a href="http://twitter.com/chicagoruby">@ChicagoRuby</a> Twitter handle. </p>

<p>We merged with Chirb, another Ruby group, in 2009. We considered switching to their shorter handle, but we stuck with ChicagoRuby because it’s the shortest name that communicates our purpose in two easy-to-remember words.</p>

<h3>Be Easy to Find</h3>

<p>To make the group easy to find, every web site in the ChicagoRuby ecosystem points to all of the group’s other web sites. For example, all of the conference sites point to the ChicagoRuby site, and vice versa. A new member who finds one part of the ecosystem will find the whole thing. People feel welcome when information is easy to find. And the <a href="http://rayhightower.com/blog/2017/08/24/highly-effective-seo/">search engines</a> love it, too.</p>

<p>Making future members feel welcome is key to growth. Free monthly events are another way to make people feel welcome.</p>

<h3>Keep Monthly Events Free</h3>

<p>I firmly believe that monthly user group meetings should be free. We never know what a member of our community is going through financially. Therefore, ChicagoRuby’s monthly meetings have always been free.</p>

<p>My personal history includes times when I was dead broke due to one entrepreneurial setback or another. So I have emotional reasons for keeping ChicagoRuby’s monthly events free of charge.</p>

<p>Of course, the money to pay for the meetings has to come from somewhere. That’s where sponsors can help.</p>

<h3>Cover Monthly Costs Through Sponsorships</h3>

<p>Companies will gladly sponsor a group that gives them a return on their investment. Sponsorships don’t always come in the form of money.</p>

<p>For example, ChicagoRuby’s first sponsor was <a href="http://www.deforestgroup.com/">DeForest Group</a>. Owners Lee DeForest and Jim DeForest provided ChicagoRuby with free meeting space and WiFi for our Saturday meetings in Elmhurst. Lee was one of the five people in the room when I became lead organizer in 2007. I will always be grateful for Lee’s early support.</p>

<p>8th Light has been a supporter and sponsor of ChicagoRuby and related conferences for over a decade. Members of 8th Light spoke at the first WindyCityRails in 2008. The company has deepened its commitment to ChicagoRuby over the past year, and our members are grateful.</p>

<p>Sponsors get to address the group at the beginning of each meeting. More important, by sponsoring ChicagoRuby over time, a company can build trust within the membership.</p>

<p>O’Reilly Media was our first publishing sponsor. O’Reilly, Pearson Education, and The Pragmatic Programmers provided books for ChicagoRuby members to review at Amazon, Slashdot, and other sites.</p>

<p>Sponsors get involved with user groups for two main reasons: To recruit developers, or to market products to developers. Everybody wins when sponsors get involved with user groups, financially or otherwise.</p>

<h3>Vet Speakers</h3>

<p>Members of ChicagoRuby trust the organizers to deliver quality events every month. I dropped the ball one month when I failed to properly vet a speaker. After that meeting, organizer <a href="https://twitter.com/dgiunta">Dave Giunta</a> wrote the first draft of the ChicagoRuby <a href="http://www.chicagoruby.org/about/speaker-guidelines/">speaker guidelines</a>. We’ve word-smithed the guidelines over the years, but the most important parts were written by Dave.</p>

<h3>Make Members Feel Awesome</h3>

<p>We humans have a need to belong to something. The feeling of belonging (awesomeness) can be strengthened when we go out for drinks after an event. Drinks are not necessarily alcohol; when we go to a bar together, some members will have a soda, juice, or coffee. Informal camaraderie makes members feel like welcome.</p>

<p>ChicagoRuby members gather together at a bar for an hour or so after our downtown meetings. Our suburban meetings end around noon on Saturdays, so we grab lunch together at a local restaurant. The conversation continues, and bonds are formed.</p>

<h3>Maintain Focus, and Explore New Areas</h3>

<p>Focus generally leads to excellence. But if our focus is too tight, we might miss the Next Great Thing. How does a group balance between focus and exploration?</p>

<p>There’s no easy answer. Like any tech organization, ChicagoRuby has struggled with focus. We’ve experimented with other ventures. We ran a job board for awhile. We ran conferences related to NoSQL and mobile.</p>

<p>In 2015, I attempted (yes, I was the driver behind this mistake) to launch a conference in Barbados. I envisioned an event with sun, beaches, coding, networking, and a tour of an island rum distillery. Unfortunately, ticket sales were dismal, so we cancelled the event.</p>

<p>The good news: While planning the Barbados event, we developed relationships with professors and developers at the <a href="https://www.cavehill.uwi.edu/">University of the West Indies at Cave Hill (UWI)</a>. Developers and engineers in and around UWI were doing cool work with open source, so we collaborated to launch <a href="http://linuxbarbados.org/">LinuxBarbados</a>, a monthly user group. So the Caribbean idea continues to yield some benefits, even today.</p>

<p>What did we learn? In hindsight, we should have launched the user group first, measured interest via monthly attendance, and then planned accordingly.</p>

<p>Conferences require more planning and resources than monthly meetups. Therefore, to maximize the odds of success, conferences should be built after user group experiments prove successful.</p>

<p>You might think of a user group as an MVP for a conference. WindyCityRails, now in its tenth year, is one example of this approach. WindyCityRails was built on the strength of ChicagoRuby.</p>

<h3>Collaborate With Other Groups</h3>

<p>Collaboration stretches our brains in unexpected and wonderful ways. One example of successful collaboration: LinuxBarbados, described above.</p>

<p>Another successful collaboration: ChicagoRuby joined forces with members of Chicago’s Python and Java communities to launch the <a href="http://chicagopolyglot.com">Chicago Polyglot Mingle</a> in 2016. The event was so successful that we repeated it in 2017, with plans to do it again in 2018. A community remains vibrant by borrowing good ideas from other communities.</p>

<h3>Conclusion</h3>

<blockquote>
<p>When smart people challenge each other to grow, great things happen!<br /> <br />~The ChicagoRuby Motto</p>
</blockquote>

<p>Every user group will grow differently depending on its particular strengths and interests. In my experience, groups that grow are those that practice consistency, teamwork, iteration, and learning from mistakes. The whole community benefits when that happens.</p>

<p>If you want to see this approach in action, we invite you to participate in <a href="http://chicagoruby.org">ChicagoRuby</a> and <a href="http://windycityrails.com">WindyCityRails</a>. Our members will make you feel welcome. And awesome!</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://prestonbyrne.com/2017/09/12/the-fca-finally-speaks/">The UK FCA (Finally) Speaks About ICOs</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://prestonbyrne.com/feed/">Preston Byrne</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">legal</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Tue Sep 12 2017 17:19:09 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            Huge disclaimer notice! In red! I’m an English lawyer, so I want to begin this blog post by saying that I don’t know you, I’m not your lawyer, this blog post is not legal advice, and since structuring ICOs involves navigating more minefields than a British battlecruiser in the Skagerrak during Battle of Jutland, you definitely need to get a qualified lawyer of your own before you do anything ICO-related. Period. If you have no idea what an ICO is, read this. This post is a general survey written by a lawyer for other lawyers with a view to opening...
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://250bpm.com/blog:101">Gift vs. Reputation in OSS</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="http://250bpm.com/feed/pages/pagename/start/category/blog/t/250bpm-blogs/h/http%3A%2F%2Fwww.250bpm.com%2Fblog">250bpm</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">architecture</span>
              <span class="tag">economics</span>
              <span class="tag">practices</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Tue Sep 12 2017 11:45:02 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <div> <p>Some time ago I've written a <a href="http://250bpm.com/blog:93">short note</a> about usage of term &quot;gift culture&quot; in &quot;Homesteading the Noosphere&quot; by Eric S. Raymond. Eric Raymond have responded <a href="http://esr.ibiblio.org/?p=7579">here</a>. Unfortunately, I haven't had time to properly respond until now. Today, finally, I've managed to write down some notes on the topic.</p> <p>But first, let me get some stuff out of the way.</p> <p>First, Raymond is correct about the terminology. Looking at Wikipedia article about gift culture it looks like the term is used for everything from Kula ring to alms giving and communism. To me it seems that having a term that broad makes it almost useless, but whatever. Just to be clear, in the remaining part of this post I am going to use the term &quot;gift culture&quot; strictly as if it referred to the reciprocal exchange systems of the nations of the Pacific rim.</p> <p>Second, when I said we should talk about &quot;reputation culture&quot; I've just used the term that seemed to describe the thing. I don't think it's used in anthropology. Anyway, what I meant was the kind of thing that must have existed in the primordial open source. Number of participants was low, likely below Dunbar number. Everyone knew everyone else. It was easy to keep track of reputation of every person involved. If you released a piece of software to public you were rewarded by increased reputation in the community. The thing I've missed and that Eric Raymond have helpfully pointed out is that the this kind of reputation seeking exists even in traditional societies. In real world, it's always a mix of both mechanisms.</p> <p>Third, he points out that copyleft licenses are not used to regulate in-group behaviour. Rather it's &quot;a lever on the behavior of outsiders coming into contact with the hacker culture.&quot; While true, it seems that most important thing here is not what it says literally but what it points to: There is an in-group and there are outsiders. There's a conflict between the two which has to be regulated. That raises more questions: In a development model so loose as open source, who's the insider and who's the alien? How can we distinguish between the two? What's the true nature of the conflict? And how is it being regulated? I'll try to address that in this post.</p> <p>If we start with the idea of primeval open source as a group of hackers driven solely by reputation it's clear that the model had to fail to scale at some point. With millions of contributors to open source, which is the case today, there's no way to keep track of individual reputations. Richard Stallman and Linus Torvalds may still have their individual reputation, but that's not the case for the nameless millions.</p> <p>One way out would be to adopt the free market system. We would abandon our naive hippie ways and do as responsible grown ups do. We would sell software licenses and acquire software patents. The market would take care of the rest. Except that markets are mechanism for allocation of scarce resources and software, unfortunately, is not a scarce resource. You can copy it ad libitum at no cost.</p> <p>Software industry has experimented with market for software for decades. They've tried to turn software into commodity. They've tried to introduce artificial scarcity. The result was a dumpster fire so bad that no decent person wants to participate in it any more. In fact, the very free software movement was born out of disgust with the market failures that we had to deal with.</p> <p>So no, markets were not the answer. And with both reputation culture ruled out by the size of the community and the market system ruled out by the non-scarce nature of software the only other option to pursue was the gift economy.</p> <p>Now, don't get me wrong. I am not saying that hackers have explored the anthropological literature, found out about potlatch and deliberately tried to implement it. What I am saying is that gift culture comes naturally. So naturally, in fact, that I wouldn't be at all surprised if it turned out that psychology that powers it is hard-wired into our brains. It's hard to find a society that doesn't exhibit at least some elements of gift exchange and even our own culture, which, out of all the cultures, has strayed furthest away from it is not completely devoid of sociological structures maintained and reinforced by reciprocated gifts.</p> <p>What I am really saying is that when the market mindset that is so laboriously instilled into us turned out to lead to a failure of epic proportions, the natural, almost instinctive, fallback was to the gift culture.</p> <p>As a big man from Papua gives away a dozen of pigs and couple of cassowaries and expects, in time, to get even more back, the programmers started to give the fruits of their labour away and, according to the logic of the gift economy, patiently waited for reciprocation.</p> <p>But the nature of software defies the gift system as much as it defies the market system. Instead of two scores of pigs and four cassowaries there came questions, complaints, bug reports and requests for features. Instead of reciprocated gifts, the programmer got even more work to do. Then there came patches which looked like true gifts until one realized that although the author of the patch invested their own time to write it they were basically dumping the code over the wall and they expected the maintainer of the project to take care of it for ever after, to fix bugs in it and, generally, to deal with any problems that the patch may have introduced. To the programmer, in the end, all the users seemed to have failed to reciprocate and, in traditional terms, to have &quot;lost their face&quot; and became &quot;rubbish men&quot;.</p> <p>The situation doesn't look much better from the user's point of view. They've invested their own work to adopt the software, which, instinctively, feels like a gift, and the software turns out to be buggy, to miss features and, generally, to not perform as advertised. So they craft a patch, that is, they give yet one more gift, and the patch gets rejected. That's a big no-no in traditional gift culture. Rejecting a gift is a big offence and can easily lead to violence. From the user's point of view, the one who has lost their face is definitely the programmer.</p> <p>In the end, we see two parties, both feeling like they were giving gifts and both feeling like the gifts were not reciprocated or even spurned. That doesn't sound like a recipe for a viable gift culture.</p> <p>***</p> <p>While the above is mostly uncontroversial and can be supported by dozens of blog posts written by frustrated open source community members, what follows is my speculation and may easily turn out to be completely wrong.</p> <p>I think that the solution to the problem described above is, in many a project, to retreat to the primeval reputation culture. It is accomplished by treating only those that are known to cooperate well as members of the community. Everyone else is treated as a potentially hostile alien. The aliens are benevolently allowed to use the software, or maybe they are shown their place by copyleft licenses, but they are not welcome to mess with the internal dealings of the tribe. The tribe is kept small so that everyone can track everyone else's reputation and can be effectively punished if they fail to reciprocate. To become a member of the tribe it's not sufficient to submit a patch. You may be, for example, asked to undergo a rite of passage such as keeping a straight face while being savaged by Linus. Or you may be asked to prove your commitment by reworking your patch over and over without being granted any guarantee that it will be ever accepted.</p> <p>As already said, I am not sure whether this &quot;feudalization of open source&quot; interpretation is correct but it seems to explain at least some phenomena seen in the wild.</p> <p><strong>Martin Sústrik, September 12th, 2017</strong></p> <div></div> </div> <p>by <a href="http://www.wikidot.com/user:info/martin-sustrik"></a><a href="http://www.wikidot.com/user:info/martin-sustrik">martin_sustrik</a></p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://bradfrost.com/blog/post/facebook-you-needy-sonofabitch/">Facebook, You Needy Sonofabitch</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="http://feeds.feedburner.com/brad-frosts-blog">Brad Frost</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">design</span>
              <span class="tag">web</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Tue Sep 12 2017 04:12:47 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>Several months ago, I turned off notifications from Facebook on my phone. Last week, I went ahead and removed the Facebook app from my phone.</p>
<p>Now, I genuinely enjoy Facebook. I use it for keeping up with with my family and my IRL friends, who are spread out all over the world. (The questions I ask when determining who to friend on Facebook: “Have they been in my house? Or would it feel natural/comfortable for them to visit my house?”)</p>
<p>But lately I’ve noticed the platform feeling increasingly grabby, to the point where they’ve broken the <a href="https://en.wikipedia.org/wiki/Fourth_wall">fourth wall</a> with me and now the whole experience is no longer enjoyable. They’ve gotten so brazen in their tactics to keep users engaged (ENGAGED!) I think <strong>it’s no longer possible to be a casual Facebook user.</strong></p>
<p>Here’s a few examples of what I’m talking about:</p>
You’ve shared x days in a row and your friends are responding.
<blockquote>
<p>Dear Jana,</p>
<p>You’ve managed to share posts two days in a row that weren’t completely lame.</p>
<p>Love, Facebook <a href="https://t.co/Uyzg83XOXi">pic.twitter.com/Uyzg83XOXi</a></p>
<p>— Jana Marie Johnson (@janamjohnson) <a href="https://twitter.com/janamjohnson/status/900372990881402880">August 23, 2017</a></p></blockquote>
<p></p>
<p>No doubt this notification is inspired by Snapchat’s <a href="https://support.snapchat.com/en-US/a/Snaps-snapstreak">snap streak</a> feature, which encourages people to keep messaging each other every day in order to keep the streak alive. But holy crap, this feels so incredibly unnatural to say out loud. It’s weird to see them so explicitly come out and say “you’re using the platform exactly how we want you to, and you have friends because of it. You want friends, right? You want to be loved, don’t you? The only way to be loved is to keep posting.”</p>
<p>Now, I can appreciate the fact that businesses want to have a solid understanding of how their audiences are responding to posts, but it seems strange and disturbing to talk to regular users like they’re all marketeers.</p>
Exploiting good intentions
<p>People enjoy wishing people happy birthday. People enjoy taking a stroll down memory lane once in a while. Facebook has masterfully taken those kind and sentimental aspects of the human condition and manipulated them for clicks.</p>
<p>For years I found myself on the hamster wheel of wishing everyone a forced happy birthday. For years! Of course I want the people in my life to have a happy birthday, but it shouldn’t feel like a tedious chore. It’s valuable to know the birthdays of your friends and family, but it’s lousy to use that as a hook to keep you coming back and playing the slots.</p>
<p></p>
<p>Same thing with memories. I occasionally enjoy looking back at experiences I have with my family and friends. And when this feature first rolled out I found myself exploring a few of my past posts. But too much of a good thing gets swept up in the rest of the noise, and I notice this feature now pops up on the regular. I can almost hear them saying “Oh hey, people seem to like this memories thing; let’s turn it up to 11!” It went from being an occasional treat to just another notification clogging up the pipes.</p>
Pay to play
<p>I have a page for my <a href="https://www.facebook.com/brad.frost.web.design/">business</a>, which is where I share links to web design resources. I can appreciate the fact that businesses paying for posts keeps the big blue ship afloat, and I can appreciate the fact that businesses would want to know if I particular post would be especially good to promote. But lately it seems they’ve really turned the screws trying to aggressively funnel you into paying to promote posts.</p>
<h3>This post is performing 95% better than others. Boost it!</h3>
<p></p>
<h3>You want to promote this post, don’t you?</h3>
<p></p>
<blockquote>
<p>Facebook is a bit pressure-y. “Haven’t heard from you in a while. Write a post.” “You said something witty. Pay us so more people see it.”</p>
<p>— jeremy haun (@jerhaun) <a href="https://twitter.com/jerhaun/status/902351972426276864">August 29, 2017</a></p></blockquote>
<p></p>
<p>There’s no respite from these messages, so it constantly feels like a gun to your head to get you to boost, promote, and pay.</p>
Miscellaneous Debris
<p>One thing that’s become obvious over the course of the last year is Facebook’s willingness to suggest more and more things that have nothing to do with my personal life experience. On one hand, I appreciate the sentiment of trying to expand someone’s horizons to open them up to new people, places, and experiences. But even if that’s the spirit of what Facebook is trying to accomplish, the execution feels like a shallow grab for clicks.</p>
<h3>So and so created a poll</h3>
<p></p>
<p>I’m not even a member of Assemble Volunteers.</p>
<h3>So and so just posted for the first time in a while.</h3>
<p></p>
<p>My cousin updated his status for the first time in a while. Good for him!</p>
<h3>So and so just joined Messenger! Be the first to send a welcome message or sticker.</h3>
<blockquote>
<p>“Your friend has just joined Messenger! Be the first to send a welcome message or sticker.”</p>
<p>Does anyone actually do this</p>
<p>— Old Salty Crab (@NoMagRyan) <a href="https://twitter.com/NoMagRyan/status/898142815489855488">August 17, 2017</a></p></blockquote>
<p></p>
<p>“Hey, we have products. Use our products.”</p>
<h3>So and so added an event near you</h3>
<blockquote>
<p>Getting sick of these <a href="https://twitter.com/facebook">@facebook</a> notifications about “this page added an event near you” <a href="https://twitter.com/hashtag/IDontCare?src=hash">#IDontCare</a></p>
<p>— Kevin Timm (@Kevin_Timm) <a href="https://twitter.com/Kevin_Timm/status/854505115486756865">April 19, 2017</a></p></blockquote>
<p></p>
<p>Surfacing events you might be interested in isn’t a bad idea, but execution is everything. For a platform that knows so much about me, I think it’s incredible how far off the mark most of their suggestions are.</p>
We haven’t heard from you in a while…
<p>This disturbs me perhaps more than anything.</p>
<p>I started the draft of this post a few days ago, and have since been taking care of work, going to a wedding, and living my life. But my several-day absence from Facebook apparently got them really worried. They started sending me a slew of emails over a period of time, highlighting recent posts from people, including my wife’s childhood friend’s husband.</p>
<p></p>
<p>Facebook got worried when I didn’t bite on any of those, so they decided to bring out the big guns.</p>
<p></p>
<p>Holy shit! My Facebook just blew up. So much has happened! I’ve apparently been poked 4 times! Despite my intention to not feed the beast and rather simply analyze their tactics for bringing me back in, being poked 4 times was just too irresistible not to check out. So I clicked through to find this:</p>
<p></p>
<p>Apparently pokes from 5 years ago are still newsworthy! Anything to get you to come back.</p>
What to make of all this
<p>This is what happens when the metric of how much time users spend using your thing supersedes the goal of providing legitimate value to your users. <strong>The tricks, hooks, and tactics Facebook uses to keep people coming back have gotten more aggressive and explicit. And I feel that takes away from the actual value the platform provides.</strong></p>
<p>There are of course plenty of weighty, important topics worth criticizing Facebook for, from their perpetuating fake news to their role in influencing the election to enabling the surveillance state and so on. But even this seemingly benign topic has huge ramifications on how people spend their time and live their lives. As users, it’s important to be aware of how the platform is manipulating you. As designers, it’s important to be mindful of how much attention we’re demanding from users and why we’re demanding that attention in the first place.</p>
<p>So that’s where I’m at. I’m likely not going to delete Facebook entirely since I do genuinely enjoy staying in touch with the people in my life, and for better or worse Facebook is where those people hang out. But I want to do use Facebook on my own terms, not theirs.</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://prestonbyrne.com/2017/09/11/blockchain-ico-claims-it-will-be-worth-1-trillion-by-2025/">Blockchain ICO claims it will be worth $1 trillion by 2025</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://prestonbyrne.com/feed/">Preston Byrne</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">legal</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Mon Sep 11 2017 21:53:23 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            You can’t make this stuff up. Another real estate #ICO warning, this time about @LATokens. pic.twitter.com/yBfNjt1EXE — Ragnar [No2x] ⚑ (@Ragnarly) September 11, 2017 Filed under: ICOs
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://bradfrost.com/blog/link/fbpurity/">FBPurity</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="http://feeds.feedburner.com/brad-frosts-blog">Brad Frost</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">design</span>
              <span class="tag">web</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Mon Sep 11 2017 16:56:27 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>I <a href="http://bradfrost.com/blog/post/facebook-you-needy-sonofabitch/">wrote about Facebook’s aggressive tactics</a>, and <a href="http://www.portigal.com/">Steve Portigal</a> responded with a recommendation for FBPurity.</p>
<blockquote><p>F.B. Purity is a browser extension / add-on that lets you clean up and customise Facebook. It filters out the junk you don’t want to see, leaving behind the stories and page elements you do wish to see.</p></blockquote>
<p>Sounds fantastic. Count me in.</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://prestonbyrne.com/2017/09/10/ico-thought-of-the-week/">ICO Thought of the Week</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://prestonbyrne.com/feed/">Preston Byrne</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">legal</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Mon Sep 11 2017 00:42:56 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            Presented without comment. &quot;everybody is a genius in a bull market&quot; – random redditor talking truth to self-fellating early crypto investors. pic.twitter.com/VBbI6MULDx — Liad Shababo (@L1AD) September 10, 2017 Filed under: ICOs
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://www.nngroup.com/articles/no-validate-in-ux/">Don’t “Validate” Designs; Test Them</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://www.nngroup.com/feed/rss/">Nielsen Norman Group</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">ux</span>
              <span class="tag">company</span>
            </div>
            <span class="subtitle is-7">Sun Sep 10 2017 17:00:00 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p><strong>Summary:</strong> The phrase “validate the design” discourages teams from finding and following up on UX issues in user testing. UX research must drive design change, not just pat designers on the back.</p><hr /><br /><p> All too often people use the word &quot;validate&quot; to justify their UX work. As in, &quot;Let's test the design to validate it,&quot; or “Let’s do an expert design review to validate this iteration.”</p><p> I strongly oppose using &quot;validate&quot; in this context. Call it a pet peeve. Call it semantics. Call me rigid. But let me tell you why I feel this way.</p><p> User research is as much a mindset as it is a science and an art. In a user study, attitude greatly affects participants, your team’s reactions to their behaviors, and follow-up actions on study findings. Planning the testing, observing users, analyzing findings, and describing the research process all require a delicate balance of curiosity, realism, diplomacy, honesty, and a profound ability to welcome and withstand criticism.</p><br /><br /><a href="/articles/no-validate-in-ux/">Read Full Article</a>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://www.nngroup.com/articles/audio-signifiers-voice-interaction/">Audio Signifiers for Voice Interaction</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://www.nngroup.com/feed/rss/">Nielsen Norman Group</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">ux</span>
              <span class="tag">company</span>
            </div>
            <span class="subtitle is-7">Sun Sep 10 2017 17:00:00 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p><strong>Summary:</strong> Provide explicit, implicit, and nonverbal signifiers to help users understand their options in voice-interaction interfaces. </p><hr /><br /><p> Good voice interfaces require not only excellent natural language comprehension, but also strategies for helping users understand the universe of actions and commands available in voice interactions — in other words, we need to bridge the Gulf of Execution. This interaction design challenge is present in all systems, but is inherently more difficult for voice interfaces.</p> The Gulf of Execution<p> To successfully interact with any system, people must be able to (1) figure out what <strong>  actions to take </strong> in order to achieve a specific goal, and (2) <strong>  understand the results </strong> of those actions. In his seminal book, <a href="https://www.nngroup.com/books/design-everyday-things-revised/">  The Design of Everyday Things </a> , our colleague Don Norman described these needs as the Gulf of Execution and the Gulf of Evaluation, respectively. Both are important, but in this article, we’ll focus on the Gulf of Execution, and how voice-interaction systems can help users understand what commands are possible.</p><p> In graphical user interfaces (GUIs), designers can help people bridge the Gulf of Execution by providing <a href="https://www.nngroup.com/articles/clickable-elements/">  visible signifiers </a> , like <a href="https://www.nngroup.com/articles/clickable-elements/">  distinctive colors for clickable text </a> . When used appropriately, these techniques enable users to understand at a glance what actions are possible; conversely, research shows that <a href="https://www.nngroup.com/articles/flat-ui-less-attention-cause-uncertainty/">  diminished graphical signifiers lead to slower task times </a> and cause click uncertainty in a GUI.</p><br /><br /><a href="/articles/audio-signifiers-voice-interaction/">Read Full Article</a>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://jvns.ca/blog/2017/09/10/vim-sessions/">Cool vim feature: sessions!</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://jvns.ca/atom.xml">Julia Evans</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">practices</span>
              <span class="tag">linux</span>
              <span class="tag">networking</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Sun Sep 10 2017 13:58:53 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>Yesterday I learned about an awesome vim feature while working on my
<a href="https://github.com/jvns/vimconfig/blob/master/vimrc">vimrc</a>! (to add fzf &amp; ripgrep search plugins
mainly). It’s a builtin feature, no fancy plugins needed.</p>

<p>So I drew a comic about it.</p>

<p>Basically you can save all your open files and current state with</p>

<pre><code>:mksession ~/.vim/sessions/foo.vim
</code></pre>

<p>and then later restore it with either <code>:source ~/.vim/sessions/foo.vim</code> or <code>vim -S ~/.vim/sessions/foo.vim</code>. Super cool!</p>

<p>Some vim plugins that add extra features to vim sessions:</p>

<ul>
<li><a href="https://github.com/tpope/vim-obsession">https://github.com/tpope/vim-obsession</a></li>
<li><a href="https://github.com/mhinz/vim-startify">https://github.com/mhinz/vim-startify</a></li>
<li><a href="https://github.com/xolox/vim-session">https://github.com/xolox/vim-session</a></li>
</ul>

<p>Here’s the comic:</p>

<div>

</div>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://jvns.ca/blog/2017/09/09/data-structure--the-treap-/">Data structure: the treap!</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://jvns.ca/atom.xml">Julia Evans</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">practices</span>
              <span class="tag">linux</span>
              <span class="tag">networking</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Sat Sep 09 2017 22:37:15 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>Here’s a quick sketch I did yesterday about a randomized data structure: the
<a href="https://en.wikipedia.org/wiki/Treap">treap</a>.  It’s basically a really cool way to implement a
balanced binary search tree. Kamal told me about it!</p>

<p>The main reason I know for sure this is useful is in case someone asks you to implement a balanced
binary search tree in an interview. More seriously though – if you want to implement an ordered map
(like C++’s <a href="http://en.cppreference.com/w/cpp/container/map">std::map</a>), then you probably want a
balanced BST!</p>

<p>C++’s std::map is usually a red-black tree, but a treap performs just as well (in expected value)
and the algorithms for insert/delete are way simpler.</p>

<div>



</div>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://www.fourmilab.ch/fourmilog/archives/2017-09/001710.html">Floating Point Benchmark: Scala Language Added</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://www.fourmilab.ch/fourmilog/atom_10.xml">Fourmilog - Fourmilab Change Log</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Sat Sep 09 2017 14:08:49 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            I have posted an update to my trigonometry-intense <a href="/fbench/" target="Fourmilog_Aux">floating point benchmark</a> which adds
<a href="https://en.wikipedia.org/wiki/Scala_(programming_language)" target="Fourmilog_Aux">Scala</a> to the list of languages in which the benchmark is implemented.  A new release of the
<a href="/fbench/" target="Fourmilog_Aux">benchmark collection</a>
including Scala is now available for downloading.

<p>

Scala is a general purpose programming language originally
developed at the École Polytechnique Fédérale in Lausanne,
Switzerland.  Scala combines the paradigm of functional
programming with support for conventional object-oriented
imperative programming, allowing the programmer to choose
whichever style is most expressive of the algorithm being
implemented.  Unlike <a href="https://en.wikipedia.org/wiki/Haskell_(programming_language)" target="Fourmilog_Aux">Haskell</a>, which forces the programmer into a
strict functional style, Scala contains control structures for
iteration, mutable variables, and a syntax which C and Java
programmers will find familiar.  Scala runs on the <a href="https://en.wikipedia.org/wiki/Java_virtual_machine" target="Fourmilog_Aux">Java virtual
machine</a>, and Scala and Java code can interoperate, which
provides Scala access to all existing Java libraries.

</p><p>

The Scala version of the benchmark was developed and tested using
Scala 2.12.3 on an x86_64 machine running Xubuntu 16.04 kernel
4.4.0-93.  In order to compile and run this program you must
<a href="http://www.scala-lang.org/download/" target="Fourmilog_Aux">install Scala on your computer</a>.

</p><p>

Scala programs compile to byte code which is executed by an
implementation of the Java virtual machine.  I ran these tests
using:

</p><p>
<blockquote>
    openjdk version &quot;9-internal&quot;<br />
    OpenJDK Runtime Environment (build 9-internal+0-2016-04-14-195246.buildd.src)<br />
    OpenJDK 64-Bit Server VM (build 9-internal+0-2016-04-14-195246.buildd.src, mixed mode)
</blockquote>
</p><p>

The Scala implementation of the floating point benchmark is
written mostly in a pure functional style, but I did use mutable
variables and iteration where it made the code more readable. 
Scala does not optimise tail recursion as aggressively as
Haskell, so iteration may be more efficient in heavily-used
code.

</p><p>

At the time I developed this benchmark, a recent release of the
GNU C mathematical function library had been issued which halved
the mean execution speed of trigonometric functions.  This made
all comparisons of run time against the C reference
implementation of the benchmark using the earlier, more
efficient libraries, invalid. (It's said that the performance
hit was done in the interest of improved accuracy, but it made
no difference in the computations of the floating point
benchmark, which are checked to 13 significant digits.)
Consequently, I compared the execution speed of the Scala
implementation against that of the Java version, then computed
the speed relative to the original C version with the old
libraries by multiplying the relative speed of Java vs. C and
Scala vs. Java.

</p><p>

The relative performance of the various language implementations (with C taken as 1) is as follows.   All language implementations of the benchmark listed below produced identical results to the last (11th) decimal place.

</p><p>


<table>
                                                                                                 
<tr>
    <th>Language</th>
    <th>Relative<br /> Time</th>
    <th>Details</th>
</tr>
 
<tr>
    <th>C</th>
    <td>1</td>
    <td>GCC 3.2.3 -O3, Linux</td>
</tr>
 
<tr>
    <th>Visual Basic .NET</th>
    <td>0.866</td>
    <td>All optimisations, Windows XP</td>
</tr>
 
<tr>
    <th>FORTRAN</th>
    <td>1.008</td>
    <td>GNU Fortran (g77) 3.2.3 -O3, Linux</td>
</tr>
 
<tr>
    <th>Pascal</th>
    <td>1.027<br />
                   1.077</td>
    <td>Free Pascal 2.2.0 -O3, Linux<br />
    	    	  GNU Pascal 2.1 (GCC 2.95.2) -O3, Linux</td>
</tr>
 
<tr>
    <th>Swift</th>
    <td>1.054</td>
    <td>Swift 3.0.1, -O, Linux</td>
</tr>

<tr>
    <th>Rust</th>
    <td>1.077</td>
    <td>Rust 0.13.0, --release, Linux</td>
</tr>

<tr>
    <th>Java</th>
    <td>1.121</td>
    <td>Sun JDK 1.5.0_04-b05, Linux</td>
</tr>

<tr>
    <th>Visual Basic 6</th>
    <td>1.132</td>
    <td>All optimisations, Windows XP</td>
</tr>
  
<tr>
    <th>Haskell</th>
    <td>1.223</td>
    <td>GHC 7.4.1-O2 -funbox-strict-fields, Linux</td>
</tr>
  
<tr>
    <th>Scala</th>
    <td>1.263</td>
    <td>Scala 2.12.3, OpenJDK 9, Linux</td>
</tr>

<tr>
    <th>Ada</th>
    <td>1.401</td>
    <td>GNAT/GCC 3.4.4 -O3, Linux</td>
</tr>

<tr>
    <th>Go</th>
    <td>1.481</td>
    <td>Go version go1.1.1 linux/amd64, Linux</td>
</tr>

 <tr>
    <th>Simula</th>
    <td>2.099</td>
    <td>GNU Cim 5.1, GCC 4.8.1 -O2, Linux</td>
</tr>

<tr>
    <th>Lua</th>
    <td>2.515<br />
        22.7</td>
    <td>LuaJIT 2.0.3, Linux<br />
        Lua 5.2.3, Linux</td>
</tr>

<tr>
    <th>Python</th>
    <td>2.633<br /> 30.0</td>
    <td>PyPy 2.2.1 (Python 2.7.3), Linux<br />
        Python 2.7.6, Linux
</td>
</tr>

 <tr>
    <th>Erlang</th>
    <td>3.663<br />
    	    	   9.335</td>
    <td>Erlang/OTP 17, emulator 6.0, HiPE [native, {hipe, [o3]}]<br />
    	         Byte code (BEAM), Linux</td>
</tr>
 <tr>
    <th>ALGOL 60</th>
    <td>3.951</td>
    <td>MARST 2.7, GCC 4.8.1 -O3, Linux</td>
</tr>

<tr>
    <th>Lisp</th>
    <td>7.41 <br />
                   19.8</td>
    <td>GNU Common Lisp 2.6.7, Compiled, Linux<br />
                  GNU Common Lisp 2.6.7, Interpreted</td>
</tr>

 <tr>
    <th>Smalltalk</th>
    <td>7.59</td>
    <td>GNU Smalltalk 2.3.5, Linux</td>
</tr>
   
<tr>
    <th>Forth</th>
    <td>9.92</td>
    <td>Gforth 0.7.0, Linux</td>
</tr>

<tr>
    <th>COBOL</th>
    <td>12.5<br />
				   46.3</td>
    <td>Micro Focus Visual COBOL 2010, Windows 7<br />
				  Fixed decimal instead of computational-2</td>
</tr>

 <tr>
    <th>Algol 68</th>
    <td>15.2</td>
    <td>Algol 68 Genie 2.4.1 -O3, Linux</td>
</tr>

<tr>
    <th>Perl</th>
    <td>23.6</td>
    <td>Perl v5.8.0, Linux</td>
</tr>

<tr>
    <th>Ruby</th>
    <td>26.1</td>
    <td>Ruby 1.8.3, Linux</td>
</tr>

<tr>
    <th>JavaScript</th>
    <td>27.6 <br />
                   39.1 <br />
                   46.9</td>
    <td>Opera 8.0, Linux<br />
                  Internet Explorer 6.0.2900, Windows XP<br />
                  Mozilla Firefox 1.0.6, Linux</td>
</tr>
 
<tr>
    <th>QBasic</th>
    <td>148.3</td>
    <td>MS-DOS QBasic 1.1, Windows XP Console</td>
</tr>

<tr>
    <th>Mathematica</th>
    <td>391.6</td>
    <td>Mathematica 10.3.1.0, Raspberry Pi 3, Raspbian</td>
</tr>

</table>
</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://codewithoutrules.com/2017/09/09/learn-a-new-programming-language/">The better way to learn a new programming language</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://codewithoutrules.com/atom.xml">Code Without Rules</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">practices</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Sat Sep 09 2017 05:00:00 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>Have you ever failed to learn a new programming language in your spare time?
You pick a small project to implement, get a few functions written… and then you run out of time and motivation.
So you give up, at least until the next time you give it a try.</p>

<p>There’s a better way to learn new programming languages, a method that I’ve applied multiple times.
Where starting a side project often ends in failure and little knowledge gained, this method starts with success.
For example, the last time I did this was with Ruby: I <em>started</em> by publishing a whole new Ruby Gem, and getting a bug fix accepted into the <a href="http://www.sinatrarb.com/">Sinatra framework</a>.</p>



<p>In this post I will:</p>

<ul>
<li>Explain why a side project is a difficult way to learn a new language.</li>
<li>Share my story of learning a tiny bit of Ruby.</li>
<li>Explain my preferred learning method in detail.</li>
</ul>

Side projects: the hard way to learn a language

<p>Creating a new software project from scratch in your spare time is a tempting way to learn a new language.
You get to build something new: building stuff is fun.
You get to pick your language: you have the freedom to choose.</p>

<p>Unfortunately, learning via a side project is a difficult way to learn a new language.
When you’re learning a new programming language you need to learn:</p>

<ul>
<li>The build and run toolchain.</li>
<li>The packaging toolchain.</li>
<li>The testing toolchain.</li>
<li>How to use 3rd party packages.</li>
<li>The syntax.</li>
<li>The semantics: memory management, units of abstraction, concurrency, execution model, and so forth.</li>
<li>Standard idioms.</li>
<li>The standard library.</li>
<li>Common 3rd party libraries relevant to your problem domain: which to use, and their APIs.</li>
</ul>

<p>This is a huge amount of knowledge, and you’re doing so with multiple handicaps:</p>

<p><strong>Learning on your own:</strong> You have to figure out everything on your own.</p>

<p><strong>Blank slate:</strong> You’re starting from scratch.
Quite often there’s no scaffolding to help you, no good starting point to get you going.</p>

<p><strong>Simultaneous learning:</strong> You are trying to learn everything in the list above at the same time.</p>

<p><strong>Limited time:</strong> You’re doing this in your spare time, so you may have only limited amounts of spare time to apply to the task.</p>

<p><strong>Lack of motivation:</strong> If you care about the side project’s success, you probably will be motivated to switch back to a language you know.
If you just care about learning the language, you’ll be less motivated to do all the boring work to make the side project succeed.</p>

<p><strong>Vague goals:</strong> “Learning a language” is an open-ended task, since there’s always more to learn.
How will you know you’ve achieved something?</p>

<p>Personally I have very limited free time: I can’t start a new side project in a language I already know, let alone a new one.
But I do occasionally learn a new language.</p>

That time I learned some Ruby

<p>Rather than learning new languages at home, I use a better method: learning a language by solving problems at my job.</p>

<p>For example, I know very little Ruby, and when I started learning it I knew even less.
One day, however, I joined a company that was publishing a SDK in multiple languages, one of which was Ruby.</p>

<h3>A tiny gem</h3>

<p>My first task involving Ruby was integrating the SDK with popular Ruby HTTP clients and servers.
Which is to say, I started learning a new language with a <em>specific goal</em>, <em>motivation</em>, and <em>time to learn at work</em>.
Much better than a personal side project!</p>

<p>I started by learning <em>one</em> thing, not multiple things simultaneously: which 3rd party HTTP libraries were popular.
Once I’d found the popular HTTP clients and servers, my next task was implementing the SDK integration.
One integration was with Sinatra, a popular Ruby HTTP server framework.</p>

<p>As a coding task this was pretty simple:</p>

<ol>
<li>The Sinatra docs pointed me towards a library called Rack, a standard way to write HTTP server middleware for Ruby.</li>
<li>Rack has documentation and tutorials on how to create middleware.</li>
<li>There are lots of pre-existing middleware packages I could use as examples, for both the code itself and for tests.</li>
<li>I only needed to learn just enough Ruby syntax and semantics to write the middleware.
Googling tutorials was enough for that.</li>
</ol>

<p>I learned <em>just enough</em> to implement the middleware: 40 lines of trivial code.</p>

<p>Next I needed to package the middleware as a gem, Ruby’s packaging format.
Once again, I was only working on a single task, a well-documented task with many examples.
And I had motivation, specific goals, examples to build off of, and the time to do it.</p>

<p>At this point I’d learned: a tiny bit of syntax and semantics, some 3rd party libraries, packaging, and a little bit of the toolchain.</p>

<h3>A bugfix to an existing project</h3>

<p>Shortly after creating our SDK integration I discovered a bug in Sinatra: Sinatra middleware was only initialized after the first request.
So I tracked down the bug in Sinatra… which gave me an opportunity to learn more of the language’s syntax, semantics, and idioms by reading a real-world existing code base.
And, of course, the all-important skill of knowing how to add debug print statements to the code.</p>

<p>Reading code is a lot easier than writing code.
And since Sinatra was a pre-existing code base, I could rely on pre-existing tests as examples when I wrote a test for my patch.
I didn’t need to figure out how to structure a large project, or every edge case of the syntax that wasn’t relevant to the bug.
I had a specific goal, and I learned just enough to reach it.</p>

<p>At the end of the process above I still couldn’t start a Ruby project from scratch, or write more than tiny amounts of Ruby.
And I haven’t done much with it since.
But I do know enough to deal with packaging, and if I ever started writing Ruby again I’d start with a lot more knowledge of the toolchain, to the point where I’d be able to focus purely on syntax and semantics.</p>

<p>But I’ve used a similar method to learn other languages to a much greater extent: I learned C++ by joining a company that used it, and I became a pretty fluent C++ programmer for a while.</p>

Learning a new language: a better method

<p>How should you learn a new programming language?
As in my story above, the best way to do so is at work, and ideally by joining an existing project.</p>

<h3>Existing projects</h3>

<p>The easiest way to learn a new language is to join an existing project or company that uses a language you don’t know.
None of the problems you would have with a side project apply:</p>

<ul>
<li>There’s lot of existing examples to learn from and modify, you’re not starting with blank slate.</li>
<li>You don’t have to learn the build/run, testing, and packaging toolchains before you can do anything useful: it’s mostly going to be setup for you, so you can learn it by osmosis over time.</li>
<li>You have specific goals: fix this bug, add this feature.</li>
<li>You have co-workers you can ask for help, who can review your code, and can help you write more idiomatically.</li>
</ul>

<h3>New projects</h3>

<p>Lacking an existing project to join, look out for opportunities where there’s a strong motivation for your project to add a new language.
Some examples:</p>

<ul>
<li>You need to do some data science, so you need to add Python or maybe R.</li>
<li>Your project has a problem with a computationally intensive bottleneck, so you need to add something like Rust, C++, or C.</li>
</ul>

<p>Starting a new project is not quite as easy a learning experience, unfortunately.
But you’re still starting with specific goals in mind, and with time at work to learn the language.</p>

<p>Make sure to limit yourself to only learning one thing at a time.
In my example above I sequentially learned about: which 3rd party libraries existed, the API for one library, writing miniscule amounts of trivial integration code, packaging, and then how to read a lot more syntax and semantics.
If you’re doing this with co-workers you can split up tasks: you do the packaging while your co-worker builds the first prototype, and then you can teach each other what you’ve learned.</p>

Learning at work is the best learning

<p>More broadly, your job is a wonderful place to learn.
Every task you do at work involves skills, skills you can practice and improve.
You can get better at debugging, or notice a repetitive task and automate it, or learn how to write better bug reports.
Perhaps you could figure out what needs changing so you can get make changes done faster (processes? architecture? APIs?).
Maybe you can figure out how to test your code better to reduce the number of bugs you ship.
And if that’s not enough, <a href="https://jvns.ca/blog/2017/08/06/learning-at-work/">Julia Evans has even more ideas</a>.</p>

<p>In all these cases you’ll have motivation, specific goals, time, and often an existing code base to build off of.
And best of all, you’ll be able to learn while you’re getting paid.</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://feedproxy.google.com/~r/TroyHunt/~3/ud1uDEIXliw/">Weekly update 51 (Melbourne edition)</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://feeds.feedburner.com/TroyHunt">Troy Hunt</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">security</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Fri Sep 08 2017 08:37:41 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p><a href="http://www.symantec.com/ready-secure-go/it-smb/?om_ext_cid=ws_ad_TROY_DBC-Bulldog_IT-SMB_Encrypt"><strong>Presently sponsored by:</strong> Get a security solution that will keep your website up and running—and keep you sleeping soundly: Symantec Website Security. Learn how</a></p><div><p>Really quick intro as I rush between events today: I'm in Melbourne and have just finished a &quot;Hack Yourself First&quot; workshop then it's OWASP conference time tomorrow. It's been a <em>mostly</em> fun week with a couple of oddball experiences thrown in, but leave you to watch the video or listen to the podcast to enjoy those :)</p>
<p><a href="https://itunes.apple.com/au/podcast/troy-hunts-weekly-update-podcast/id1176454699">iTunes podcast</a> | <a href="https://goo.gl/app/playmusic?ibi=com.google.PlayMusic&amp;isi=691797987&amp;ius=googleplaymusic&amp;link=https://play.google.com/music/m/If3tw7npymckucxq4q76762ncny?t%3DTroy_Hunt%27s_Weekly_Update_Podcast">Google Play Music podcast</a> | <a href="http://www.omnycontent.com/d/playlist/1439345f-6152-486d-a9c2-a6bf0067f2b7/3ba9af7f-3bfb-48fd-aae7-a6bf00689c10/fde26e49-9fb8-457d-8f16-a6bf00696676/podcast.rss">RSS podcast</a></p>

References
<ol>
<li><a href="http://cynosureprime.blogspot.com.au/2017/08/320-million-hashes-exposed.html">CynoSure Prime did some cool work cracking the 320m Pwned Passwords</a> (somehow, some press outlets misinterpretted the significance of this)</li>
<li><a href="https://twitter.com/j_aksim/status/905742429294342144">I copped a pretty unhinged rant from the reincarnated SEO lady</a> (I'm starting to wonder if she's actually serious...)</li>
<li><a href="https://www.troyhunt.com/how-i-finally-fixed-my-parents-dodgy-wifi-with-amplifi/">I rolled out AmpliFi at my parent's house</a> (this is actually really neat as a consumer-grade solution)</li>
<li><a href="https://www.goldsecurity.com/">Gold Security is sponsoring my blog this week</a> (big thanks to those guys for being a repeat sponsor!)</li>
</ol>
</div>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://ronjeffries.com/articles/017-08ff/how-i-tdd/">How I work (mostly) TDD-style</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="http://ronjeffries.com/feed.xml">Ron Jeffries</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">practices</span>
              <span class="tag">agile</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Fri Sep 08 2017 05:00:00 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            Responding to a mistaken TDD flowchart.
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://technology.condenast.com/story/vogue-on-google-home">Ok Google, Talk to Vogue</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://technology.condenast.com/feed/rss">Condé Nast Technology</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">web</span>
              <span class="tag">company</span>
            </div>
            <span class="subtitle is-7">Thu Sep 07 2017 16:39:50 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            Exclusive audio from the Vogue September available only on Google Home
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://belkadan.com/blog/2017/09/The-New-Kingdom-of-Nouns/">The New Kingdom of Nouns</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="http://belkadan.com/blog/atom">-dealloc</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">practices</span>
              <span class="tag">tools</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Thu Sep 07 2017 08:00:00 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p><a href="http://belkadan.com/blog/2017/09/Over-abstraction/">Last time</a> I talked about how algebraic abstractions like “monoid” and “semigroup” didn’t seem to be pulling their weight, despite love from functional programmers. That focused on a practical issue: do these abstractions aid or harm comprehension? Do they make programming easier or harder? (Of course, there’s not a simple answer to this question.)</p>

<p>This time, though, I want to talk about something more exploratory: do we have the right tools to <em>talk</em> about these abstractions? This post is therefore going to be <em>much</em> longer and contain a lot more rambling.</p>

<p>(I’ll note up front that these really are exploratory ideas, not something I’d intend to put into Swift directly. Things to consider for the <em>next</em> language, perhaps, since <a href="https://www.xkcd.com/568/">we’ll never find a programming language that frees us from the burden of clarifying ideas</a>.)</p>

<hr />

<p>Let’s start with something “simple”: the abstraction of a <em>semigroup.</em> This consists of a type and an operation, and the operation has to be <a href="https://simple.wikipedia.org/wiki/Associativity">associative</a>. If we were to write this as a Swift protocol, we could do something like this:</p>

<div><pre><code>protocol Semigroup {
  // Binary semigroup operation
  // **AXIOM** Should be associative:
  //   a.op(b.op(c)) == (a.op(b)).op(c)
  func op(_ g: Self) -&gt; Self
}
</code></pre></div>

<p>(from Brandon Williams’ “<a href="http://www.fewbutripe.com/swift/math/algebra/2015/02/17/algebraic-structure-and-protocols.html">Algebraic Structure and Protocols</a>”)</p>

<p>Now, <code>Int</code> can certainly conform to this protocol:</p>

<div><pre><code>extension Int: Semigroup {
  func op(_ b: Int) -&gt; Int {
    return self + b
  }
}
</code></pre></div>

<p>This works great, because integer addition is associative.<a href="#fn:float">1</a> And as Brandon Kase mentions in his talk “<a href="http://2017.funswiftconf.com">Beyond Types in Swift</a>”, this means it’s safe to use with a parallel implementation of <code>reduce</code>. This means I can get the sum of a collection of integers much faster than if I do a regular, serial <code>reduce</code>.</p>

<p>But wait, we could also have written it this way:</p>

<div><pre><code>extension Int: Semigroup {
  func op(_ b: Int) -&gt; Int {
    return self * b
  }
}
</code></pre></div>

<p>Integer multiplication is also associative, and getting the product of a collection of integers can also be a useful operation.</p>

<p><strong>It has always, <em>always</em> bothered me that there are two ways for <code>Int</code> to be a semigroup.</strong> Why do people act like <code>+</code> is obviously the “right” way to do it? And it’s even worse for <code>Bool</code>, where <code>||</code> and <code>&amp;&amp;</code> are much closer to being symmetric.</p>

<p>Haskell, as a language with a standard library and community that cares strongly about algebraic abstractions, <a href="https://hackage.haskell.org/package/semigroups-0.18.1/docs/Data-Semigroup.html">actually agrees with me here.</a> Rather than make <code>Int</code> conform to <code>Semigroup</code>, they use simple wrapper structs (in Swift terms) called <code>Sum</code> and <code>Product</code>. In Swift, this would look something like this:</p>

<div><pre><code>struct Sum&lt;Wrapped: Arithmetic&gt;: RawRepresentable {
  var rawValue: Wrapped
  init(rawValue: Wrapped) { … }
  init(_ rawValue: Wrapped) { … } // for convenience
}
extension Sum: Semigroup {
  func op(_ other: Sum&lt;Wrapped&gt;) -&gt; Sum&lt;Wrapped&gt; {
    return Sum(self.rawValue + other.rawValue)
  }
}
</code></pre></div>

<p>(and similar for <code>Product</code>)</p>

<p>Pause.</p>

<hr />

<p>Back in 2006, Steve Yegge wrote a blog post called “<a href="http://steve-yegge.blogspot.com/2006/03/execution-in-kingdom-of-nouns.html">Execution in the Kingdom of Nouns</a>”. I hesitate a little to link to it because it probably qualifies as an instance of <a href="http://belkadan.com/blog/2015/12/Re-Contempt-Culture/">Contempt Culture</a>, but it does have some good points, and it relates to what I’m about to bring up, and without it I’d have to think of a different title for this blog post. But I’ll summarize it here for those who don’t want to read through Yegge’s parable about Java:</p>

<blockquote>
  <p>Object-oriented languages, especially 2000s-era OO languages, especially 2000s-era Java, have a (natural) tendency to “make everything a noun”, i.e. to not have free functions or function values. Instead, you have to make a class that has a particular operation, and then make that a subclass of some known class (or implement an interface, Java’s equivalent of Swift’s protocols) so that the operation you want can be invoked via dynamic dispatch. This adds boilerplate to your code and obscures the fact that you <em>really</em> just want to talk about an operation.</p>

  <pre><code>threadPool.execute(new Runnable() {
  @Override public void run() { … }
})
</code></pre>
</blockquote>

<p>(As an addendum, Java 8 got some shorthand to handle these much more concisely, which they call <a href="http://www.drdobbs.com/jvm/lambda-expressions-in-java-8/240166764">lambda expressions</a>. The operation is still strongly typed on the declaration side, though, so people could reasonably still be unhappy.)</p>

<p>So, the problem in Java is (was) that <strong>if you want to talk about an operation</strong> (a verby thing like “add”), <strong>you have to do it using a type</strong> (a nouny thing like “BinaryOperation”, or its concrete implementation “Sum”). Swift is much better because it allows you to directly use functions as values, either by referencing them by name or by using closure expressions. Problem solved, right?</p>

<hr />

<p>Last time I mentioned at the end of the post—snuck in, really—Dim Sum Thinking’s idea that <strong><a href="https://twitter.com/dimsumthinking/status/887498387293167618">algebraic abstractions act like “design patterns”</a></strong>, in that the power came from the intuition and understanding that went with saying “this behaves like a semigroup” in the same way as “this behaves like a delegate”. Someone who’s been programming Cocoa for a while almost certainly knows what I mean when I say the latter (“weakly-referenced object that is consulted and/or informed about various things that happen to the main object”), and someone who’s familiar with algebraic abstractions knows what I mean about the former (“type with a combining binary operation that’s associative, so I can parallel-reduce it”). This is <em>useful</em> for communication.</p>

<p>Only…there’s an idea (both longstanding and controversial) that <a href="http://wiki.c2.com/?AreDesignPatternsMissingLanguageFeatures">a design pattern compensates for a missing language feature</a>. The compiler can check (to some extent) if you’re using a language feature directly, and may provide special affordances to make it simpler and easier to get right. So, in that case, what language feature is missing?</p>

<hr />

<p>At this point you might be wondering “what does this all have to do with <code>Sum</code> and <code>Product</code>?” Well, recall that I said that a useful thing about associative operations is that you can perform <code>reduce</code> in parallel. That means we’d define <code>parallelReduce</code> something like this:</p>

<div><pre><code>extension RandomAccessCollection where Element: Semigroup {
  func parallelReduce(_ resultIfEmpty: Element) -&gt; Element { … }
}
</code></pre></div>

<p>Notice that the extension is constrained: the element of the collection must be a <code>Semigroup</code>. This is a good thing; <strong>we want the compiler to tell us if we try to <code>parallelReduce</code> with a non-associative operation</strong>, like subtraction. <strong>A design pattern can’t do this</strong>; it’s “just” guidelines on how to write code. But using this protocol correctly with the rest of Swift becomes a bit unwieldy.</p>

<div><pre><code>let questionScores: [Int] = …
// let totalScore: Int = questionScores.reduce(0, +)
let totalScore: Int =
    questionScores.lazy
                  .map { Sum($0) }
                  .parallelReduce(Sum(0))
                  .rawValue
</code></pre></div>

<p>That is, we have to jump in and out of the <code>Sum</code> type in order to talk about the summing operation.<a href="#fn:lazy">2</a></p>

<p>We could get clever and throw more code at the problem:</p>

<div><pre><code>extension RandomAccessCollection {
  func parallelReduce&lt;Operation: Semigroup&gt;(
    _ resultIfEmpty: Element,
    _ operation: Operation.Type) -&gt; Element
  where Operation: RawRepresentable, Operation.RawValue == Element {
    return self.lazy
               .map { Operation(rawValue: $0) }
               .parallelReduce(Operation(rawValue: resultIfEmpty))
               .rawValue
  }
}

let questionScores: [Int] = …
// let totalScore: Int = questionScores.reduce(0, +)
let totalScore: Int = questionScores.parallelReduce(0, Sum.self)
</code></pre></div>

<p>But now, even though we’ve gotten down to something much closer to the original <code>reduce</code> code, we’re starting to see a familiar complaint. <strong>In order to get the guarantee that <code>+</code> is associative, we had to <em>define a type.</em></strong> We’re back in the Kingdom of Nouns.</p>

<hr />

<p>I <em>think</em> the trouble is that we don’t <em>really</em> want to say “<code>Int</code> is a semigroup”, or even a more correct “<code>Int</code> forms a semigroup under addition” (or “with <code>+</code>”, if you want to sound less mathy). The natural way for <em>me</em> to talk about these things is “<code>+</code> on <code>Int</code>s is associative”. And indeed, that’s what I <em>have</em> been saying in the prose, throughout this whole post.</p>

<p>So, is that the missing language feature? Maybe we really want a way to describe the properties of <em>operations,</em> specific values of function type.</p>

<div><pre><code>// For illustrative purposes only.
operationgroup Semigroup&lt;Element&gt;: (Element, Element) -&gt; Element

func +(left: Int, right: Int) -&gt; Int: Semigroup { … }
func *(left: Int, right: Int) -&gt; Int: Semigroup { … }
func -(left: Int, right: Int) -&gt; Int { … }

extension RandomAccessCollection {
  func parallelReduce(
    _ resultIfEmpty: Element,
    _ combine: Semigroup&lt;Element&gt;) -&gt; Element { … }
}

let questionScores: [Int] = …
// let totalScore: Int = questionScores.reduce(0, +)
let totalScore: Int = questionScores.parallelReduce(0, +) // ahhh...
</code></pre></div>

<p>There we go. Now we can continue referring to functions directly, but also the compiler can check if we screw up at the use site, and even do some basic checking to make sure the function’s type makes sense. I’m not exactly sure how it would works with closure literals, but let’s not worry about that right now.</p>

<p>(If you’re wondering why there isn’t a swift-evolution proposal about this right now, keep reading.)</p>

<p>This feels a lot better to me than what we had before. We don’t have to say a <em>data type</em> has these particular properties. We don’t have to make a dummy type when there’s more than one way to satisfy the abstraction. We don’t even end up with the “generic operation” problem from the <a href="http://belkadan.com/blog/2017/09/Over-abstraction/">previous post</a>, because we use whatever names are natural for the operation.</p>

<p>(Okay, that last one is cheating, because now we have to pass the operation directly to <code>parallelReduce</code>, instead of inferring it from the element type. For <code>Semigroup</code> I think that’s the right tradeoff, but that might not always be the case. Keep reading.)</p>

<p>To summarize: <strong>what this hypothetical syntax does is put the conformance into the <em>function</em> type rather than the <em>data</em> type.</strong> It’s still “type checking”, and it’s still enforced by the compiler, so it’s more than just a design pattern, but at the same time it puts the abstraction in the right place.</p>

<p>…or does it?</p>

<hr />

<p>After defining <code>Semigroup</code>—actually, the very first protocol covered—<a href="http://www.fewbutripe.com/swift/math/algebra/2015/02/17/algebraic-structure-and-protocols.html">Williams goes on to define <code>Monoid</code></a>.</p>

<div><pre><code>protocol Monoid: Semigroup {
  // Identity value of monoid
  // **AXIOM** Should satisfy:
  //   Self.e().op(a) == a.op(Self.e()) == a
  // for all values a
  static func e() -&gt; Self
}
</code></pre></div>

<p>(I probably would have used a static property <code>identity</code> rather than a function, but whatever.)</p>

<p>The immediate advantage of a monoid is that you now have a good default for the <code>initialResult</code> parameter of <code>reduce</code> and the <code>resultIfEmpty</code> parameter of <code>parallelReduce</code>: the identity value.</p>

<div><pre><code>extension Sequence where Element: Monoid {
  func reduce() -&gt; Element {
    return self.reduce(.e(), { $0.op($1) })
  }
}
</code></pre></div>

<p>There’s an argument to be had over whether this is <em>sufficiently</em> more expressive than <code>Semigroup</code> to be worth a separate protocol and another method on <code>Sequence</code>, but let’s set that aside. The problem we run into is that <strong>the hypothetical <code>operationgroup</code> I came up with in the last section now has to grow actual members</strong>.</p>

<div><pre><code>operationgroup Semigroup&lt;Element&gt;: (Element, Element) -&gt; Element {}
operationgroup Monoid&lt;Element&gt;: Semigroup&lt;Element&gt; {
  var identity: Element
}

func +(left: Int, right: Int) -&gt; Int: Monoid {
  …
} where {
  var identity: Int { return 0 }
}
</code></pre></div>

<p>My makeshift syntax is getting uglier, and I have even less idea how it would work for closures at this point. Maybe this <code>operationgroup</code> thing isn’t going to fly.</p>

<hr />

<p>This whole time I’ve been saying that the interesting part of a semigroup is the operation:</p>

<ul>
  <li>It takes two values with the same type, and returns another value of that type.</li>
  <li>It’s associative. (This actually implies the previous condition, but let’s separate that just to be explicit.)</li>
</ul>

<p>The way I’ve phrased the definition, the element type feels almost incidental, and therefore it makes sense to talk about the operation holding the “conformance”. But these aren’t the only kind of algebraic structures that show up in computer science. There’s another one called a <em>lattice</em> that looks something like this:</p>

<div><pre><code>protocol Lattice {
  // **AXIOM** Should satisfy:
  //   Commutativity: a.join(b) == b.join(a)
  //   Associativity: a.join(b).join(c) == a.join(b.join(c))
  //   Absorption:    a.join(a.meet(b)) == a
  //                  a.meet(a.join(b)) == a
  // for all values a, b, and c
  func join(_ other: Self) -&gt; Self

  // **AXIOM** Should satisfy:
  //   Commutativity: a.meet(b) == b.meet(a)
  //   Associativity: a.meet(b).meet(c) == a.meet(b.meet(c))
  //   Absorption:    a.join(a.meet(b)) == a
  //                  a.meet(a.join(b)) == a
  // for all values a, b, and c
  func meet(_ other: Self) -&gt; Self
}
</code></pre></div>

<p>A concrete example of a lattice is a <code>Set</code> with its <code>union</code> and <code>intersection</code> operations.<a href="#fn:refines">3</a></p>

<div><pre><code>struct SetInclusion&lt;Element&gt;: RawRepresentable {
  var rawValue: Set&lt;Element&gt;
  init(rawValue: Set&lt;Element&gt;) { … }
  init(_ rawValue: Set&lt;Element&gt;) { … } // for convenience
}
extension SetInclusion: Lattice {
  func join(_ other: SetInclusion&lt;Element&gt;) -&gt; SetInclusion&lt;Element&gt; {
    return SetInclusion(self.rawValue.union(other.rawValue))
  }
  func meet(_ other: SetInclusion&lt;Element&gt;) -&gt; SetInclusion&lt;Element&gt; {
    return SetInclusion(self.rawValue.intersection(other.rawValue))
  }
}
</code></pre></div>

<p>This is a death knell for the idea of <code>operationgroup</code>. Neither <code>join</code> nor <code>meet</code> is a “fundamental” operation that carries the “lattice-ness” of <code>SetInclusion</code>, and moreover their requirements both depend on the other operation!</p>

<p>The trouble is that <strong>algebraic abstractions don’t constrain themselves to relating a single operation with a single type</strong>; they can have multiple operations or even multiple types and still be useful. We could certainly decide that some <em>subset</em> of abstractions deserves to be privileged with a language feature, but then we’ll have to make compromises when we go beyond that subset.</p>

<p>(In fact, you could say we’ve already done this: abstractions that really <em>do</em> feel like they’re about one type in particular work very nicely as protocols, like <code>Sequence</code> and <code>RawRepresentable</code>.)</p>

<hr />

<p>So, the missing language feature here, perhaps, is a way to relate <em>multiple</em> types and <em>multiple</em> operations, and refer to those relations directly. Surprise, there is in fact a language that does this: <a href="https://en.wikipedia.org/wiki/Standard_ML#Module_system">ML</a>! ML’s equivalent of protocols is <em>signatures,</em> but the things that conform to them aren’t the <em>datatypes</em> (in Swift terms, enums with tuple payloads) but containers called <em>structures.</em> ML structures aren’t like Swift structs; they’re “just” relations between types and operations that satisfy signatures. In Swift syntax they’d look something like this:</p>

<div><pre><code>structure SetInclusion&lt;T&gt;: Lattice {
  typealias Element = Set&lt;T&gt;
  func join(_ left: Set&lt;T&gt;, _ right: Set&lt;T&gt;) -&gt; Set&lt;T&gt; {
    return left.union(right)
  }
  func meet(_ left: Set&lt;T&gt;, _ right: Set&lt;T&gt;) -&gt; Set&lt;T&gt; {
    return left.intersection(right)
  }
}
</code></pre></div>

<p>(Disclaimer: I have never actually <em>written</em> anything in ML; this is all accrued knowledge and surface-level research done in the process of writing this post, so I could very easily be getting things wrong.)</p>

<p>Let’s rewrite the example with the <code>Sum</code> struct in this way too.</p>

<div><pre><code>structure Sum&lt;T: Arithmetic&gt;: Semigroup {
  typealias Element = T
  func op(_ left: T, _ right: T) -&gt; T {
    return left + right
  }
}

extension RandomAccessCollection {
  func parallelReduce&lt;Operation: Semigroup&gt;(
    _ resultIfEmpty: Element,
    _ operation: Operation) -&gt; Element
  where Operation.Element == Element {
    …
  }
}

let questionScores: [Int] = …
// let totalScore: Int = questionScores.reduce(0, +)
let totalScore: Int = questionScores.parallelReduce(0, Sum)
</code></pre></div>

<p>So, we’re in the Kingdom of Nouns again, but now we know why. <strong>The noun isn’t the name of the operation; it’s the name of the relation between the type(s) and operation(s).</strong> Curiously, math doesn’t usually give these relations nice, easy names, but that’s because a lot of them are ad hoc: <code>Sum</code> is “(<a href="https://en.wikipedia.org/wiki/Integer">ℤ</a>, +)”, <code>SetInclusion</code> is “(<em>S,</em> ∪, ∩)”, and so on. That wouldn’t be great for us programmers, where we may actually need to refer to the relation in multiple places.</p>

<p>(And if you haven’t spotted it already, surprise #2 is that you don’t <em>really</em> need a new declaration kind to do this in Swift. The “structure” above is equivalent to the current idiom for sub-module “namespaces”: an enum with no cases.)</p>

<p>Some of the uneasiness here, at least for me, comes from the clash with normal object-oriented style. We have operations that operate on a particular type, and yet we don’t necessarily want to make them methods of that type. Instead, we’re mapping existing methods into a generic interface, and that generic interface may not have a good choice to be the <code>self</code> type.</p>

<p>On the other hand, being able to directly manipulate “structures” lets you do convenient things like having a <code>Set</code> that uniques class instances by pointer identity instead of calling <code>==</code>.<a href="#fn:conformance">4</a> Hm, this is starting to sound like the <a href="https://en.wikipedia.org/wiki/Strategy_pattern">strategy pattern</a>…</p>

<p>…and with that we’ve come full circle back to design patterns.</p>

<hr />

<p>So, to conclude:</p>

<ul>
  <li>Yes, being able to check properties of operations pulls us into the Kingdom of Nouns in today’s Swift.</li>
  <li>No, Swift cannot really do better in the general case, because it’s not always about just one operation.</li>
  <li>Yes, a <code>RawRepresentable</code> struct is the way to go when a type can conform to a protocol in multiple ways.</li>
</ul>

<p>I think a possible takeaway from this for Swift is that defining such structs could be easier. There are a few ideas floating around about that, primarily a keyword like <code>newtype</code> that would handle the <code>RawRepresentable</code> conformance, and then some way to forward protocol implementations to the wrapped value. And possibly some automatic wrapping/unwrapping. But we’ll see.</p>

<p>Hope you’ve enjoyed this exploration of mine! Thanks for following along.</p>

<hr />
<div>
  <ol>
    <li>

      <p>Fun fact: <a href="https://en.wikipedia.org/wiki/Associative_property#Nonassociativity_of_floating_point_calculation">addition of floating-point values is <em>not</em> associative</a>. This isn’t important at all for this post, but it is one of the reasons why for a long time Swift didn’t have a common “Arithmetic” protocol that covers both integers and floating-point numbers: the straightforward implementation of a generic algorithm often doesn’t do the right thing for floats.</p>

      <p>Strictly speaking, integer addition is not associative in Swift either because <code>+</code> traps when you overflow: consider <code>.max + 1 + (-1)</code>. <code>&amp;+</code> <em>is</em> associative but doesn’t always behave like addition. I left this out of the original post because I literally forgot to consider adding negative numbers. <a href="#fnref:float">↩︎</a></p>
    </li>
    <li>
      <p>The <code>.lazy</code> is there because we don’t <em>really</em> care about the intermediate state of <code>Sum</code> values. Many other languages default to this behavior; Swift chose not to in order not to thrust types like <code>LazyMappedCollection</code> in new users’ faces when they really just wanted an Array back. One can argue over whether this was the right decision; I won’t address it further in this post. <a href="#fnref:lazy">↩︎</a></p>
    </li>
    <li>
      <p>Why not make <code>Set</code> conform to <code>Lattice</code> directly? Turns out there’s another way that Sets can form a lattice: if you can do some kind of ordering on their elements. For example, a set of sets can form a “partition refinement lattice”, as illustrated in <a href="https://en.wikipedia.org/wiki/Lattice_(order)#/media/File:Lattice_of_partitions_of_an_order_4_set.svg">this diagram from Wikipedia</a>. But I think the union/intersection lattice is probably the one you’d see most often. <a href="#fnref:refines">↩︎</a></p>
    </li>
    <li>
      <p>The Swift compiler actually <em>does</em> reason directly about “structures”, calling them “conformances”. Just as an ML structure describes relations between types and operations that satisfy a signature, a Swift conformance describes how a particular type conforms to a protocol, including all of the operations and associated types. A conformance even has a run-time representation, which shows up as a hidden argument to a generic function. But the language doesn’t give you any ability to interact with conformances directly. <a href="#fnref:conformance">↩︎</a></p>
    </li>
  </ol>
</div>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://8thlight.com/blog/ray-hightower/2017/09/07/chicagoruby-new-leadership.html">New Leadership for ChicagoRuby</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://8thlight.com/blog/feed/atom.xml">8th Light Blog</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">practices</span>
              <span class="tag">principles</span>
              <span class="tag">company</span>
            </div>
            <span class="subtitle is-7">Thu Sep 07 2017 06:00:00 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>After 10 years as lead organizer of <a href="https://chicagoruby.org/">ChicagoRuby</a>, I have decided to step down. My colleague here at 8th Light, <a href="https://www.meetup.com/ChicagoRuby/members/190520517/">Nicole Carpenter</a>, will be taking over leadership duties effective September 7, 2017. Please join me in congratulating Nicole!</p>

<p>I assumed leadership of the Chicago Area Ruby on Rails Meetup Group in 2007. We had 78 members and a mouthful of a name. We immediately shortened the name to ChicagoRuby. Growing the membership took time. Today, ChicagoRuby has over 4,200 members.</p>

<p>We started as a small band in the Western Suburbs of Chicago. A separate Ruby group, Chirb, met in downtown Chicago. Several local Ruby enthusiasts attended meetings hosted by both groups; everything was friendly. We merged the two groups under the ChicagoRuby banner in 2009.</p>

<p>Ruby was just over a decade old when ChicagoRuby began, but Rails was relatively new. When one member asked “Why isn’t there a Rails conference in Chicago?” we stepped up and launched the <a href="https://windycityrails.com/">WindyCityRails</a> conference. The event attracts speakers and attendees planet-wide. Hosting WindyCityRails, now in its 10th year, enables all of us to learn about Ruby, Rails, and related tech from an even wider audience. </p>

<p>Meeting the needs of 4,200 members requires a team of organizers. Through our team of 16 organizers, ChicagoRuby has continued to maintain and grow our Meetups, organized and hosted several successful conferences, collaborated with tech enthusiasts in the Caribbean, and joined forces with colleagues in the Python and Java communities to launch the <a href="http://chicagopolyglot.com">Chicago Polyglot Mingle</a>.</p>

<p>All of this seemed farfetched 10 years ago. We were a small group of enthusiasts, and Ruby on  Rails was a framework that had yet to be embraced by mainstream businesses.</p>

<p>These days, Ruby on Rails is mainstream. Basecamp, the first Ruby on Rails application, is revered as a successfully bootstrapped business. Ruby has become a preferred language at coding bootcamps and other training programs because it is accessible to young programmers, but also because it is a language that businesses rely on.</p>

<p>Serving as lead organizer for ChicagoRuby has been exciting these last 10 years. To my fellow members of ChicagoRuby: Thank you for the opportunity to serve. You and I will see each other around the community from time to time. And perhaps we will embark on an adventure together!</p>

<p>Speaking of growth: If you want to boost your skills with Ruby, Rails, or related tech, consider WindyCityRails. The presentations are always outstanding and the networking is powerful. Use the discount code <strong>8THLIGHT</strong> when you register for the 2017 conference, and you’ll get $50 off. See you at <a href="https://windycityrails.com/">WindyCityRails</a>!</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://ronjeffries.com/articles/017-08ff/ipad-l-dates/">Restarting after long gap. Maybe some dates.</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="http://ronjeffries.com/feed.xml">Ron Jeffries</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">practices</span>
              <span class="tag">agile</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Thu Sep 07 2017 05:00:00 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            It has been a week since Tozier and I last worked, and I spent most of the time under repair. I'm thinking we'll work on the date-sensitive stuff. I'm mostly wrong.
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://www.fourmilab.ch/fourmilog/archives/2017-09/001709.html">Reading List: Making Contact</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://www.fourmilab.ch/fourmilog/atom_10.xml">Fourmilog - Fourmilab Change Log</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Thu Sep 07 2017 01:14:05 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            
Scoles, Sarah.
<a href="http://www.amazon.com/dp/1681774410/?tag=fourmilabwwwfour" target="Amazon_Fourmilab">Making Contact</a>.
New York: Pegasus Books, 2017.
ISBN 978-1-68177-441-1.

There are few questions in our scientific inquiry into the universe and
our place within it more profound than “are we alone?”  As we have learned
more about our world and the larger universe in which it exists, this
question has become ever more fascinating.  We now know that our planet,
once thought the centre of the universe, is but one of what may be
hundreds of billions of planets in our own galaxy, which is one of hundreds
of billions of galaxies in the observable universe.  Not long ago, we knew
only of the planets in our own solar system, and some astronomers
believed planetary systems were rare, perhaps formed by freak
encounters between two stars following their orbits around the galaxy.
But now, thanks to exoplanet hunters and, especially, the
<a href="https://en.wikipedia.org/wiki/Kepler_%28spacecraft%29" target="Fourmilab_readingListAux">Kepler spacecraft</a>,
we know that it's “planets, planets, everywhere”—most stars
have planets, and many stars have planets where conditions may be
suitable for the origin of life.
<p>
If this be the case, then when we gaze upward at the myriad stars in the
heavens, might there be other eyes (or whatever sense organs they use
for the optical spectrum) looking back from planets of those stars toward our
Sun, wondering if <em>they</em> are alone?  Many are the children, and
adults, who have asked themselves that question when standing under a
pristine sky.  For the ten year old Jill Tarter, it set her on a path
toward a career which has been almost
coterminous with humanity's efforts to discover communications from
extraterrestrial civilisations—an effort which continues today,
benefitting from advances in technology unimagined when she undertook
the quest.
</p><p>
World War II had seen tremendous advancements in radio communications,
in particular the short wavelengths (“microwaves”) used
by radar to detect enemy aircraft and submarines.  After the war,
this technology provided the foundation for the new field of
radio astronomy, which expanded astronomers' window on the
universe from the traditional optical spectrum into wavelengths
that revealed phenomena never before observed nor, indeed, imagined,
and hinted at a universe which was much larger, complicated, and
violent than previously envisioned.
</p><p>
In 1959, Philip Morrison and Guiseppe Cocconi published a paper in
Nature in which they calculated that using only
technologies and instruments already existing on the Earth, intelligent
extraterrestrials could send radio messages across the distances to the
nearby stars, and that these messages could be received, detected, and
decoded by terrestrial observers.  This was the origin of SETI—the Search
for Extraterrestrial Intelligence.  In 1960, Frank Drake used a radio
telescope to
<a href="https://en.wikipedia.org/wiki/Project_Ozma" target="Fourmilab_readingListAux">search for signals</a>
from two nearby star systems; he heard nothing.
</p><p>
As they say, absence of evidence is not evidence of absence, and this
is acutely the case in SETI.  First of all, consider that you must first
decide what kind of signal aliens might send.  If it's something which
can't be distinguished from natural sources, there's little hope you'll be
able to tease it out of the cacophony which is the radio spectrum.  So we
must assume they're sending something that doesn't appear natural.  But
what is the variety of natural sources?  There's a dozen or so Ph.D. projects
just answering that question, including some surprising discoveries of
natural sources nobody imagined, such as
<a href="https://en.wikipedia.org/wiki/Pulsar" target="Fourmilab_readingListAux">pulsars</a>, which were sufficiently
strange that when first observed they were called “LGM” sources
for “Little Green Men”.  On what frequency are they sending (in
other words, where do we have to turn our dial to receive them, for those
geezers who remember radios with dials)? The most efficient signals
will be those with a very narrow frequency range, and there are billions of
possible frequencies the aliens might choose.  We could be pointed in the
right place, at the right time, and simply be tuned to the wrong station.
</p><p>
Then there's that question of “the right time”.  It would be
absurdly costly to broadcast a beacon signal in all directions at all times:
that would require energy comparable to that emitted by a star (which, if
you think about it, does precisely that).  So it's likely that any civilisation
with energy resources comparable to our own would transmit in a narrow beam to
specific targets, switching among them over time.  If we didn't happen to
be listening when they were sending, we'd never know they were calling.
</p><p>
If you put all of these constraints together, you come up with what's called
an “observational
<a href="https://en.wikipedia.org/wiki/Phase_space" target="Fourmilab_readingListAux">phase space</a>”—a
multidimensional space of frequency, intensity, duration of
transmission, angular extent of transmission, bandwidth, and
other parameters which determine whether you'll detect the
signal.  And that assumes you're listening at all, which depends
upon people coming up with the money to fund the effort and pursue
it over the years.
</p><p>
It's beyond daunting.  The space to be searched is so large, and our ability to
search it so limited, that negative results, even after decades of observation,
are equivalent to walking down to the seashore, sampling a glass of ocean
water, and concluding that based on the absence of fish, the ocean contained
no higher life forms.  But suppose you find a fish?  That would change
<em>everything</em>.
</p><p>
Jill Tarter began her career in the mainstream of astronomy.  Her
Ph.D. research at the University of California, Berkeley was on
<a href="https://en.wikipedia.org/wiki/Brown_dwarf" target="Fourmilab_readingListAux">brown dwarfs</a>
(bodies more massive than gas giant planets but too small to
sustain the nuclear fusion reactions which cause stars to
shine—a brown dwarf emits weakly in the infrared as it
slowly radiates away the heat from the gravitational contraction
which formed it).  Her work was supported by a federal grant, which
made her uncomfortable—what relevance did brown dwarfs
have to those compelled to pay taxes to fund investigating them?  During her Ph.D.
work, she was asked by a professor in the department to help
with an aged computer she'd used in an earlier project.  To acquaint
her with the project, the professor asked her to read the
<a href="https://en.wikipedia.org/wiki/Project_Cyclops" target="Fourmilab_readingListAux">Project Cyclops</a>
report.  It was a conversion experience.
</p><p>
Project Cyclops was a NASA study conducted in 1971 on how to perform
a definitive search for radio communications from intelligent
extraterrestrials.  Its
<a href="https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/19730010095.pdf" target="Fourmilab_readingListAux">report</a>
[18.2 Mb PDF], issued in 1972, remains the “bible”
for radio SETI, although advances in technology, particularly in computing,
have rendered some of its recommendations obsolete.  The product of a NASA
which was still conducting missions to the Moon, it was grandiose in
scale, envisioning a large array of radio telescope dishes able to search
for signals from stars up to 1000 light years in distance (note that this is
still a tiny fraction of the stars in the galaxy, which is around 150,000
light years in diameter).  The estimated budget for the project was between
6 and 10 billion dollars (multiply those numbers by around six to get
present-day funny money) spent over a period of ten to fifteen years.
The report cautioned that there was no guarantee of success during that
period, and that the project should be viewed as a long-term endeavour
with ongoing funding to operate the system and continue the search.
</p><p>
The Cyclops report arrived at a time when NASA was downsizing
and scaling back its ambitions: the final three planned lunar landing
missions had been cancelled in 1970, and production of additional Saturn V
launch vehicles had been terminated the previous year.  The budget
climate wasn't hospitable to Apollo-scale projects of any description,
especially those which wouldn't support lots of civil service and
contractor jobs in the districts and states of NASA's patrons in
congress.  Unsurprisingly, Project Cyclops simply landed on the pile
of ambitious NASA studies that went nowhere.  But to some who read it,
it was an inspiration.  Tarter thought, “This is the first time
in history when we don't just have to believe or not believe.  Instead
of just asking the priests and philosophers, we can try to find an
answer.  This is an old and important question, and I have the
opportunity to change how we try to answer it.”  While some
might consider searching the sky for “little green men”
frivolous and/or absurd, to Tarter this, not the arcana of
brown dwarfs, was something worthy of support, and of her time and
intellectual effort, “something that could impact people's
lives profoundly in a short period of time.”
</p><p>
The project to which Tarter had been asked to contribute,
<a href="https://en.wikipedia.org/wiki/SERENDIP" target="Fourmilab_readingListAux">Project SERENDIP</a>
(a painful acronym of Search for Extraterrestrial Radio
Emissions from Nearby Developed Intelligent Populations)
was extremely modest compared to Cyclops.  It had no dedicated
radio telescopes at all, nor even dedicated time on existing
observatories.  Instead, it would “piggyback” on
observations made for other purposes, listening to the feed from
the telescope with an instrument designed to detect the kind of
narrow-band beacons envisioned by Cyclops.  To cope with the
problem of not knowing the frequency on which to listen, the
receiver would monitor 100 channels simultaneously.  Tarter's
job was programming the
<a href="https://en.wikipedia.org/wiki/PDP-8" target="Fourmilab_readingListAux">PDP 8/S</a>
computer to monitor the receiver's output and
search for candidate signals.  (Project SERENDIP is still in
operation today, employing hardware able to simultaneously
monitor 128 million channels.)
</p><p>
From this humble start, Tarter's career direction was set.  All
of her subsequent work was in SETI.  It would be a roller-coaster
ride all the way.  In 1975, NASA had started a modest study to
research (but not build) technologies for microwave SETI searches.
In 1978, the program came into the sights of senator William
Proxmire, who bestowed upon it his “Golden Fleece”
award.  The program initially survived his ridicule, but in 1982, the
budget zeroed out the project.  Carl Sagan personally intervened
with Proxmire, and in 1983 the funding was reinstated, continuing
work on a more capable spectral analyser which could be used with
existing radio telescopes.
</p><p>
Buffeted by the start-stop support from NASA and encouraged by
Hewlett-Packard executive
<a href="https://en.wikipedia.org/wiki/Bernard_M._Oliver" target="Fourmilab_readingListAux">Bernard
Oliver</a>, a supporter of SETI from its inception, Tarter
decided that SETI needed its own institutional home, one dedicated
to the mission and able to seek its own funding independent of the
whims of congressmen and bureaucrats.  In 1984,
the <a href="https://en.wikipedia.org/wiki/SETI_Institute" target="Fourmilab_readingListAux">SETI Institute</a>
was incorporated in California.  Initially funded by Oliver, over
the years major contributions have been made by technology moguls
including William Hewlett, David Packard, Paul Allen, Gordon Moore,
and Nathan Myhrvold.  The SETI Institute receives no government
funding whatsoever, although some researchers in its employ, mostly
those working on astrobiology, exoplanets, and other topics not
directly related to SETI, are supported by research grants from
NASA and the National Science Foundation.  Fund raising was a
skill which did not come naturally to Tarter, but it was mission
critical, and so she mastered the art.  Today, the SETI Institute
is considered one of the most savvy privately-funded research
institutions, both in seeking large donations and in grass-roots
fundraising.
</p><p>
By the early 1990s, it appeared the pendulum had swung once
again, and NASA was back in the SETI game.  In 1992, a program was
funded to conduct a two-pronged effort: a targeted search of 800
nearby stars, and an all-sky survey looking for stronger beacons.
Both would employ what were then state-of-the-art spectrum analysers
able to monitor 15 million channels simultaneously.  After
just a year of observations, congress once again pulled the plug.
The SETI Institute would have to go it alone.
</p><p>
Tarter launched
<a href="http://www.seti.org/seti-institute/project/details/project-phoenix" target="Fourmilab_readingListAux">Project Phoenix</a>,
to continue the NASA targeted search program using the orphaned
NASA spectrometer hardware and whatever telescope time could be
purchased from donations to the SETI Institute.  In 1995,
observations resumed at the Parkes radio telescope in Australia,
and subsequently a telescope at the National Radio Astronomy
Observatory in Green Bank, West Virginia, and the 300 metre dish
at Arecibo Observatory in Puerto Rico.  The project continued
through 2004.
</p><p>
What should SETI look like in the 21st century?  Much had changed since
the early days in the 1960s and 1970s.  Digital electronics and
computers had increased in power a billionfold, not only making it
possible to scan a billion channels simultaneously and automatically
search for candidate signals, but to combine the signals from a
large number of independent, inexpensive antennas (essentially,
glorified satellite television dishes), synthesising the aperture of
a huge, budget-busting radio telescope.  With progress in electronics
expected to continue in the coming decades, any capital investment in
antenna hardware would yield an exponentially growing science harvest
as the ability to analyse its output grew over time.  But to take
advantage of this technological revolution, SETI could no longer rely
on piggyback observations, purchased telescope time, or allocations
at the whim of research institutions: “SETI needs its own
telescope”—one optimised for the mission and designed to
benefit from advances in electronics over its lifetime.
</p><p>
In a series of meetings from 1998 to 2000, the specifications of such an
instrument were drawn up: 350 small antennas, each 6 metres in diameter,
independently steerable (and thus able to be used all together, or in
segments to simultaneously observe different targets), with electronics
to combine the signals, providing an effective aperture of 900 metres
with all dishes operating.  With initial funding from Microsoft
co-founder Paul Allen (and with his name on the project, the
<a href="https://en.wikipedia.org/wiki/Allen_Telescope_Array" target="Fourmilab_readingListAux">Allen Telescope
Array</a>), the project began construction in 2004.  In 2007, observations
began with the first 42 dishes.  By that time, Paul Allen had lost
interest in the project, and construction of additional dishes was placed
on hold until a new benefactor could be found.  In 2011, a funding crisis
caused the facility to be placed in hibernation, and the
observatory was sold to SRI International for US$ 1.  Following
a crowdfunding effort led by the SETI Institute, the observatory was
re-opened later that year, and continues operations to this date.  No
additional dishes have been installed: current work concentrates on
upgrading the electronics of the existing antennas to increase
sensitivity.
</p><p>
Jill Tarter retired as co-director of the SETI Institute in 2012,
but remains active in its scientific, fundraising, and outreach
programs.  There has never been more work in SETI underway than
at the present.  In addition to observations with the Allen
Telescope Array, the
<a href="https://en.wikipedia.org/wiki/Breakthrough_Listen" target="Fourmilab_readingListAux">Breakthrough
Listen</a> project, funded at US$ 100 million over ten years by
Russian billionaire Yuri Milner, is using thousands of hours of
time on large radio telescopes, with a goal of observing a million
nearby stars and the centres of a hundred galaxies.  All data are
available to the public for analysis.  A new frontier, unimagined in
the early days of SETI, is optical SETI.  A pulsed laser, focused
through a telescope of modest aperture, is able to easily outshine the
Sun in a detector sensitive to its wavelength and pulse duration.
In the optical spectrum, there's no need for fancy electronics to
monitor a wide variety of wavelengths: all you need is a prism or
diffraction grating.  The SETI Institute has just successfully completed
a US$ 100,000 Indiegogo campaign to crowdfund the first phase of the
<a href="https://www.indiegogo.com/projects/laser-seti-first-ever-all-sky-all-the-time-search-science#/" target="Fourmilab_readingListAux">Laser
SETI</a> project, which has as its ultimate goal an all-sky,
all-the-time search for short pulses of light which may be
signals from extraterrestrials or new natural phenomena to which
no existing astronomical instrument is sensitive.
</p><p>
People often ask Jill Tarter what it's like to spend your entire
career looking for something and not finding it.  But she, and
everybody involved in SETI, always knew the search would not be
easy, nor likely to succeed in the short term.  The reward for
engaging in it is being involved in founding a new field of
scientific inquiry and inventing and building the tools which allow
exploring this new domain.  The search is vast, and to date we have
barely scratched the surface.  About all we can rule out, after more
than half a century, is a Star Trek-like universe
where almost every star system is populated by aliens chattering
away on the radio.  Today, the SETI enterprise, entirely privately
funded and minuscule by the standards of “big science”,
is strongly coupled to the exponential growth in computing power
and hence, roughly doubles its ability to search around every
two years.
</p><p>
The question “are we alone?” is one which has profound
implications either way it is answered.  If we discover one or more
advanced technological civilisations (and they will almost certainly
be more advanced than we—we've only had radio for a little more
than a century, and there are stars and planets in the galaxy
billions of years older than ours), it will mean it's possible to
grow out of the daunting problems we face in the adolescence of our
species and look forward to an exciting and potentially unbounded future.
If, after exhaustive searches (which will take at least another
fifty years of continued progress in expanding the search space),
it looks like we're alone, then intelligent life is so rare that we
may be its only exemplar in the galaxy and, perhaps, the universe.
Then, <em>it's up to us</em>.  Our destiny, and duty, is to ensure that
this spark, lit within us, will never be extinguished.
</p>

          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://brandur.org/http-transactions">Using Atomic Transactions to Power an Idempotent API</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://brandur.org/articles.atom">Brandur Leach</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">practices</span>
              <span class="tag">tools</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Wed Sep 06 2017 17:00:14 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>The software industry as a whole contains a lot of people
doing a lot of different things, but for every developer
working on new embedded firmware, there’s about ten
building the linchpin of modern software – CRUD apps that
serve requests over HTTP. A lot of these apps are backed by
MVC frameworks like Ruby on Rails or ASP.NET, and backed by
ACID-compliant relational databases like Postgres or SQL
Server.</p>

<p>Sharp edges in production can lead to all kinds of
unexpected cases during the execution of an HTTP request –
client disconnects, application bugs that fail a request
midway through, and timeouts are all extraordinary
conditions that will occur regularly given enough request
volume. Databases can protect applications against
integrity problems with their transactions, and it’s worth
taking a little time to think about how to make best use of
them.</p>

<p>There’s a surprising symmetry between an HTTP request and a
database’s transaction. Just like the transaction, an HTTP
request is a transactional unit of work – it’s got a
clear beginning, end, and result. The client generally
expects a request to execute atomically and will behave as
if it will (although that of course varies based on
implementation). Here we’ll look at an example service to
see how HTTP requests and transactions apply nicely to one
another.</p>

<a href="#one-to-one">The 1:1 Model</a>

<p>I’m going to make the case that for a common idempotent
HTTP request, requests should map to backend transactions
at 1:1. For every request, all operations are committed or
aborted as part of a single transaction within it.</p>


  <p><a href="/assets/http-transactions/http-transactions.svg"></a></p>
  Transactions (tx1, tx2, tx3) mapped to HTTP requests at a 1:1 ratio.


<p>At first glance requiring idempotency may sound like a
sizeable caveat, but in many APIs operations can be made to
be idempotent by massaging endpoint verbs and behavior, and
moving non-idempotent operations like network calls to
background jobs.</p>

<p>Some APIs can’t be made idempotent and those will need a
little extra consideration. We’ll look at what to do about
them in more detail later as a follow up to this article.</p>

<a href="#create-user">A simple user creation service</a>

<p>Let’s build a simple test service with a single “create
user” endpoint. A client hits it with an <code>email</code> parameter,
and the endpoint responds with status <code>201 Created</code> to
signal that the user’s been created. The endpoint is also
idempotent so that if a client hits the endpoint again with
the same parameter, it responds with status <code>200 OK</code> to
signal that everything is still fine.</p>

<pre><code>PUT /users?email=jane@example.com
</code></pre>

<p>On the backend, we’re going to do three things:</p>

<ol>
<li>Check if the user already exists, and if so, break and
do nothing.</li>
<li>Insert a new record for the user.</li>
<li>Insert a new “user action” record. It’ll serve as an
audit log which comes with a reference to a user’s ID,
an action name, and a timestamp.</li>
</ol>

<p>We’ll build our implementation with Postgres, Ruby, and an
ORM in the style of ActiveRecord or Sequel, but these
concepts apply beyond any specific technology.</p>

<h3><a href="#database-schema">Database schema</a></h3>

<p>The service defines a simple Postgres schema containing
tables for its users and user actions <a href="#footnote-1">1</a>:</p>

<pre><code>CREATE TABLE users (
    id    BIGSERIAL PRIMARY KEY,
    email TEXT      NOT NULL CHECK (char_length(email) &lt;= 255)
);

-- our &quot;user action&quot; audit log
CREATE TABLE user_actions (
    id          BIGSERIAL   PRIMARY KEY,
    user_id     BIGINT      NOT NULL REFERENCES users (id),
    action      TEXT        NOT NULL CHECK (char_length(action) &lt; 100),
    occurred_at TIMESTAMPTZ NOT NULL DEFAULT now()
);
</code></pre>

<h3><a href="#implementation">Backend implementation</a></h3>

<p>The server route checks to see if the user exists. If so,
it returns immediately. If not, it creates the user and
user action, and returns. In both cases, the transaction
commits successfully.</p>

<pre><code>put &quot;/users/:email&quot; do |email|
  DB.transaction(isolation: :serializable) do
    user = User.find(email)
    halt(200, 'User exists') unless user.nil?

    # create the user
    user = User.create(email: email)

    # create the user action
    UserAction.create(user_id: user.id, action: 'created')

    # pass back a successful response
    [201, 'User created']
  end
end
</code></pre>

<p>The SQL that’s generated in the case of a successful
insertion looks roughly like:</p>

<pre><code>START TRANSACTION
    ISOLATION LEVEL SERIALIZABLE;

SELECT * FROM users
    WHERE email = 'jane@example.com';

INSERT INTO users (email)
    VALUES ('jane@example.com');

INSERT INTO user_actions (user_id, action)
    VALUES (1, 'created');

COMMIT;
</code></pre>

<a href="#concurrency-protection">Concurrency protection</a>

<p>Readers with sharp eyes may have noticed a potential
problem: our <code>users</code> table doesn’t have a <code>UNIQUE</code>
constraint on its <code>email</code> column. The lack of one could
potentially allow two interleaved transactions to run their
<code>SELECT</code> phase one concurrently and get empty results.
They’d both follow up with an <code>INSERT</code>, leaving a
duplicated row.</p>


  <p><a href="/assets/http-transactions/concurrent-race.svg"></a></p>
  A data race causing two concurrent HTTP requests to insert the same row.


<p>Luckily, in this example we’ve used an even more powerful
mechanism than <code>UNIQUE</code> to protect our data’s correctness.
Invoking our transaction with <code>DB.transaction(isolation:
:serializable)</code> starts it in <code>SERIALIZABLE</code>; an isolation
level so powerful that its guarantees might seem
practically magical.  It emulates serial transaction
execution as if each outstanding transaction had been
executed one after the other, rather than concurrently. In
cases like the above where a race condition would have
caused one transaction to taint the results of another, one
of the two will fail to commit with a message like this
one:</p>

<pre><code>ERROR:  could not serialize access due to read/write dependencies among transactions
DETAIL:  Reason code: Canceled on identification as a pivot, during commit attempt.
HINT:  The transaction might succeed if retried.
</code></pre>

<p>We’re not going to look into how <code>SERIALIZABLE</code> works, but
sufficed to say it may detect a number of different data
races for us, and if it does it’ll abort a transaction when
it tries to commit.</p>

<h3><a href="#abort-retry">Retrying an abort</a></h3>

<p>Even though in our example a race should be rare, we’d
prefer to handle it correctly in our application code so
that it doesn’t bubble up as a 500 to a client. This is
possible by wrapping the request’s core operations in a
loop:</p>

<pre><code>MAX_ATTEMPTS = 2

put &quot;/users/:email&quot; do |email|
  MAX_ATTEMPTS.times do
    begin
      DB.transaction(isolation: :serializable) do
        ...
      end

      # Success! Leave the loop.
      break

    rescue Sequel::SerializationFailure
      log.error &quot;Failed to commit serially: #{$!}&quot;
      # Failure: fall through to the next loop.
    end
  end
end
</code></pre>

<p>In this case, we might have more than one of the same
transaction mapped to the HTTP request like so:</p>


  <p><a href="/assets/http-transactions/transaction-retry.svg"></a></p>
  An aborted transaction being retried within the same request.


<p>These loops will be more expensive than usual, but again,
we’re protecting ourselves against an unusual race. In
practice, unless callers are particularly contentious,
they’ll rarely occur.</p>

<p>Gems like <a href="https://github.com/jeremyevans/sequel">Sequel</a> can handle this for you
automatically (this code will behave similarly to the loop
above):</p>

<pre><code>DB.transaction(isolation: :serializable,
    retry_on: [Sequel::SerializationFailure]) do
  ...
end
</code></pre>

<h3><a href="#layers">Data protection in layers</a></h3>

<p>I’ve taken the opportunity to demonstrate the power of a
serializable transaction, but in real life you’d want to
put in a <code>UNIQUE</code> constraint on <code>email</code> even if you
intended to use the serializable isolation level. Although
<code>SERIALIZABLE</code> will protect you from a duplicate insert, an
added <code>UNIQUE</code> will act as one more check to protect your
application against incorrectly invoked transactions or
buggy code. It’s worth having it in there.</p>

<a href="#background-jobs">Background jobs</a>

<p>It’s a common pattern to add jobs to a background queue
during an HTTP request so that they can be worked
out-of-band and a waiting client doesn’t have to block on
an expensive operation.</p>

<p>Let’s add one more step to our user service above. In
addition to creating user and user action records, we’ll
also make an API request to an external support service to
tell it that a new account’s been created. We’ll do that by
queuing a background job because there’s no reason that it
has to happen in-band with the request.</p>

<pre><code>put &quot;/users/:email&quot; do |email|
  DB.transaction(isolation: :serializable) do
    ...

    # enqueue a job to tell an external support service
    # that a new user's been created
    enqueue(:create_user_in_support_service, email: email)

    ...
  end
end
</code></pre>

<p>If we used a common job queue like Sidekiq to do this work,
then in the case of a transaction rollback (like we talked
about above where two transactions conflict), we could end
up with an invalid job in the queue. It’s referencing data
that no longer exists, so no matter how many times job
workers retried it, it can never succeed.</p>

<h3><a href="#staged-jobs">Transaction-staged jobs</a></h3>

<p>A way around this is to create a job staging table into our
database. Instead of sending jobs to the queue directly,
they’re sent to a staging table first, and an <strong><em>enqueuer</em></strong>
pulls them out in batches and puts them to the job queue.</p>

<pre><code>CREATE TABLE staged_jobs (
    id       BIGSERIAL PRIMARY KEY,
    job_name TEXT      NOT NULL,
    job_args JSONB     NOT NULL
);
</code></pre>

<p>The enqueuer selects jobs, enqueues them, and then removes
them from the staging table <a href="#footnote-2">2</a>. Here’s a rough
implementation:</p>

<pre><code>loop do
  DB.transaction do
    # pull jobs in large batches
    job_batch = StagedJobs.order('id').limit(1000)

    if job_batch.count &gt; 0
      # insert each one into the real job queue
      job_batch.each do |job|
        Sidekiq.enqueue(job.job_name, *job.job_args)
      end

      # and in the same transaction remove these records
      StagedJobs.where('id &lt;= ?', job_batch.last).delete
    end
  end
end
</code></pre>

<p>Because jobs are inserted into the staging table from
within a transaction, its <em>isolation</em> property (ACID’s “I”)
guarantees that they’re not visible to any other
transaction until after the inserting transaction commits.
A staged job that’s rolled back is never seen by the
enqueuer, and doesn’t make it to the job queue.</p>

<p>I call this pattern a <a href="/job-drain"><em>transactionally-staged job
drain</em></a>.</p>

<p>It’s also possible to just put the job queue directly in
the database itself with a library like <a href="https://github.com/chanks/que">Que</a>, but <a href="/postgres-queues">because
bloat can be potentially dangerous in systems like
Postgres</a>, this probably isn’t as good of an idea.</p>

<a href="#non-idempotent-requests">Non-idempotent requests</a>

<p>What we’ve covered here works nicely for HTTP requests that
are idempotent. That’s probably a healthy majority given a
well-designed API, but there are always going to be some
endpoints that are not idempotent. Examples include calling
out to an external payment gateway with a credit card,
requesting a server to be provisioned, or anything else
that needs to make a synchronous network request.</p>

<p>For these types of requests we’re going to need to build
something a little more sophisticated, but just like in
this simpler case, our database has us covered. In part two
of this series we’ll look at how to implement <a href="https://stripe.com/blog/idempotency">idempotency
keys</a> on top of multi-stage transactions.</p>


<div>
  <p><a href="#footnote-1-source">1</a> Note that for the purposes of this simple example we
could probably make this SQL more succinct, but for good
hygiene, we use length check, <code>NOT NULL</code>, and foreign key
constraints on our fields even if it’s a little more noisy
visually.</p>

<p><a href="#footnote-2-source">2</a> Recall that like many job queues, the “enqueuer” system
shown guarantees “at least once” rather than “exactly once”
semantics, so the job themselves must be idempotent.</p>

</div>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://stratechery.com/2017/everything-is-changing-so-should-antitrust/">Everything is Changing; So Should Antitrust</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="http://stratechery.com/feed/">Stratechery</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">business</span>
              <span class="tag">economics</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Wed Sep 06 2017 15:23:46 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>Late last month WPP, the largest advertising group in the world, announced <a href="https://www.nytimes.com/2017/08/23/business/media/advertising-giant-wpp-lowers-forecast-as-clients-cut-back.html?mcubz=0&amp;_r=0">results and forecasts that were sharply down</a>. Those results, though, were not what I found striking about CEO Martin Sorrell’s <a href="https://finance.yahoo.com/news/edited-transcript-wpp-l-earnings-235637417.html">remarks on the group’s earnings call</a>; after all, <a href="https://stratechery.com/2016/tv-advertisings-surprising-strength-and-inevitable-fall/">I argued last summer</a> that such a decline was inevitable.</p>
<p>Rather, it was striking just how feeble Sorrell’s proposed response was:</p>
<blockquote><p>
  So what’s our response to all this? Well, further focus on our 4 strategic priorities. Horizontality, which we moved up from, I think it was #4 a year or so ago to #1, is our first critical priority. And it really means ensuring that our people work seamlessly. They’re accustomed to working vertically and by agency brand, but they work seamlessly horizontally across the group together through client teams, I’ll come on to those, and country managers and subregional managers to provide an integrated benefit for clients. And clients are pressurizing us for more effectiveness and more efficiency, this is the way, probably the most significant way that we can respond.
</p></blockquote>
<p>Make no mistake, I’m <a href="https://stratechery.com/2013/the-uncanny-valley-of-a-functional-organization/">a student of organizational structure</a> and the <a href="https://stratechery.com/2016/apples-organizational-crossroads/">importance of aligning an organization</a> to the challenge at hand. Moreover, WPP’s conglomerate nature make any sort of <a href="https://www.wsj.com/amp/articles/ad-giants-are-under-pressure-to-streamline-complex-structures-1503943756">cross-agency collaboration challenging</a>; that, though gets at the real problem. If WPP must change the way it works internally, that by definition means the environment in which it is operating is fundamentally different than the one that existed while WPP grew into the organization it is today.</p>
<h4>Ad Agencies and the Internet</h4>
<p>I wrote about ad agencies earlier this year in the context of <a href="https://stratechery.com/2017/ad-agencies-and-accountability/">complaints about ads appearing next to objectionable content</a>:</p>
<blockquote><p>
  Way back in 1841, Volney B. Palmer, the first ad agency, was opened in Philadelphia. In place of having to take out ads with multiple newspapers, an advertiser could deal directly with the ad agency, vastly simplifying the process of taking out ads. The ad agency, meanwhile, could leverage its relationships with all of those newspapers by serving multiple clients:</p>
<p>  <a href="https://stratechery.com/2017/ad-agencies-and-accountability/"></a></p>
<p>  It’s a classic example of how being in the middle can be a really great business opportunity, and the utility of ad agencies only increased as more advertising formats like radio and TV became available. Particularly in the case of TV, advertisers not only needed to place ads, but also needed a lot more help in making ads; ad agencies invested in ad-making expertise because they could scale said expertise across multiple clients.
</p></blockquote>
<p>Over the past few weeks (<a href="https://stratechery.com/2017/disney-follow-up-the-re-bundlers-medium-claps/">here</a> and <a href="https://stratechery.com/2017/uber-ceo-follow-up-buzzfeed-adds-banner-ads-google-offers-cheaper-networking/">here</a>) I have been revisiting Clayton Christensen’s Law of Conservation of Attractive Profits, a theory I first explored in <a href="https://stratechery.com/2015/netflix-and-the-conservation-of-attractive-profits/">this 2015 article about Netflix</a>:</p>
<blockquote><p>
  Breaking up a formerly integrated system — commoditizing and modularizing it — destroys incumbent value while simultaneously allowing a new entrant to integrate a different part of the value chain and thus capture new value.</p>
<a href="https://stratechery.com/2015/netflix-and-the-conservation-of-attractive-profits/"></a>Commoditizing an incumbent’s integration allows a new entrant to create new integrations — and profit — elsewhere in the value chain.</blockquote>
<p>This starts to get at what is happening to WPP: the very idea of an ad agency arose from the opportunity to integrate the creation and placement of ads across disparate outlets, creating a one-stop shop for advertisers.</p>
<p>Those outlets, though, were only ever a proxy; advertisers don’t run ads for the sake of running ads, but rather to reach consumers. And on the Internet, where distribution is free and content abundant, more and more consumers found themselves relying on two services that focused on discovery and personalization: Google and Facebook. From that piece on ad agencies:</p>
<blockquote><p>
  There are really only two options for the sort of digital advertising that reaches every person an advertiser might want to reach:</p>
<p>  <a href="https://stratechery.com/2017/ad-agencies-and-accountability/"></a></p>
<p>  That’s a problem for the ad agencies: when there are only two places an advertiser might want to buy ads, the fees paid to agencies to abstract complexity becomes a lot harder to justify.
</p></blockquote>
<p>In fact, the issue is not even really about the money, but theory. Google and Facebook are the new integration points in the advertising value chain; it follows, then, that the rest of the value chain will modularize itself and re-organize around the integrated players. Or, as Sorrell put it, “horizontality.”</p>
<p>The problem for Sorrell is two fold: first, fundamentally re-orienting a business away from a vertical integrative approach to a horizontal modular approach is extremely difficult, both in terms of company culture and the effect on the bottom line. In truth I expect WPP to continue to lose business to digital agencies structured from day one with the assumption that Google and Facebook are the integrators in the advertising value chain.</p>
<p>The second point, though, is worse: what is happening to WPP is happening to the rest of WPP’s ecosystem — media on one side, and advertisers on the other.</p>
<h4>The Reorganization of Everything</h4>
<p>There used to be a limited number of media outlets — newspapers, radio, and television, primarily — all of which had substantial barriers to entry. That meant these outlets had a monopoly on reaching customers, leaving advertisers no choice but to pay up.</p>
<p>Now, though, there is an effectively unlimited amount of media: countless web pages, streaming music and podcasts, and services like Netflix and YouTube that, unbounded from the constraints of linearity, offer far more content than was ever accessible before. That, as noted above, meant that discovery mattered most, which meant Google and Facebook.</p>
<p>The parallel should be obvious: the clearest manifestation of how <a href="https://stratechery.com/2015/popping-the-publishing-bubble/">the media value chain has been fundamentally reconfigured</a> is the fact that advertising has fled newspapers in particular; in other words, the media story is an advertising story, which is to say that given the upheaval in the media industry, the most surprising part of WPP’s struggles is that it took this long to manifest (thanks, primarily, to television’s resilience).</p>
<p>The exact same value chain reorganization is happening to WPP’s clients: major advertisers like consumer packaged goods companies which built their businesses on integrating the creation and distribution of consumer staples. From <a href="https://stratechery.com/2016/dollar-shave-club-and-the-disruption-of-everything/">Dollar Shave Club and the Disruption of Everything</a>:</p>
<blockquote><p>
  P&amp;G leveraged these resources in a simple formula that led to repeated success:</p>
<ul>
<li>Spend significant resources on developing new products (more blades!) that can command a price premium</li>
<li>Spend even more resources on advertising the new product (mostly on TV) to create consumer awareness and demand</li>
<li>Spend yet more resources to ensure the new product is front-and-center in retail locations everywhere</li>
</ul>
<p>  In a world of scarcity this approach paid off time and again: P&amp;G grew not only because its markets grew, but also because it continually justified price increases due to its innovations.
</p></blockquote>
<p>The most obvious change has been the rise of Amazon: instead of limited shelf space, the selection is orders of magnitude greater than any bricks-and-mortar store, and integrated with a scaled fulfillment operation. That new integration means that suppliers and merchants have no choice but to modularize and build their businesses around Amazon.</p>
<p>Of course that isn’t the only option: new, smaller companies, like the aforementioned Dollar Shaving Club, can leverage the big platform providers — YouTube, Facebook, AWS, etc. — to compete with massive companies like P&amp;G on far more equal terms than before.</p>
<h4>A New Reality</h4>
<p>For long time Stratechery readers this analysis isn’t that novel; the shift in value chains that result from the Internet enabling zero distribution and zero transactional costs are the foundation of <a href="https://stratechery.com/2015/aggregation-theory/">Aggregation Theory</a>. It certainly is gratifying, in a way, to see the theory play out in what has long been the part of the value chain most resistant to upheaval (TV advertising and TV advertisers).</p>
<p>There is another context, though: the increasing appreciation outside of technology of just how dominant companies like Google, Facebook, Amazon, and even Netflix have become, and more and more discussion about whether antitrust is the answer. The problem is that much of this discussion is rooted in the old value chain, where power came from controlling distribution.</p>
<p>What is critical to understand is that that world is fading away; the fundamental nature of the Internet is abundance, and the critical competency is discovery. Moreover, the platform that harnesses discovery also harnesses a virtuous cycle between users and suppliers that leads to a winner-take-all situation inherent in two-sided networks. In other words, to the extent these platforms are monopolies, said monopoly is much more akin to AT&amp;T than it is to Standard Oil.</p>
<p>This matters for three reasons:</p>
<ul>
<li>First, the fact that newspapers, for example, or perhaps one day WPP, are being driven out of business is not a reason for antitrust action; their problem is <a href="https://stratechery.com/2017/the-local-news-business-model/">their business model</a> is obsolete. The world has changed, and invoking regulation to try to change that reality is a terrible idea.</li>
<li>Second, the consumer-friendly approach of these platform companies is no accident: when market power comes from owning demand, then the way to gain power is to create a great experience for consumers. The casual way in which many antitrust crusaders ignore the fact that, for example, Amazon is genuinely beloved by consumers — and for good reason! — is frustrating intellectually and eye-rolling politically.</li>
<li>Third, the presence of these platforms creates incredible new opportunities for businesses that were never before possible. I already described how Dollar Shaving Club was enabled by platform companies; Amazon has also enabled a multitude of merchants, Facebook an entire ecosystem of apps and personalized startups, and Google every possible service under the sun. </li>
</ul>
<p>In a 30-second commercial, of the sort that WPP might have made, drawing clear villains and easy narratives is valuable; the reality of aggregators is far more complicated. That Google, Facebook, Amazon, and other platforms are as powerful as they are is not due to their having acted illegally but rather to the fundamental nature of the Internet and the way it has reorganized value chains in industry after industry.</p>
<p>Moreover, these platforms have far more positive outcomes than distribution-based monopolies ever did: the consumer experience is better, and there are huge new opportunities to build new businesses (especially serving niches completely ignored in a distribution-based world) on top of them. That is a good thing, worth preserving.</p>
<p>To that end any antitrust regulation, if it comes, needs a fresh approach rooted in the reality of the Internet. I agree that too much concentrated power has inherent problems; I also believe a structural incentive to provide a great customer experience, along with the potential to create completely new kinds of businesses, is worth preserving. Antitrust crusaders, to whom <a href="https://stratechery.com/2017/facebook-and-the-cost-of-monopoly/">I am clearly sympathetic</a>, ignore these realities at their political peril.</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://ronjeffries.com/articles/017-08ff/ramps/">Sense of Urgency</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="http://ronjeffries.com/feed.xml">Ron Jeffries</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">practices</span>
              <span class="tag">agile</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Wed Sep 06 2017 05:00:00 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            Mike (GeePaw) Hill storified about sense of urgency and an acronym RAMPS. I like what he said and will muse upon it.
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://bradfrost.com/blog/link/documenting-the-pittsburgh-potty-an-architectural-mystery-in-our-basements/">Documenting The Pittsburgh Potty: An Architectural Mystery In Our Basements</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="http://feeds.feedburner.com/brad-frosts-blog">Brad Frost</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">design</span>
              <span class="tag">web</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Wed Sep 06 2017 03:58:58 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>I have a toilet in my basement. Not a bathroom, just a toilet sitting in my corner.</p>
<p>Lots of other Pittsburghers have Pittsburgh potties as well. In the steel industry’s heyday, workers would come home filthy from the mills, wash (worsh) up in the basement, then come upstairs for supper.</p>
<blockquote><p>People say that mill workers came home, they were super dirty, they didn’t want to dirty the house, so they went in through the basement, showered in the basement and did their bathing in the basement and then came upstairs where the house was clean</p></blockquote>
<p>This article proffers another explanation, that basements were servants’ quarters. While that’s no doubt true for some of the more gigantic houses in Pittsburgh, it can’t be true for all of them. My grandpa, who’s in his late 80s, still showers in the cinderblock shower stall in the small house where my dad was born and raised.</p>
<p> </p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://jvns.ca/blog/2017/09/05/finding-out-where-packets-are-being-dropped/">Finding out if&#x2F;why a server is dropping packets</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://jvns.ca/atom.xml">Julia Evans</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">practices</span>
              <span class="tag">linux</span>
              <span class="tag">networking</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Wed Sep 06 2017 00:39:28 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>When packets are being dropped on a computer, they’re being dropped for
a <em>reason</em>. How do you find out whether/why packets are being dropped?</p>

<p>Here’s the situations we want to understand:</p>

<ol>
<li>a packet enters the network stack of your computer (<code>RX</code>) (say on
port 8000). It gets dropped before the application listening on port
8000 receives it.</li>
<li>you send a packet (<code>TX</code>). Before it makes it out of your computer, it
gets dropped.</li>
</ol>

<p>I’m not interested here in “packets are being dropped somewhere else on the
internet, let’s diagnose it with traceroute / by counting TCP retransmits”
(though that’s important too!)</p>

<h3>how do you even know if packets are being dropped?</h3>

<p>I asked on Twitter and got the very useful answer “look at <code>netstat -i</code>!”
Here’s what that looks like on my laptop:</p>

<pre><code>bork@kiwi~&gt; sudo netstat -i
Kernel Interface table
Iface       MTU Met   RX-OK RX-ERR RX-DRP RX-OVR    TX-OK TX-ERR TX-DRP TX-OVR Flg
docker0    1500 0         0      0      0 0             0      0      0      0 BMU
enp0s25    1500 0   1235101      0    242 0        745760      0      0      0 BMRU
lo        65536 0     21558      0      0 0         21558      0      0      0 LRU
nlmon0     3776 0    551262      0      0 0             0      0      0      0 ORU
</code></pre>

<p>It looks like there are some received (<code>RX</code>) packets being lost on <code>enp0s25</code>
(my wireless card). No <code>TX</code> packets lost though!</p>

<p>Someone also told me that running <code>ethtool -S</code> is useful but my ethtool doesn’t have a <code>-S</code> option.</p>

<h3>how do you know <strong>why</strong> packets are being dropped</h3>

<p>I was googling and I found this cool tool called <code>dropwatch</code>. There isn’t any Ubuntu package for it but it’s on github: <a href="https://github.com/pavel-odintsov/drop_watch">https://github.com/pavel-odintsov/drop_watch</a>.</p>

<p>Here are the instructions that worked for me to compile it:</p>

<pre><code>sudo apt-get install -y libnl-3-dev libnl-genl-3-dev binutils-dev libreadline6-dev
git clone https://github.com/pavel-odintsov/drop_watch
cd drop_watch/src
vim Makefile # comment out the -Werror argument to gcc
make
</code></pre>

<p>And here’s the output! It tells me at which kernel function I’m losing packets. Cool!</p>

<pre><code>sudo ./dropwatch -l kas
Initalizing kallsyms db
dropwatch&gt; start
Enabling monitoring...
Kernel monitoring activated.
Issue Ctrl-C to stop monitoring

1 drops at tcp_v4_do_rcv+cd (0xffffffff81799bad)
10 drops at tcp_v4_rcv+80 (0xffffffff8179a620)
1 drops at sk_stream_kill_queues+57 (0xffffffff81729ca7)
4 drops at unix_release_sock+20e (0xffffffff817dc94e)
1 drops at igmp_rcv+e1 (0xffffffff817b4c41)
1 drops at igmp_rcv+e1 (0xffffffff817b4c41)
</code></pre>

<h3>monitoring dropped packets with perf</h3>

<p>Here’s another cool way to debug what’s happening!</p>

<p><a href="https://twitter.com/tgraf__">thomas graf</a> told me that you can monitor the
<code>kfree_skb</code> event using perf, and that will tell you when packets are being
dropped (where in the kernel stack it happened):</p>

<pre><code>sudo perf record -g -a -e skb:kfree_skb
sudo perf script
</code></pre>

<h3>advanced reading</h3>

<p>There’s also these two cool articles:</p>

<ul>
<li><a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/">Monitoring and Tuning the Linux Networking Stack: Receiving Data</a></li>
<li><a href="https://blog.packagecloud.io/eng/2017/02/06/monitoring-tuning-linux-networking-stack-sending-data/">Monitoring and Tuning the Linux Networking Stack: Sending Data</a></li>
</ul>

<p>I still haven’t read them in full but they are extremely detailed and cool.</p>

<h3>tell me if you know more!</h3>

<p>I still haven’t ever used these tools to seriously debug a packet loss problem
yet, just wanted to write this down so that I have the notes if I do want to in
the future!</p>

<p>If you have better tips for debugging whether/why packets are being dropped on
a computer, let me know!</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://bradfrost.com/blog/link/building-a-design-system-for-healthcare-gov/">Building a design system for HealthCare.gov</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="http://feeds.feedburner.com/brad-frosts-blog">Brad Frost</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">design</span>
              <span class="tag">web</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Tue Sep 05 2017 13:53:49 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>This is a great post that lays out the nuts and bolts of the design system used to build Healthcare.gov. It details a lot of architecture and tooling, and I’m super happy to hear they took inspiration from my post about <a href="http://bradfrost.com/blog/post/css-architecture-for-design-systems/">css architecture for design systems</a>.</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://belkadan.com/blog/2017/09/Over-abstraction/">Over-abstraction</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="http://belkadan.com/blog/atom">-dealloc</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">practices</span>
              <span class="tag">tools</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Tue Sep 05 2017 08:00:00 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>Back in July I got myself into a <a href="https://twitter.com/UINT_MIN/status/887348345831698432">discussion on Twitter</a> about whether some of the more algebraic concepts in functional programming were net-useful, after reading <a href="http://www.fewbutripe.com">Brandon Williams</a>’ (great) articles on how they can be applied in Swift. <a href="http://hkr.me">Brandon Kase</a> suggested I watch his talk “Beyond Types in Swift” from this year’s <a href="http://2017.funswiftconf.com">Functional Swift</a> conference.</p>

<p>I admit I’m still unconvinced. I also admit that I might still just not get it. But I had two interesting thoughts that I wanted to write out in longform. I thought about cramming them both into one article, but figured it’d be better for discussion purposes to just do one at a time. So this is an exploration of the first idea, “Over-abstraction”. The next one will come in about a week or so.</p>

<p>(I’ve already loaded the discussion just from the title, but hey, it’s my blog.)</p>

<hr />

<p>I think this is my primary problem with using algebraic abstractions for programming. It’s <em>true</em> to say that functions and Optionals are both <a href="http://hackage.haskell.org/package/base-4.10.0.0/docs/Data-Monoid.html">Monoids</a>, but is it useful? The criterion for this in the Swift standard library is usually “can you define any meaningful reusable generic operations on this protocol?”.</p>

<p>I’m not going to directly talk about whether Monoid itself has useful generic operations; it’s just an example. But it leads to people writing code like this:</p>

<div><pre><code>let fullArticle: View&lt;Article, [Node]&gt; =
  articleHeader
    &lt;&gt; articleBody
    &lt;&gt; articleFooter
</code></pre></div>

<p>(from Williams’ “<a href="http://www.fewbutripe.com/swift/html/dsl/2017/06/29/composable-html-views-in-swift.html">Composable HTML Views in Swift</a>”)</p>

<p>The trouble is I (hyperbolically) have no idea what that means. Because <code>&lt;&gt;</code> (or <code>mappend</code> in Haskell) is a high-level operation, I know that it has certain properties I can rely on, like <a href="https://simple.wikipedia.org/wiki/Associativity">associativity</a>. But I don’t know what that means for a <code>View</code>. I can <em>guess,</em> but the generic name is in the domain of the abstraction rather than the concrete type.</p>

<hr />

<p>Let’s take an example from human language. Imagine a friend has a problem with their bird feeder: the seeds keep getting eaten before the birds can get there. “What’s been eating them?”, you ask. Now, it would be <em>valid</em> for your friend to say “mammals keep eating the seeds”, but you’d probably give them a funny look. “Cats and squirrels” would go over much better. If the abstraction level is too far removed from the domain, things get <em>harder</em> to understand.</p>

<p>(This also works in reverse. “Siamese cats, Maine coons, and Western gray squirrels” would be considered an over-specific answer. <em>Some</em> abstraction is usually a good thing.)</p>

<p>To stretch the example even further, let’s say you wanted to ask if the cats and squirrels could reach the bird feeder directly from the ground with their legs, or if they had to climb something to get to the delicious birdseed. But not all mammals have legs, so instead you have to ask whether their <em>appendages</em> are long enough. You’ve used the correct general term, but now it’s harder to understand.</p>

<p>I’ve tortured my poor metaphor enough, but hopefully you see what I’m getting at.</p>

<hr />

<p>Note that we already have some of this problem with just the Swift standard library. A <a href="https://developer.apple.com/documentation/swift/set">Set</a> is a <a href="https://developer.apple.com/documentation/swift/sequence">Sequence</a>, which means you can call <code>dropFirst</code> on it. What’s the first element of an unordered set? What’s the “rest”? And yet you <em>can</em> use this to build useful reusable operations on Sequence, and then it’s useful to be able to apply those operations to a Set.</p>

<p>So, my thesis: Abstractions are good because they let you reuse both your intuitions and your code across different types…but they can also hurt comprehension when working with a type concretely. That leads to having to choose between sacrificing the domain-specific name in favor of the generic name, and having two names for the same operation.</p>

<p>(Also, it doesn’t help that the names chosen by mathematicians don’t feel very approachable.)</p>

<hr />

<p>I <em>do</em> want to highlight some of the important criteria for inclusion of a new protocol in the Swift standard library:</p>

<ul>
  <li>
    <p>Are there generic operations that work with this protocol? That is, will there be protocol extension methods, or somewhere that uses this protocol as a generic constraint?</p>
  </li>
  <li>
    <p>Is there more than one concrete type that would conform to this protocol?</p>
  </li>
  <li>
    <p>If this protocol is part of a hierarchy, are there types that conform to the parent protocol and not this one? If not, it’s probably not worth making a separate protocol for it.</p>
  </li>
  <li>
    <p>Does the operation have any semantic requirements? (This is what keeps String’s <code>+</code> and Int’s <code>+</code> from being in a protocol.)</p>
  </li>
</ul>

<p>These criteria gave us the distinctions between <a href="https://developer.apple.com/documentation/swift/sequence">Sequence</a>, <a href="https://developer.apple.com/documentation/swift/collection">Collection</a>, <a href="https://developer.apple.com/documentation/swift/bidirectionalcollection">BidirectionalCollection</a>, and <a href="https://developer.apple.com/documentation/swift/randomaccesscollection">RandomAccessCollection</a>, but not between <a href="https://developer.apple.com/documentation/swift/numeric">Numeric</a> and a hypothetical “<a href="https://en.wikipedia.org/wiki/Semiring#Definition">Additive</a>” that doesn’t support subtraction. There just aren’t enough concrete types that only support addition <em>and</em> still make sense to write generic algorithms over for this to be in the standard library.</p>

<p>How the algebraic abstractions we’ve been talking about match up against these criteria is a matter of debate. And of course, just because something’s not in the standard library doesn’t mean it can’t be useful. But I think the loss of domain-specific terminology is a high price to pay in the common case; the benefit you get should be commensurate.</p>

<hr />

<p>P.S. One idea that came out of the original Twitter discussion was a distinction between abstractions as <em>protocols</em> (“type classes” in Haskell) and abstractions as <em><a href="https://twitter.com/dimsumthinking/status/887498387293167618">design patterns</a>.</em> It’s <em>useful</em> to know when an operation is associative, because that lets you perform folds (<code>reduce</code>) in parallel; this is a key part of the “<a href="http://www.fewbutripe.com/swift/math/algebra/2015/02/17/algebraic-structure-and-protocols.html#semigroup">semigroup</a>” abstraction. And even when you can’t build meaningful generic operations on top of a particular abstraction, it may still be useful to other developers reading your code or using your types to get that shared intuition of how something behaves.</p>

<p>However, the difference between a design pattern and a language feature is that the compiler usually won’t check that you’re using a design pattern correctly, which means using actual protocols does have value. I’ll return to this idea in <a href="http://belkadan.com/blog/2017/09/The-New-Kingdom-of-Nouns/">the next (much longer) post</a>.</p>

<p>P.P.S. After I posted this, Brandon Williams made a <a href="https://twitter.com/mbrandonw/status/905458906121547778">bit of a response</a> in tweetstorm format, and Elviro Rocca made a <a href="https://broomburgo.github.io/fun-ios/post/on-abstraction/">blog post</a> a few days after. You can also read <a href="https://twitter.com/UINT_MIN/status/905452666771226624">other Twitter people’s responses</a>.</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://medium.com/@copyconstruct/monitoring-and-observability-8417d1952e1c?source=rss-87c8c84f24b1------2">Monitoring and Observability</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://medium.com/feed/@copyconstruct">Cindy Sridharan</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">practices</span>
              <span class="tag">devops</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Tue Sep 05 2017 05:52:44 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>During lunch with a few friends in late July, the topic of observability came up. I have a talk coming up at <a href="https://conferences.oreilly.com/velocity/vl-ny">Velocity</a> in less than a month called <strong><em>Monitoring in the time of Cloud Native</em></strong>, so I’ve been speaking with friends about how they approach monitoring where they work. During this conversation, one of my friends mentioned:</p><blockquote><p>OH - &quot;Observability - because devs don't like to do &quot;monitoring&quot; we need to package it in new nomenclature to make it palatable and trendy.</p><p> — <a href="https://twitter.com/copyconstruct/status/891066003844546560">@copyconstruct</a></p></blockquote><p>He was only half joking. I’ve heard several variations of this zinger, some of them being:</p><blockquote>— Why call it monitoring? That’s not sexy enough anymore.</blockquote><blockquote>— Observability, because rebranding Ops as DevOps wasn’t bad enough, now they’re devopsifying monitoring too</blockquote><blockquote>— I’m an engineer that can help provide monitoring to the other engineers in the organization<br />&gt; Great, here’s $80k.<br />I’m an architect that can help provide observability for cloud-native, container-based applications<br />&gt; Awesome! Here’s $300k!</blockquote><blockquote>— Is that supposed to be like the second coming of DevOps? Or was it the Second Way? I can’t remember. It all felt so cultish anyway.</blockquote><p>So then. What is the difference between “monitoring” and “observability”, if any? Or is the latter just the latest buzzword on the block, to be flogged and shoved down our throats until it has been milked for all its worth?</p><h4>Once upon a time there was “Monitoring”</h4><p>“Monitoring” traditionally was an Ops preserve. The term often invokes not very pleasant memories in minds of many who’ve been doing it for long enough they can remember the time when Nagios was state-of-the-art. In the eyes of many, “monitoring” harks back to many dysfunctional aspects of the old school way of operating software, not least the unsophistication of tooling available back in the day that ruled the roost so consummately that the term “monitoring” to this day causes some people to think of simple up/down checks.</p><p>While it’s true that a decade ago, up/down checks might’ve been all a “monitoring” tool would have been capable of, in the recent years “monitoring” tools have evolved <em>greatly</em>, to the point where many, many, many people no longer think of “monitoring” as just external pings. While they might still call it “monitoring”, the methods and tools they use are more powerful and streamlined. Time series, logs and traces are all more in vogue than ever these days and are forms of “<em>whitebox </em>monitoring”, which refers to a category of “monitoring” based on the information derived from the internals of systems.</p><blockquote><p><a href="http://twitter.com/rakyll" target="_blank">@rakyll</a> <a href="http://twitter.com/copyconstruct" target="_blank">@copyconstruct</a> Though I personally also use the term &quot;monitoring&quot; to be more broad, including time series collection, alerting, possibly even logs + traces</p><p> — <a href="https://twitter.com/juliusvolz/status/891348711795830784">@juliusvolz</a></p></blockquote><p>Whitebox monitoring isn’t really a revolutionary idea anymore, at least not since Etsy published the <a href="https://codeascraft.com/2011/02/15/measure-anything-measure-everything/">seminal blog post</a> introducing statsd. The blog post goes on to state that:</p><blockquote>In general, we tend to measure at three levels: network, machine, and application. Application metrics are usually the hardest, yet most important, of the three. They’re very specific to your business, and they change as your applications change (and Etsy changes a lot). Instead of trying to plan out everything we wanted to measure and putting it in a classical <a href="http://www.opscode.com/chef/">configuration management system</a>, we decided to make it ridiculously simple for any engineer to get anything they can count or time into a graph with almost no effort.</blockquote><p>For all its flaws, statsd was a <em>game changer</em>, its popularity and ubiquity being a testament to how it struck the right chord with huge swathes of the industry, so much so that most open source time series based systems as well as vendors have supported statsd style metrics for years now. While not perfect, statsd style metrics collection was a huge improvement over the way one did “monitoring” previously.</p><h4>Baby’s first Observability</h4><p>“Observability”, on the other hand, was a term I first encountered while reading a <a href="https://blog.twitter.com/engineering/en_us/a/2013/observability-at-twitter.html">post</a> on Twitter’s tech blog a few years ago and have been hearing the term ever since, not in real life or at the places where I’ve worked but at tech conferences. Twitter has since published a <a href="https://blog.twitter.com/engineering/en_us/a/2016/observability-at-twitter-technical-overview-part-i.html">two</a> <a href="https://blog.twitter.com/engineering/en_us/a/2016/observability-at-twitter-technical-overview-part-ii.html">part</a> blog post on its current Observability stack. The posts are more about the architecture of the different components than the term itself, but the first post begins by stating that:</p><blockquote>These are the four pillars of the Observability Engineering team’s charter:</blockquote><blockquote>- Monitoring<br />- Alerting/visualization<br />- Distributed systems tracing infrastructure<br />- Log aggregation/analytics</blockquote><p>“Observability”, according to this definition, is a <em>superset</em> of “monitoring”, providing certain benefits and insights that “monitoring” tools come a cropper at. Before examining what these gains might be and when they are even needed, let’s first understand what “monitoring” really is, what its shortcomings are and why “monitoring” alone isn’t sufficient for certain<em> </em>use cases.</p><h4>Monitoring is for symptom based Alerting</h4><p>The <a href="https://landing.google.com/sre/book/index.html">SRE book</a> states:</p><blockquote>Your monitoring system should address two questions: what’s broken, and why? The “what’s broken” indicates the symptom; the “why” indicates a (possibly intermediate) cause.“What” versus “why” is one of the most important distinctions in writing good monitoring with maximum signal and minimum noise.</blockquote><p>Blackbox monitoring — that is, monitoring a system from the outside by treating it as a blackbox — is something I find very good at answering the <em>what </em>and<em> </em>for alerting about a problem that’s already occurring (and ideally end user-impacting). Whitebox monitoring, on the other hand, is fantastic for the signals we can <em>anticipate </em>in advance and be on the lookout for. In other words, whitebox monitoring is for the <strong><em>known, hard </em></strong>failure modes of a system, the sort that lend themselves well toward exhibiting any deviation in a <em>dashboardable</em> manner, as it were.</p><p>A good example of something that needs “monitoring” would be a storage server running out of disk space or a proxy server running out of file descriptors. An I/O bound service has different failure modes compared to a memory bound one. An HA system has different failure moves compared to a CP system.</p><blockquote><p><a href="http://twitter.com/mipsytipsy" target="_blank">@mipsytipsy</a> <a href="http://twitter.com/lizthegrey" target="_blank">@lizthegrey</a> <a href="http://twitter.com/honeycombio" target="_blank">@honeycombio</a> So for example, I am oncall for a global key value store that has three alerts: error rate, query latency, and repl latency</p><p> — <a href="https://twitter.com/jaqx0r/status/898709693178105856">@jaqx0r</a></p></blockquote><p>Building “monitorable” systems requires being able to understand the failure domain of the critical components of the system <strong><em>proactively. </em></strong>And that’s a tall order. <em>Especially </em>for complex systems. More so for simple systems that interact <em>complexly</em> with one another.</p>One of my favorite kellabyte rants of all time<p>The more mature a system, the better understood are its failure modes. Battle hardened, <a href="http://mcfunley.com/choose-boring-technology">“boring” technology</a> often comes with <a href="https://medium.com/@Pinterest_Engineering/learn-to-stop-using-shiny-new-things-and-love-mysql-3e1613c2ce14">well-publicized, well-understood and <em>monitorable </em>failure modes</a>. Being able to entirely architect away a failure mode is the best thing one can do while building systems. The next best thing one can do is to be able to “monitor” impending failures and alert accordingly.</p><blockquote><p><a href="http://twitter.com/sarahmei" target="_blank">@sarahmei</a> <a href="http://twitter.com/mipsytipsy" target="_blank">@mipsytipsy</a> if goal is to build *monitorable* systems, then one can have predictable failure modes and be on the lookout for them. else all bets are off</p><p> — <a href="https://twitter.com/copyconstruct/status/902651610890813440">@copyconstruct</a></p></blockquote><p>As strange as it might sound, I’m beginning to think one of the design goals while building systems should be to make it as <em>monitorable </em>as possible — which means minimizing the number of unknown-unknowns. For monitoring to be <em>effective, </em>it becomes salient to be able to identify a small set of hard failure modes of a system or a core set of metrics that has the potential to offset one’s assumptions underpinning some of the system design choices. Some believe that the ideal number of signals to be “monitored” is anywhere between 3–5, and definitely no more than 7-10. One of the common pain points that keeps cropping up in my conversations with friends is how <em>noisy </em>their “monitoring” is. Or as one of my friends put it,</p><blockquote>We have a <strong>ton</strong><em> </em>of metrics, all right. We try to collect <strong><em>everything</em></strong><em> </em>but the vast majority of these metrics are never looked at. It leads to a case of severe metric fatigue to the point where some of our engineers now don’t see the point of adding new metrics to the mix, because why bother when only a handful are ever really used?</blockquote><p>This is something the SRE book warns against:</p><blockquote>The sources of potential complexity are never-ending. Like all software systems, monitoring can become so complex that it’s fragile, complicated to change, and a maintenance burden. Therefore, design your monitoring system with an eye toward simplicity. In choosing what to monitor, keep the following guidelines in mind:</blockquote><blockquote>- The rules that catch real incidents most often should be as <strong>simple, predictable, and reliable</strong> as possible.<br />- Data collection, aggregation, and alerting configuration that is rarely exercised (e.g., less than once a quarter for some SRE teams) should be up for removal.<br />- Signals that are collected, but not exposed in any prebaked dashboard nor used by any alert, are candidates for removal.</blockquote><p>The corollary of the aforementioned points is that <em>monitoring </em>data<em> </em>needs to <em>actionable. </em>What I’ve gathered talking to many people is that when not used to directly drive alerts, monitoring data should be optimized for providing a bird’s eye view of the <em>overall health</em> of a system. In the event of a failure, monitoring data should immediately be able to provide visibility into impact of the failure as well as the effect of any fix deployed. As one of my friends put it:</p><blockquote>It should be obvious, but the most important requirement of a system is that it should be <strong>up</strong><em>. </em>The second most important requirement of a system is that it should be <strong>healthy</strong>. Slow is defect and a bug, but it’s still preferable to being flat out <strong>down</strong><em>. </em>When a system is <strong>down</strong>, there’s nothing I want more than for it to be back <strong>up</strong> again. What I really want my monitoring system to tell me when I’m down is how widespread the impact is, and how any fix deployed — and many a time it’s a stopgap fix — is helping us get back to being as healthy as we can hope, given the circumstances.</blockquote><p>The crucial thing to understand here is that monitoring doesn’t guarantee that failure can be completely avoided. Monitoring provides a good <em>approximation </em>of the health of a system, but monitoring doesn’t <em>prevent</em> failure entirely.</p><p>I feel especially poignant writing this because just the previous week we had several hours of degraded performance where I work. It wasn’t a void in our monitoring that caused it. It was MySQL exhibiting one of its nastiest failure modes. Boring old MySQL which was chosen a few years ago with eyes wide open for its boringness and its maturity and our prior experience with it. Boring old MySQL which was the right choice when it was introduced into the stack, which isn’t the right choice anymore given our evolving needs. Like monitoring, boring technology, in and of itself, isn’t a panacea.</p><p>Monitoring can furnish one with a panoramic view of systems’ performance and behavior in the wild. Monitoring can also greatly assist in understanding the shortcomings and the evolving needs. Monitoring, as such, is an absolute <em>requirement</em> for building, operating and running systems. It, however, does not make our systems completely <em>impregnable</em> to failure, and that shouldn’t be its <em>goal </em>either.</p><h4>And then there’s “Observability”</h4><p>Quoting the SRE book again:</p><blockquote>It can be tempting to combine monitoring with other aspects of inspecting complex systems, such as detailed system profiling, single-process debugging, tracking details about exceptions or crashes, load testing, log collection and analysis, or traffic inspection. While most of these subjects share commonalities with basic monitoring, blending together too many results in overly complex and fragile systems.</blockquote><p>The SRE book doesn’t quite use the term “observability”, but clearly lays out everything that “monitoring” isn’t and shouldn’t aim to be. I find this to be an interesting paradox, that the “monitoring” of complex systems should itself be <em>simple</em>. And yet it makes perfect sense, since the alternative is empirically proving to be falling short.</p><p>The goals of “monitoring” and “observability” are different. “Observability” isn’t a substitute for “monitoring” nor does it obviate the need for “monitoring”; they are complementary. “Observability” might be a fancy new term on the horizon, but it really isn’t a novel idea. Events, tracing, exception tracking are all a derivative of logs, and if one has been using any of these tools, one already has some form of “observability”. True, new tools and new vendors will have their own definition and understanding of the term, but in essence “observability” captures what “monitoring” doesn’t (and ideally, <em>shouldn’t</em>).</p><p>“Monitoring” is best suited to report the overall health of systems. Aiming to “monitor everything” can prove to be an anti-pattern. Monitoring, as such, is best limited to key business and systems metrics derived from time-series based instrumentation, known failure modes as well as blackbox tests. “Observability”, on the other hand, aims to provide highly granular insights into the behavior of systems <strong><em>along with rich context</em></strong>, perfect for debugging purposes. Since it’s still not possible to predict every single failure mode a system could potentially run into or predict every possible way in which a system could <em>misbehave</em>, it becomes important that we build systems that can be <strong><em>debugged</em></strong> armed with evidence and not conjecture.</p><h4><strong>Debugging</strong></h4><p>Two of my favorite recent talks were <a href="https://www.slideshare.net/bcantrill/debugging-under-fire-keeping-your-head-when-systems-have-lost-their-mind"><strong><em>Debugging under fire: Keeping your head when systems have lost their mind</em></strong></a> and <a href="https://www.slideshare.net/bcantrill/zebras-all-the-way-down-the-engineering-challenges-of-the-data-path"><strong>Zebras all the way down: The engineering challenges of the data path</strong></a>, both by Bryan Cantrill. Since I possibly can’t say it better, I’m going to borrow a couple of slides from those talks here (the entire deck is definitely worth checking out).</p><p>Debugging is an <em>iterative</em> process, involving iterative introspection of the various observations and facts reported by the system, making the right deductions and testing whether the theory holds water. Evidence cannot be conjured out of thin air nor can it be extrapolated from aggregates, averages, percentiles, historic patterns or any other forms of data primarily collected for <em>monitoring </em>purposes. Evidence needs to be <em>reported</em> by the systems in the form of highly precise and contextual facts and observations, which can later be used while debugging to theorize as to why something might be not working as expected.</p><p>Furthermore, unlike “monitoring” which is <em>known</em> <em>failure centric</em>, “observability” doesn’t necessarily have to be closely tied to an outage or a user complaint. It can be used as a way to better understand system performance and behavior, even during the what can be perceived as “normal” operation of a system.</p><h4>Context Matters</h4><p>Another theme that came up during my recent conversations is how simply buying or setting up a tool doesn’t lead to everyone in the organization actually using it. As one of the people I spoke with noted in dismay:</p><blockquote>We have Zipkin! We went to great lengths to rope in developers to instrument everything and get Zipkin up and running, but the dev teams don’t use it very much. In retrospect, it wasn’t worth the effort.</blockquote><p>Tools can only help so much. Before buying or building a tool, it becomes important to evaluate the maximum utility it can provide for the unique set of engineering challenges specific teams face. Yet another one of my friends who works on an extremely popular open source project modeled along the lines of an internal tool at a big company said to me:</p><blockquote>When we open sourced [redacted] a few years ago, the questions in the mailing list were extremely intelligent and high quality. Now that [redacted] has gotten really popular, what we’re seeing is that the quality of questions is plummeting. We now have enterprises ask us if we can provide [redacted] as a SaaS service, since by their own admission their developers aren’t the hipster types who want to or will be able to run [redacted] themselves.</blockquote><p>Context is key. A tool that worked swimmingly well at company X won’t necessarily work just as well at company Y. This is <em>especially </em>true when it comes to bringing big company tooling to the masses. The organization structure and culture, quality of developers and Operations engineers, tooling already in use, state of the codebase, appetite for risk all play a huge part into how successful a tool will prove to be if introduced.</p><h4>One last thing</h4><p>Observations can lead a developer to the answers, it can’t make them necessarily find it. The process of examining the evidence (observations) at hand and being able to deduce still requires a good understanding of the system, the domain as well as a good sense of intuition. No amount of “observability” or “monitoring” tooling can ever be a substitute to good engineering intuition and instincts.</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://feedproxy.google.com/~r/TroyHunt/~3/AVLepeXMlBo/">How I Finally Fixed My Parents Dodgy Wifi With AmpliFi</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://feeds.feedburner.com/TroyHunt">Troy Hunt</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">security</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Mon Sep 04 2017 09:13:41 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p><a href="http://www.symantec.com/ready-secure-go/it-smb/?om_ext_cid=ws_ad_TROY_DBC-Bulldog_IT-SMB_Encrypt"><strong>Presently sponsored by:</strong> Get a security solution that will keep your website up and running—and keep you sleeping soundly: Symantec Website Security. Learn how</a></p><div><p>I have no idea who it was that first modified <a href="https://en.wikipedia.org/wiki/Maslow%27s_hierarchy_of_needs">Maslow's hierarchy of needs</a> in this fashion, but I do know that's it's never been truer than now:</p>
<p></p>
<p>More wifi things used by more people in more corners of the house. Many people do now consider connectivity to be a pretty essential need in their life yet as our wifi demands have increased, many people are still attempting to make do with the networking gear of yesteryear. I was one of those people, at least I was until late last year when I finally lost my mind and <a href="https://www.troyhunt.com/ubiquiti-all-the-things-how-i-finally-fixed-my-dodgy-wifi/">installed Ubiquiti's UniFi devices all through the house</a>. Ubiquiti is actually the world's largest producer of wireless access points and their UniFi range is a suite of products that all integrate together to provide this really neat centralised management experience.</p>
<p>I moved on and <a href="https://www.troyhunt.com/how-i-finally-fixed-the-dodgy-wifi-on-my-jet-ski-with-ubiquitis-unifi-mesh/">extended the network out to my jet ski with their Mesh products</a>, <a href="https://www.troyhunt.com/wiring-a-home-network-from-the-ground-up-with-ubiquiti/">did a ground-up build in my brother's house</a> (which I remain jealous of) and just last month, <a href="https://www.troyhunt.com/heres-what-this-ubiquiti-unifi-stuff-is-all-about/">released a free course on UniFi commissioned by Ubiquiti</a>. Clearly, I'm a UniFi convert.</p>
<p>But UniFi isn't for everyone. It's a &quot;prosumer&quot; product which means it's great for everyone from technical people installing it in their homes through to professionals building out entire shopping centres or stadiums with the gear. But it's not great for non-techies; there's both design and setup involved and frankly, a heap of features they'll never need. That's where <a href="https://www.amplifi.com/">AmpliFi</a> comes in, Ubiquiti's consumer line for the home. When I wrote the aforementioned course, Ubiquiti sent me out all the UniFi components which went into that build but they also chucked in an AmpliFi box:</p>
<p></p>
<p>Now frankly, I have no need for more wifi; I have enough access points running in my house at present to easily kit out a place 5 times the size. But my parents, well, their wifi wasn't so hot and that seemed like a good opportunity to play with tech bits :) Besides, I've been asked by enough people what I thought of AmpliFi and if they should get that or UniFi so I thought I'd write up the install process then comment on which way I think you should go under what circumstances.</p>
<p>Firstly, here's what's in the box:</p>
<p></p>
<p>What you're looking at here is a base station in the middle accompanied by 2 wireless mesh points. This is the same basic premise as having multiple UniFi access points - you need more connectivity these days than what a single central unit provides so you augment it with remote APs. The base station has the circular interface you see above as well as a bunch of ethernet jacks on the rear:</p>
<p></p>
<p>All three of the units have wifi access points and you place them around the house in order to provide connectivity where required. Like the UniFi gear, it all forms a &quot;mesh&quot; of wifi that devices seamlessly roam between as you move around the house. More so than the UniFi gear, this should be dead simple for pretty much anyone to setup so that's what I want to do - go through the entire process in an entirely non-techie fashion and see how accessible the gear really is to mere mortals.</p>
<p>First impression after taking bits out of the box is that everything feels very Apple-like in terms of packaging and quality. Whereas the UniFi gear is very utilitarian in its packaging (neat, functional but not showy), this is all plastic protective wrap on polished surfaces and a box opening ceremony that's clearly designed to show of the product.</p>
<p>Doing my best &quot;I'm just an average everyday consumer&quot; impression, I start following the instructions (this is hard for me!) which means kicking off with <a href="https://itunes.apple.com/au/app/amplifi-wifi/id1115225220?mt=8">the AmpliFi app on my iPhone</a>. While that's loading, I kill power to the old unit mum and dad have been running for probably the last decade and power up the AmpliFi base station:</p>
<p></p>
<p>Well, <em>eventually</em> I power it up because it took a while to figure out where to put it such that the very short USB Type-C power cable would reach. That's going to be one of the few things I'll fault in this writeup and it's a minor (albeit unnecessary) gripe. I uplink it to the old modem via the WAN port on the back (it has the little globe and blue surrounds on it) because as crappy as that old device is, it's still how internet gets into the house via the built-in ADSL modem. Keep that in mind folks - you still need inbound internet somewhere upstream of the AmpliFi kit. Power up the old modem again, open the app and we're ready to go:</p>
<p></p>
<p>Now about here I should mention 2 things:</p>
<ol>
<li>I don't think I've ever setup a wireless access point without a PC before. It's just always &quot;what you did&quot;, but not this time.</li>
<li>You don't <em>have</em> to use a mobile device, you can actually configure everything on the base station itself via the touch screen. Yeah, I didn't know you could control it like this either until I began the setup.</li>
</ol>
<p>Moving on, you can either setup a whole new system, just a mesh point or connect to an existing one:</p>
<p></p>
<p>Obviously, this is a newie so I begin the setup which kicks off a step by step wizard which I'd already done anyway by following the instructions in the box:</p>
<p></p>
<p></p>
<p></p>
<p></p>
<p>Each one of the steps above animates and shows you precisely what to plug in to where. Putting my everyday consumer hat back on, this is pretty cool and makes it <em>very</em> hard to screw something up. When it's all done, AmpliFi appears:</p>
<p></p>
<p>The app actually connects via Bluetooth which is a pretty neat way of initiating the setup. Select it and wait for the connection to be established:</p>
<p></p>
<p>Whilst connecting, the display on the physical unit gives feedback as to what's going on:</p>
<p></p>
<p>Once that's ready, the app progresses to the setup screen:</p>
<p></p>
<p>This is pretty self-explanatory but also a reminder of the consumer-centric nature of it. By default, AmpliFi is going to use the same password for connecting to the wifi as it is to administer the whole thing. This makes it easier for everyday folks, but it also means that if you give someone else the password for the wifi you're also giving them the password that allows everything to be reconfigured. Let's not do that:</p>
<p></p>
<p>I call the network &quot;Josephine&quot; after the street they live on and elect to define a different password for the admin which causes it to go away and think for a bit:</p>
<p></p>
<p>And then we're done:</p>
<p></p>
<p>It's only been on Bluetooth up until now so I still need to actually connect to the wifi network which I do next (and yeah, I chose the old network name originally):</p>
<p></p>
<p>Flick back to the AmpliFi app and I get the first glimpse of the system overview. Apparently, everything is great:</p>
<p></p>
<p>Around this time, the device advises an update is available:</p>
<p></p>
<p>This is one of the things I love about UniFi, namely that even after I bought all the gear, it keeps getting better. I can't remember <em>ever</em> updating my old wireless access points for either security fixes or new features but have since grown used to Ubiquiti pushing out new bits to the UniFi range. Touch the screen above, wait for our lousy Aussie bandwidth to do its thing then it's job done.</p>
<p></p>
<p>Once the upgrade is complete, the base station defaults to the date and time which it's automatically worked out (I assume based on outbound IP address):</p>
<p></p>
<p>Touching the screen causes the unit to cycle through to another display so I flick through each one to see what you get:</p>
<p></p>
<p></p>
<p></p>
<p></p>
<p>They're all pretty self-explanatory so I won't dwell on them here. I connect dad's iPhone too then click on the &quot;Clients&quot; icon in the system page:</p>
<p></p>
<p>The first thing you see here is the ability to pause internet connectivity on all devices. One area where AmpliFi excels over UniFi is simple access to features like this. You can certainly do this with UniFi, but it's clear this is meant to be a &quot;family friendly&quot; approach. Same again with the ability to create profiles which can schedule allowable periods for devices. Mum and dad won't need this but were we all 30 years earlier on in life, it'd be handy for keeping the kids' internet access under control.</p>
<p>We're seeing dad's iPhone listed then mine under that. The pause button to the left of the clients immediately cuts off internet to that device which again, is far more easily accessible than in the UniFi world. Seeing the bandwidth next to the client is also really useful and I'll give you a great example of why: mum and dad get 4Mbs down. No, not a typo, welcome to semi-rural Australia! If they're sitting there trying to watch some Netflix (and yes, that does actually work with only 4Mbps) and it's buffering like crazy, they can pull out the app and see the PC pulling down an update then just cut that machine off without getting off the couch. That's actually pretty cool.</p>
<p>I drill down on my phone:</p>
<p></p>
<p>Another big pause button plus the ability to change the device's traffic priority (I later set the Apple TV to &quot;streaming&quot; which should give it a higher priority). While I'm there, I click on my device name and give it a friendlier title then do the same with Dad's too, just like I would with UniFi:</p>
<p></p>
<p>I've never seen the AmpliFi UI so I have a browse around and check out the performance details:</p>
<p></p>
<p></p>
<p>It's some basic throughput info but it's the ability to do an ISP test that I found more interesting. This is not dissimilar to just using <a href="http://beta.speedtest.net/">Speedtest</a> but I like that it's built into the access point and keeps a history of the results in a central location. Depressing, aren't they? :(</p>
<p>Moving on, I thought I'd check out the &quot;Diagnose&quot; page:</p>
<p></p>
<p>Pretty basic stuff as it stands, but I can see how it'd be useful if things were playing up. But that red mark on the mesh points has gotta go so it's time to plug one of those in. We go down to the very far end of the house to the spare room and plug in a mesh unit:</p>
<p></p>
<p>The lights animate in a fashion that implies it's looking for a signal. A few minutes later, just the 2 bottom ones remain lit. I open up the app again and see the unit now connected:</p>
<p></p>
<p>From a &quot;how easy is this to use&quot; perspective, this is an absolute no-brainer. The mesh point was automatically identified and joined to the network, I didn't need to do a thing. All the times on the top of the iPhone are legit too - I first opened the app at 11:47 and 39 minutes later I had the base station configured, devices connected and named plus the mesh unit joined to the whole thing. Plus, I screen capped stuff, took photos and started drafting this post :)</p>
<p>I drilled down on the mesh unit by clicking on it in the image above:</p>
<p></p>
<p>The signal strength is the main thing that stood out at me, that and the use of the 2.4G network rather than 5G. This is at the absolute other end of the house with many walls and rooms in between including a laundry with a hot water heater in it, a bathroom with tiled walls, a toilet, a kitchen and a dining area so signal strength was always going to cop it. I angled the antenna as best I could (it swivels on a magnetic base), but that was the best I could get.</p>
<p>Time for the other mesh unit and I put this one in the rumpus room which causes the lights to do the same dance. That takes a few minutes so I go off and connect a bunch of other devices to the new network. Shortly later, 3 lights remain illuminated on the second mesh unit:</p>
<p></p>
<p>Back in the app, I drill down into both the mesh units and name them in a more logical fashion then jump back to the system page to review it all:</p>
<p></p>
<p>Clearly the rumpus room has a better signal quality and I also note it's connected over 5G (faster speed but less range). That stands to reason as there's a lot less stuff between it and the base station. It's only a little bit closer physically, but there's a lot more open space. You can get a rough sense of the positioning from the overview below with the spare room towards the left of screen and the rumpus room towards the top with the base station at far right:</p>
<p></p>
<p>This is all looking pretty neat now, but it looks like there's more updating to be done:</p>
<p></p>
<p>The mesh units obviously run firmware too and between the time they've shipped and we've eventually installed them, updates have landed. As the updates begin, the mesh units drop off the system page:</p>
<p></p>
<p>Just like the UniFi range, access points can come and go and clients will simply hop to whatever is still available. Having spent a heap of time with UniFi now, the family ties are obvious, they just manifest themselves here at a higher level that's more consumer friendly.</p>
<p>While the mesh units are updating, I connect the Apple TV then set the client to be optimised for streaming:</p>
<p></p>
<p>A few minutes later and the mesh units are back online:</p>
<p></p>
<p>I wanted to have a play with the guest network while I was there. There's no problem doing guest networks with the UniFi gear (at least not if you're a techie), but I could see it being a challenge for everyday folks. I jump in and take a look at the AmpliFi interface:</p>
<p></p>
<p>The guest network name defaults to the name of the primary one with a &quot;-guest&quot; suffix and allows up to 3 guests for a predefined period of time before the network will be turned off. From a UX perspective, this is good as it's immediately clear. I start the network:</p>
<p></p>
<p>Seems easy enough so I grab another machine and look for the network:</p>
<p></p>
<p>Um, &quot;Open&quot;? Turns out the guest network defaults to not being password protected. That's not going to do so it's back to the AmpliFi app and into the settings where the security can be changed to something more appropriate:</p>
<p></p>
<p>I'd prefer to see WPA2 on by default. I get that this is about minimising friction and that the network may only be active for a little while, but I worry that you'll end up with people having an open network for a longer period of time and putting themselves at risk. In mum and dad's case, I create a password that won't be hard for guests to enter but will keep out those who aren't meant to be in there. Looking at it again yields a much more pleasing result:</p>
<p></p>
<p>I connect to the network and then out of curiosity, grab my iPhone and look at the client details:</p>
<p></p>
<p>This is again reminiscent of UniFi in terms of the breadth of data exposed. My Yoga 910 has connected to the rumpus room mesh unit over 5G and has a 67% signal strength and a fantastic connection speed (too bad about the internet speed once it makes any outbound requests...)</p>
<p>Jumping back to the diagnostics now, it looks like we're all done:</p>
<p></p>
<p>I connect all the remaining devices and review them in the app:</p>
<p></p>
<p>Just for kicks, I've paused the connection on my Yoga 910. Every time I do that, the base station beeps and the lights flash whilst immediately killing the internet connection on the device. This could provide endless &quot;entertainment&quot; for folks wanting to mess with their kids :)</p>
<p>Finally, because the old hardware is now just there to provide internet, I killed the wifi on it (I'll set it merely to run in bridge mode next time I visit):</p>
<p></p>
<p>So that's all the fundamentals and looking back on it with my consumer hat on, it was definitely the easiest wifi setup I've ever done. You can still drill down into the settings of that base station unit and tweak all the usual things like port forwarding and DHCP, but that's stuff your everyday folks are rarely going to touch. Have a read through <a href="https://amplifi.com/docs/AmpliFi_UG.pdf">the AmpliFi user guide</a> if you'd like to better understand all the options there.</p>
<p>I installed all this yesterday, walked around a bit and made sure devices roamed and reached a good distance outside then left. Subsequently, I can't comment on the long-term performance or how it behaves with a heap of devices or any other attributes that are only observable over time, but given Ubiquiti's reputation for quality and how rock solid all the UniFi builds I've done have been I think it's a pretty foregone conclusion. By coincidence, I bumped into one of my neighbours the day before doing this install and he'd rolled out AmpliFi after I'd shown him the box of bits Ubiquiti sent me. He was rapt - totally sorted out his dead spots, he had the kids under control and he now wanted to invest in the company (he works in the finance industry), so that's pretty positive feedback.</p>
<p>The only real gripe I have (other than the power cable length), is that I'd like some more &quot;secure by default&quot; approaches. I understand why they're reusing the admin password by default and not protecting a guest network by default, but I'd like to see them nudging people in the right direction. It'd only take a short sentence underneath to do it: &quot;Using the same password could allow anyone you share it with to reconfigure your network&quot;. But of course, in terms of a prospective purchaser, the security is still configurable anyway.</p>
<p>So, AmpliFi or UniFi? Well firstly, if you've got an existing network that covers everywhere you need and it performs fine, you may not want either. You'd have to actually <em>need</em> some of those features that AmpliFi offers above and beyond your old gear to justify it. Ok, either need or &quot;want&quot; because it is pretty slick and features like scheduled hours for the kids, traffic prioritisation and good visibility of which clients are sucking down data (and the ability to pause them), are really pretty neat. But you need to remember that this is a cost <em>on top of</em> an existing modem so for many people that will mean having whatever basic device it is the ISP shipped them then going and spending money on AmpliFi too.</p>
<p>If you're an everyday non-techie consumer <em>or</em> are tech-minded but simply don't need features beyond what I've described here, you'll be very happy with AmpliFi. My friend <a href="https://www.hanselman.com/blog/ReviewTheAmpliFiHDHighDensityHomeWiFiMeshNetworkingSystem.aspx">Scott Hanselman recently installed AmpliFi</a> and he's certainly a very techie bloke who is very happy with the decision. AmpliFi does actually do some things UniFi doesn't too; it's more accessible to perform a range of tasks plus the base station with the real time readouts is actually pretty cool and if I'm honest, I wish I had something to that effect myself.</p>
<p>There are multiple cases where UniFi makes more sense and the first is in scenarios like <a href="https://www.troyhunt.com/wiring-a-home-network-from-the-ground-up-with-ubiquiti/">the install I did in my brother's house</a>. We did a ground-up install whilst he was renovating which meant putting in-wall access points in every room and running Cat6 back to a central patch board and switch. This is a <em>very</em> neat install which has fantastic coverage plus gives him a couple of RJ45 jacks in every room (one with PoE). In a perfect world, this is what you want in your house.</p>
<p>In my house, the whole place was already wired with a couple of jacks in every room and a patch board in the garage. Plugging UniFi access points directly into a wired connection is <em>always</em> going to yield a better result than AmpliFi mesh units wirelessly up-linking to the base station through walls and floors and that sort of thing. Plus, there's a lot more options in terms of how the network is designed given how many different styles of access point and switches there are (check out <a href="https://www.troyhunt.com/heres-what-this-ubiquiti-unifi-stuff-is-all-about/">my free UniFi course</a> for more on this).</p>
<p>There's a heap more control and configurability with UniFi because it's meant to be used in far more complex configurations. The sort of things I mean here is everything from deep packet inspection (which devices are talking to which external services), automatically generated topology maps (<a href="https://www.troyhunt.com/content/images/2017/03/Topology-Map.png">a visual map of what's connected to where</a>) and a huge amount of configurability on each AP itself (channel width, transmit power, minimum RSSI etc). Thing is though, you need to decide if that's any use to you.</p>
<p>Lastly, a determining factor is inevitably cost. I spent over A$2k setting my house up with UniFi. Admittedly, I did go a bit overboard but it was about the same for my brother's setup too. You can definitely get in much cheaper than that - less than half if you're frugal - but you can <a href="http://www.wireless4now.com.au/ubiquiti-amplifi-high-density-hd-home-wi-fi-router.html">get into AmpliFi for only a couple of hundred bucks</a>. That's just the base station that'll still give you all the features above but without the additional range from the mesh units, you're looking at <a href="http://www.wireless4now.com.au/ubiquiti/ubiquiti-amplifi/ubiquiti-amplifi-afi-high-density-home-wi-fi-mesh-technology.html">well over $500 for the setup I put into mum and dad's</a>. But for many people, that'll easily be worth it for the sanity and productivity of actually having reliable internet everywhere!</p>
</div>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://jvns.ca/blog/2017/09/03/debugging-netlink-requests/">Debugging netlink requests</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://jvns.ca/atom.xml">Julia Evans</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">practices</span>
              <span class="tag">linux</span>
              <span class="tag">networking</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Sun Sep 03 2017 22:46:00 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>This week I was working on a Kubernetes networking problem. Basically
our container network backend was reporting that it couldn’t delete
routes, and we didn’t know why.</p>

<p>I started reading the code that was failing, and it was using a library
called “netlink”. I’d never heard of that before this week.</p>

<h3>what’s netlink?</h3>

<p>Wikipedia says:</p>

<blockquote>
<p>Netlink socket family is a Linux kernel interface used for
inter-process communication (IPC) between both the kernel and
userspace processes, and between different userspace processes, in a
way similar to the Unix domain sockets.</p>
</blockquote>

<p>The program I was debugging was creating/deleting routes from the route table.
It seems like netlink is capable of doing lots of things (communicate kernel
&lt;-&gt; userspace and userspace &lt;-&gt; userspace), but in this case what was happening
was pretty simple</p>

<ol>
<li>userspace program creates a netlink socket</li>
<li>userspace program sends a message with that socket asking the kernel
to delete a route</li>
<li>kernel deletes the route (or in our case, fails and returns an error message)</li>
</ol>

<h3>how to see netlink messages with strace</h3>

<p>Let’s create some netlink messages! Luckily this is easy: if we use the
<code>ip</code> tool to create and delete a route, it uses netlink.</p>

<pre><code>ip route add 172.16.5.0/24 via 127.0.0.1 dev lo
ip route del 172.16.5.0/24 via 127.0.0.1 dev lo
</code></pre>

<p>Cool, let’s strace it! Here’s the command:</p>

<pre><code>strace -s 100 -f -o out -x ip route add 172.16.5.0/24 via 127.0.0.1 dev lo
</code></pre>

<p>and the output:</p>

<pre><code>socket(PF_NETLINK, SOCK_RAW|SOCK_CLOEXEC, NETLINK_ROUTE) = 3
bind(3, {sa_family=AF_NETLINK, pid=0, groups=00000000}, 12) = 0
getsockname(3, {sa_family=AF_NETLINK, pid=13058, groups=00000000},
sendmsg(3, {msg_name(12)={sa_family=AF_NETLINK, pid=0, groups=00000000},
    msg_iov(1)=[{&quot;\x34\x00\x00\x00\x18\x00\x05\x06\x6e\xbc\xac\x59\x00\x00\x00\x00\x02\x18\x00\x00\xfe\x03\x00\x01\x00\x00\x00\x00\x08\x00\x01\x00\xac\x10\x05\x00\x08\x00\x05\x00\x7f\x00\x00\x01\x08\x00\x04\x00\x01\x00\x00\x00&quot;,
    52}], msg_controllen=0, msg_flags=0}, 0) = 52
recvmsg(3, {msg_name(12)={sa_family=AF_NETLINK, pid=0,
    msg_iov(1)=[{&quot;\x24\x00\x00\x00\x02\x00\x00\x00\x6e\xbc\xac\x59\x02\x33\x00\x00\x00\x00\x00\x00\x34\x00\x00\x00\x18\x00\x05\x06\x6e\xbc\xac\x59\x00\x00\x00\x00&quot;,
    32768}], msg_controllen=0, msg_flags=0}, 0) = 36

</code></pre>

<p>So we see that it:</p>

<ul>
<li>creates the netlink socket &amp; binds to it</li>
<li>sends a message (<code>\x34\x00...</code>)</li>
<li>receives a response</li>
</ul>

<p>Okay, but what does that message <strong>say</strong>? Here’s the message again:</p>

<pre><code>\x34\x00\x00\x00\x18\x00\x05\x06\x9e\xbc\xac\x59\x00\x00\x00\x00\x02\x18\x00\x00\xfe\x03\x00\x01\x00\x00\x00\x00\x08\x00\x01\x00\xac\x10\x05\x00\x08\x00\x05\x00\x7f\x00\x00\x01\x08\x00\x04\x00\x01\x00\x00\x00
</code></pre>

<p>Not super understandable, right? Well, luckily there’s a Python tool
that can help us understand it! We’ll save this to a file called
<code>message</code>.</p>

<h3>decoding netlink messages with pyroute2</h3>

<p>I googled how to decode netlink messages and I found this great page:
<a href="http://docs.pyroute2.org/debug.html">http://docs.pyroute2.org/debug.html</a>.</p>

<p>Decoding my netlink message turned out to be pretty simple: I just had
to run this:</p>

<pre><code>pip install pyroute2
wget https://raw.githubusercontent.com/svinota/pyroute2/72e444714f37a313fb15bdb22734e517feefa9e9/tests/decoder/decoder.py
python decoder.py pyroute2.netlink.rtnl.rtmsg.rtmsg message
</code></pre>

<p>Here’s the output!</p>

<pre><code>{'attrs': [('RTA_DST', '172.16.5.0'),
           ('RTA_GATEWAY', '127.0.0.1'),
           ('RTA_OIF', 1)],
 'dst_len': 24,
 'family': 2,
 'flags': 0,
 'header': {'flags': 1541,
            'length': 52,
            'pid': 0,
            'sequence_number': 1504493250,
            'type': 24},
 'proto': 3,
 'scope': 0,
 'src_len': 0,
 'table': 254,
 'tos': 0,
 'type': 1}
</code></pre>

<p>I don’t understand all of this but we’re just going to focus on this part:</p>

<pre><code>{'attrs': [('RTA_DST', '172.16.5.0'),
           ('RTA_GATEWAY', '127.0.0.1'),
           ('RTA_OIF', 1)],
</code></pre>

<p>The dst and gateway fields are pretty easy to understand there!</p>

<h3>why the program I was debugging wasn’t working</h3>

<p>You see this <code>RTA_OIF</code> field? This field is a <strong>network interface id</strong>. For
example, on my laptop right now I have 5 network interfaces, numbered 1 through
5. The (correct) message above has <code>RTA_OIF</code> set to 1, for the <code>lo</code> loopback interface.</p>

<pre><code>1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: enp0s25: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000
    link/ether 3c:97:0e:55:b3:7f brd ff:ff:ff:ff:ff:ff
3: wlp3s0: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc mq state DOWN mode DORMANT group default qlen 1000
    link/ether 60:67:20:eb:7b:bc brd ff:ff:ff:ff:ff:ff
4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default 
    link/ether 02:42:a0:c5:c1:be brd ff:ff:ff:ff:ff:ff
5: nlmon0: &lt;NOARP,UP,LOWER_UP&gt; mtu 3776 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1
    link/netlink 
</code></pre>

<p>But in our errant program, the <code>RTA_OIF</code> field was set to 0! 0 is not even a
valid value for this field, I don’t think! (0 is not a valid network interface ID)</p>

<h3>pyroute2 is great</h3>

<p>pyroute2 is really cool, if I wanted to write a quick script to understand
what’s going on with my network interfaces &amp; routes I would 100% definitely try
pyroute2. There are a lot of <a href="http://docs.pyroute2.org/general.html#rtnetlink-sample">great examples here</a>.</p>

<p>For example! If I want to run the equivalent of <code>ip route add 172.16.5.0/24 via 127.0.0.1 dev lo</code>, that’s:</p>

<pre><code>from pyroute2 import IPRoute
ip = IPRoute()
ip.route('add',
         dst='172.16.0.0/24',
         gateway='127.0.0.1',
         oif=1)
</code></pre>

<p>Super simple! <code>oif=1</code> means the same as <code>dev lo</code>.</p>

<h3>other ways to capture netlink messages: tcpdump + wireshark</h3>

<p>You can also use tcpdump to capture netlink messages! here’s how:</p>

<pre><code># create the network interface
sudo ip link add  nlmon0 type nlmon
sudo ip link set dev nlmon0 up
sudo tcpdump -i nlmon0 -w netlink.pcap # capture your packets
wireshark netlink.pcap # look at the results with wireshark
</code></pre>

<p>I tried this but had trouble for a couple reasons</p>

<ol>
<li>It didn’t work for me on the server I was working on (though it works on my laptop now)</li>
<li>I actually found it harder to work with than the strace method – it captured too many packets and I found it hard to filter them in Wireshark.</li>
</ol>

<h3>ip monitor</h3>

<p>You can also run <code>ip monitor</code> and it’ll tell you some netlink requests. when I run it it prints out this stuff:</p>

<pre><code>$ sudo ip monitor
fd68:29:f8f6::1 dev enp0s25 lladdr c4:6e:1f:95:d8:3e router STALE
fe80::c66e:1fff:fe95:d83e dev enp0s25 lladdr c4:6e:1f:95:d8:3e router STALE
192.168.1.144 dev enp0s25 lladdr 14:30:c6:ba:e4:6c STALE
192.168.1.144 dev enp0s25 lladdr 14:30:c6:ba:e4:6c PROBE
192.168.1.144 dev enp0s25 lladdr 14:30:c6:ba:e4:6c STALE
192.168.1.144 dev enp0s25 lladdr 14:30:c6:ba:e4:6c REACHABLE
fd68:29:f8f6::1 dev enp0s25 lladdr c4:6e:1f:95:d8:3e router PROBE
fd68:29:f8f6::1 dev enp0s25 lladdr c4:6e:1f:95:d8:3e router REACHABLE
fe80::c66e:1fff:fe95:d83e dev enp0s25 lladdr c4:6e:1f:95:d8:3e router PROBE
fe80::c66e:1fff:fe95:d83e dev enp0s25 lladdr c4:6e:1f:95:d8:3e router REACHABLE
</code></pre>

<p>It didn’t give me the information I wanted though.</p>

<h3>nltrace</h3>

<p>There’s also <a href="https://github.com/socketpair/nltrace">nltrace</a> (for instance <code>nltrace ip route list</code>) but in this case it didn’t give me the information I wanted. It’s not a maintained project but looks maybe useful!</p>

<h3>that’s all!</h3>

<p>It always makes me happy when I learn about a NEW LINUX THING during the course
of my job. When I was in the middle of this I tweeted</p>

<blockquote>
<p>kubernetes is cool but definitely not easy, my experience is definitely like
“learn how all the networking works in excruciating detail”</p>
</blockquote>

<p>which definitely feels true, it’s less like “set up networking and it works”
and more like “pick a networking backend, wait a month, discover weird
problems, strace it, learn things about netlink and what a <code>RTA_OIF</code> is, fix
the bugs, eventually it works”. Maybe that isn’t everyone’s experience but that
is my experience so far!</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://jvns.ca/blog/2017/09/03/network-interfaces/">What&amp;#39;s a network interface?</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://jvns.ca/atom.xml">Julia Evans</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">practices</span>
              <span class="tag">linux</span>
              <span class="tag">networking</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Sun Sep 03 2017 22:42:00 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>I’ve been working with container networking a bunch this week. When learning
about new unfamiliar stuff (like container networking / virtual ethernet
devices / bridges / iptables), I often realize that I don’t fully understand
something much more fundamental.</p>

<p>This week, that thing was: network interfaces!!</p>

<p>You know, when you run <code>ifconfig</code> and it lists devices like <code>lo</code>, <code>eth0</code>,
<code>br0</code>, <code>docker0</code>, <code>wlan0</code>, or whatever. Those.</p>

<p>This is a thing I <strong>thought</strong> I understood but it turns out there are at least
2 things I didn’t know about them.</p>

<p>I’m not going to try to give you a crisp definition, instead we’re going to
make some observations, do some experiments, ask some questions, and make some
guesses.</p>

<h3>What happens if you don’t have any network interfaces?</h3>

<p>I was messing around with network namespaces, and I created a new one with:</p>

<p><code>sudo ip netns add ns1</code></p>

<p>It turns out that when you create a new network namespace, it doesn’t have any
network interfaces at all! What does that mean?  Let’s explore and see what it
looks like:</p>

<p>We can run commands inside this new network namespace with <code>sudo ip netns exec ns1 COMMAND</code>. I’m just going to run a shell inside this network namespace, and then
try out some things.</p>

<p>So let’s start with <code>sudo ip netns exec ns1 bash</code></p>

<pre><code>$ sudo ip netns exec ns1 bash
$ ifconfig
(no output)
</code></pre>

<p>That makes sense, this is a new network namespace so there are no network
interfaces set up yet. Still inside that network namespace, let’s try to make a
webserver and connect to it.</p>

<pre><code>$ nc -l 8900 &amp; # make a server on port 8900
$ netstat -tulpn # list open ports
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address   PID/Program name
tcp        0      0 0.0.0.0:8900    2918/nc 
$ curl localhost:8900
curl: (7) Couldn't connect to server
</code></pre>

<p>Okay, so this is sort of interesting. I can create a server on port 8900 with
<code>nc -l 8900</code>. And netstat shows that that server exists. But when I try to
<code>curl localhost:8900</code>, nothing happens!</p>

<p>What if I try to create a server listening on 127.0.0.1?</p>

<pre><code>sudo nc -l 127.0.0.1 8080
nc: Cannot assign requested address
</code></pre>

<p>Doesn’t work. Makes sense.</p>

<p>I think what’s happening here is:</p>

<ul>
<li><code>nc -l 8900</code> is listening on 0.0.0.0:8900, which means “all network interfaces”</li>
<li>but there are no network interfaces</li>
<li>so when we do <code>curl localhost:8900</code>, no packets actually get sent (when I ran tcpdump, no packets show up)</li>
<li>so <code>nc</code> never receives any packets</li>
</ul>

<p>Let’s do an experiment to try to confirm our hypotheses: let’s add a network
interface! The idea is that if we have a <code>lo</code> network interface, then <code>curl
localhost:8900</code> will actually send packets, <code>nc</code> will receive them, and
everything will work.</p>

<pre><code>$ ip link set dev lo up # this sets uo the 'lo' loopback interface
$ curl localhost:8900                                                                               
# BAM! this totally works! 
# the backgrounded netcat prints out this output:
GET / HTTP/1.1
User-Agent: curl/7.35.0
Host: localhost:8900
Accept: */*
</code></pre>

<p>This is rad. What we know now:</p>

<ul>
<li>if you don’t have any network interfaces, you can’t do any networking (but you can start servers on 0.0.0.0 and netstat shows those servers)</li>
<li>when we add a network interface, our server starts working right away (without having to restart the server)</li>
</ul>

<h3>A packet can appear multiple times in tcpdump</h3>

<p>Something I’ve been observing recently but haven’t fully understood is – sometimes I’ll be on a machine which has</p>

<ul>
<li>virtual network interfaces for each container (<code>vethXXXXXXX</code>)</li>
<li>a bridge interface (<code>cni0</code>)</li>
<li>and a “real” network interface to the outside world (<code>eth0</code>)</li>
</ul>

<p>When containers send packets to the outside world and I’m running <code>sudo tcpdump -i any</code>, I’ll see those packets <strong>3 times</strong>.</p>

<p>I know a few more things about how tcpdump works:</p>

<ol>
<li>I can run <code>sudo tcpdump -i cni0</code> to listen on a specific interface. When I do that, the packets appear only once</li>
<li>tcpdump happens at the “beginning” of the network stack. I think that means that packets are captured by tcpdump when packets enter a network interface</li>
</ol>

<p>What does “enter a network interface” actually mean, though? I tried to look at
<a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/">this 20,000 word article on the iinux network stack</a>
and I think I have a workable theory!</p>

<h3>What happens when a packet is created?</h3>

<p>Okay, so I skimmed <a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/">Monitoring and Tuning the Linux Networking Stack: Receiving Data</a> and I think I have a working hypothesis for how packets</p>

<ul>
<li>get assigned network interfaces</li>
<li>get captured by tcpdump</li>
<li>can be assigned more than one network interface</li>
</ul>

<p>First thing first, this document refers to “network interfaces” as “network devices”. I think those are the same thing.</p>

<p>So!! Let’s say I create a packet on my computer.</p>

<p><strong>step 0</strong>: iptables prerouting rules</p>

<p><strong>step 1</strong>: the packet gets <strong>routed</strong>.</p>

<p>Routing a packet means “assigning it a network device”.</p>

<p>Let’s do a tiny experiment in routing – I have 3 interfaces on my computer right now</p>

<pre><code>$ ifconfig
docker0   Link encap:Ethernet  HWaddr 02:42:ef:ab:0d:ac  
          inet addr:172.17.0.1  Bcast:0.0.0.0  Mask:255.255.0.0

enp0s25   Link encap:Ethernet  HWaddr 3c:97:0e:55:b3:7g  
          inet addr:192.168.1.213  Bcast:192.168.1.255  Mask:255.255.255.0

lo        Link encap:Local Loopback  
          inet addr:127.0.0.1  Mask:255.0.0.0
</code></pre>

<p>and here are the routes:</p>

<pre><code>$ sudo ip route list table all
default via 192.168.1.1 dev enp0s25  proto static  metric 100 
169.254.0.0/16 dev docker0  scope link  metric 1000 linkdown 
172.17.0.0/16 dev docker0  proto kernel  scope link  src 172.17.0.1 linkdown 
192.168.1.0/24 dev enp0s25  proto kernel  scope link  src 192.168.1.213  metric 100 
local 127.0.0.0/8 dev lo  table local  proto kernel  scope host  src 127.0.0.1 
local 127.0.0.1 dev lo  table local  proto kernel  scope host  src 127.0.0.1 
local 172.17.0.1 dev docker0  table local  proto kernel  scope host  src 172.17.0.1 
</code></pre>

<p>So – if I make a request to 172.17.0.1 (<code>curl 172.17.0.1:8080</code>), it seems like that would end up on the
<code>docker0</code> device. Right? Wrong, apparently.</p>

<p>If I run <code>tcpdump -i lo</code> packets to 172.17.0.1 show up, and if I run <code>tcpdump -i docker0</code>,
the packets don’t show up. So it seems right now, on my machine,
packets sent to 172.17.0.1 go through the <code>lo</code> device.</p>

<p>The reason they get sent to <code>lo</code> instead of <code>docker0</code> is that there’s a route
for 172.17.0.1  in my route table that says <code>local</code> – the same reasons that
packets to <code>127.0.0.1</code> get sent to <code>lo</code>.</p>

<p><strong>step 2</strong> tcpdump gets the packet</p>

<p>This is pretty straightforward – once there’s a network device attached to the
packet, then tcpdump gets the packet.</p>

<p>That’s all I know for now!</p>

<h3>ok so what do we know about network interfaces?</h3>

<p>Here’s what I think so far:</p>

<ul>
<li>they can be physical network interfaces (like <code>eth0</code>) or virtual interfaces (like <code>lo</code> and <code>docker0</code>)</li>
<li>you can list them with <code>ifconfig</code> or <code>ip link list</code></li>
<li>if you don’t have any network interfaces, your packets don’t enter the linux network stack at all really. To go through the network stack you need network interfaces.</li>
<li>When you send a packet to an IP address, your <strong>route table</strong> decides which network interface that packet goes through. This is one of the first things that happens in the network stack.</li>
<li>tcpdump captures packets after they’re routed  (assigned an interface) Though there’s a <code>PREROUTING</code> chain in iptables that happens before routing!`</li>
</ul>

<p>Some of this is probably wrong, let me know what! I’m on Twitter as always (<a href="https://twitter.com/b0rk">https://twitter.com/b0rk</a>)</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://jvns.ca/blog/2017/09/03/telling-people-what-you-re-working-on/">Telling people what you&amp;#39;re working on</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://jvns.ca/atom.xml">Julia Evans</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">practices</span>
              <span class="tag">linux</span>
              <span class="tag">networking</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Sun Sep 03 2017 20:08:54 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>At work sometimes people send “kickoff” emails, basically announcing “hey,
we’re going to start working on this project, here’s why”.</p>

<p>The format is basically:</p>

<ul>
<li>explain the goals (why is this gonna be awesome?)</li>
<li>explain who’s affected by the project</li>
<li>maybe talk about the risks a little</li>
<li>talk a little about timelines</li>
</ul>

<h3>announcing what you’re working on can be scary</h3>

<p>I find it kind of scary to tell people what I’m working on! Once I was
talking about plans with a Very Experienced Person and they said “yeah, sharing
tentative plans with lots of other people <strong>is</strong> scary” and I was so happy that it
wasn’t just me.</p>

<p>Some reasons I think it’s sort of scary:</p>

<ol>
<li>what if I fail and everyone thinks I’m terrible at my job?</li>
<li>what if something unexpected happens and we have to completely change the plan?</li>
<li>what if this takes way longer to do than we think?</li>
</ol>

<p>It seems much safer to:</p>

<ol>
<li>start working on a thing</li>
<li>hide in a corner until it’s done</li>
<li>announce “look, we did it, it worked”</li>
</ol>

<p>After all, if I only tell people about things when they’re finished and
amazing, then everything I announce will have succeeded! 100% success rate!!
What could be better?</p>

<p>But this pattern of “let me hide in a corner until I’m done this project,
or at least I won’t tell anyone outside my team” sounds suspiciously like “I won’t
push my code to github until it’s perfect”, and we know it’s important to share
works in progress when you’re coding! :)</p>

<h3>reasons announcing plans is good</h3>

<p>A few reasons announcing plans is useful:</p>

<ol>
<li>Everyone else is trying to plan their work all the time. If I announce what I’m planning, that helps people who are working on related projects! For example, my team just announced a project that interacts a lot with another team’s work, so that team is going to allocate someone to work on it!</li>
<li>If we advertise a plan maybe someone will notice an important problem with it and tell us!</li>
<li>Writing a plan forces your clarify your goals</li>
<li>Communicating clear plans helps management… manage.</li>
<li>I think announcing <em>ambitious</em> plans can help kind of.. inspire the people around you? Like I sometimes see that someone else is kicking off a Very Important Project and I think, wow, that’s really cool, maybe we can do something cool like that.</li>
<li>It’s exciting to see people starting and finishing projects, like it creates momentum for the whole company if everyone talks about what they’re doing!</li>
</ol>

<p>I think the shared planning aspect is probably the most important though! If
everyone hides in a corner and doesn’t tell people what they’re doing until
they’re done… well, that wouldn’t really be a very effective way to work
together.</p>

<p>One of our sibling teams always announces what they’re working on VERY LOUDLY
(like they send tons of announcement emails) and I kind of love it because I
always know what’s going on with them. Their plans don’t always work out
exactly as planned! Sometimes they change direction! But it’s really awesome to
know how they’re spending their time &amp; what to expect from them in the near
future.</p>

<h3>ways to make it less scary</h3>

<p>Remember <strong>it’s my job/responsibility</strong> to talk about what I’m doing. We’re all
trying to build something together, and as a team we can’t do that if we don’t
talk about our plans. (I think as I get more experienced this becomes more and
more my job)</p>

<p>Another thing I think might help me is <strong>assume everything I do will succeed</strong>.
I feel like I sometimes waste a lot of time worrying “oh no, what if this goes
wrong”. And of course to some extent worrying is useful! Every project has
risks and it’s important to think about how to manage those risks. But I think
if I start with the assumption that the thing I’m embarking on will probably
work, it’ll be easier to execute and tell people about. It seems like
confidence is pretty important when you’re trying to do something hard.</p>

<p>It’s definitely important to <strong>tell people plans that are mostly true</strong>. Plans
don’t need to be 100% right (everything always changes), but for them to be
useful, they need to be at least mostly right. Like maybe do a proof of concept
first and chat with the stakeholders.</p>

<p><strong>Just tell the people who need to know about it</strong>. Like – this week we sent a
kickoff email. It was for a networking thing that is relevant to many
developers, but nobody outside engineering will really be affected by it. So we
just sent the email to the developers mailing list!</p>

<p>Maybe that’s it? Like, if when we’re planning something, we just</p>

<ol>
<li>come up with a reasonable plan and think/talk about it with folks</li>
<li>do at least some preliminary work, I think it’s good to at least do a
prototype or something first to get some confidence in the approach</li>
<li>assume that our plan will work (that when we run into problems, we’ll figure
out a way to solve them)</li>
<li>loudly communicate that plan to a reasonable list of people (not too much
bigger than it needs to be)</li>
<li>listen to the feedback we get and incorporate it when we need to</li>
</ol>

<p>then that will be fine!</p>

<h3>reasons hiding in a corner might be good</h3>

<p>I think sometimes there are actually good reasons <strong>not</strong> to tell people what
you’re doing.</p>

<p>In some work cultures, sometimes people can put out a lot of stop energy! If
you say “hey this is what we’re planning”, you might get back “hello here are
100000 reasons why what you’re doing won’t work / is a bad idea”. I think if
people do this a lot it is actually maybe kind of reasonable to react with “ok,
we’re just not going to tell people what we’re doing until we’re further
along”.</p>

<p>Philip Guo has a great vlog about how <a href="http://www.pgbovine.net/PG-Vlog-8-private-creative-projects.htm">it’s important to start personal creative projects privately</a> –
if you don’t tell anyone about an early stage baby idea, it can give you the
space/safety to develop it! That’s really important! So I think there’s a
caveat here, like “early-stage ideas are fragile and need to be sheltered” :)</p>

<h3>hiding in a corner with my keyboard is not for me though</h3>

<p>Right now I’m planning a biggish (big for me, anyway! I actually want people to
use it!) open source project I’m going to start in January and I.. don’t really
know if it will work? So it feels scary to announce “hey, I’m planning X, these
are my goals”. But I will definitely announce it! :)</p>

<p>In the past I’ve definitely worried about getting excessive stop energy /
bikeshedding when talking about plans. I think where I work it’s better to tell
people what I’m working on though! And lately when we share plans with
people, almost all of the feedback I see is super helpful!</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://www.nngroup.com/articles/flat-ui-less-attention-cause-uncertainty/">Flat UI Elements Attract Less Attention and Cause Uncertainty</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://www.nngroup.com/feed/rss/">Nielsen Norman Group</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">ux</span>
              <span class="tag">company</span>
            </div>
            <span class="subtitle is-7">Sun Sep 03 2017 17:00:01 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p><strong>Summary:</strong> Flat interfaces often use weak signifiers. In an eyetracking experiment comparing different kinds of clickability clues, UIs with weak signifiers required more user effort than strong ones.</p><hr /><br /><p> The popularity of <a href="https://www.nngroup.com/articles/flat-design/">  flat design </a> in digital interfaces has coincided with a scarcity of <a href="http://jnd.org/dn.mss/signifiers_not_affordances.html">  signifiers </a> . Many modern UIs have ripped out the perceptible cues that users rely on to understand what is clickable.</p><p> Using eyetracking equipment to track and visualize users’ eye movements across interfaces, we investigated how <strong>  strong clickability signifiers </strong> (traditional clues such as underlined, blue text or a glossy 3D button) and <strong>  weak or absent signifiers </strong> (for example, linked text styled as static text or a ghost button) impact the ways users process and understand web pages.</p> About the Study<p> There are many factors that influence a user’s interaction with an interface. To directly investigate the differences between traditional, weak, and absent signifiers in the visual treatments of interactive elements, we needed to remove any confounding variables.</p><br /><br /><a href="/articles/flat-ui-less-attention-cause-uncertainty/">Read Full Article</a>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://www.nngroup.com/articles/credibility-china/">Counterfeit or Credible? UX Design for Authenticity in China</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://www.nngroup.com/feed/rss/">Nielsen Norman Group</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">ux</span>
              <span class="tag">company</span>
            </div>
            <span class="subtitle is-7">Sun Sep 03 2017 17:00:00 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p><strong>Summary:</strong> In China, websites must work harder than in other markets to gain users’ trust. Displaying the company’s local presence, past client work, and being available to answer questions via online chat are critical.</p><hr /><br /><p> Counterfeiting is not uncommon in China. Thus, a main concern for Chinese web shoppers is making sure that the products they see online are authentic. Some Chinese online marketplaces are assumed to contain fake products, and past negative experiences with these fakes — ink cartridges that don’t last or don’t quite print the correct color — cause consumers to be extra wary when shopping online.</p><p> This guardedness is exacerbated by the restrictive Chinese return policies. It’s rare that sellers will take back a product with “no questions asked.” Chinese consumers who are displeased with a purchase will often choose to absorb the cost and treat it as a lesson rather than attempt a troublesome return process.</p><p> Participants in our latest round of usability studies conducted in China displayed a heightened awareness of fakes, and were on the lookout for markers of authenticity and trustworthiness while researching various products and services.</p><br /><br /><a href="/articles/credibility-china/">Read Full Article</a>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://www.scottaaronson.com/blog/?p=3427">GapP, Oracles, and Quantum Supremacy</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://www.scottaaronson.com/blog/?feed=rss2">Shtetl-Optimized</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">quantum</span>
              <span class="tag">compsci</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Fri Sep 01 2017 14:47:12 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>Let me start with a few quick announcements before the main entrée:</p>
<p>First, the website <a href="http://haspvsnpbeensolved.com">haspvsnpbeensolved.com</a> is now live!  Thanks so much to my friend Adam Chalmers for setting it up.  Please try it out on your favorite P vs. NP solution paper—I think you’ll be impressed by how well our secret validation algorithm performs.</p>
<p>Second, some readers might enjoy a <a href="https://www.youtube.com/watch?v=fUGjv44_X4Q&amp;index=4&amp;list=PLUz_4vZOI0H1XnMEj9uk4Ezp9AUJBVAPE">YouTube video</a> of me lecturing about the computability theory of closed timelike curves, from the <a href="http://cchep2017.quics.umd.edu/">Workshop on Computational Complexity and High Energy Physics</a> at the University of Maryland a month ago.  <a href="https://www.youtube.com/playlist?list=PLUz_4vZOI0H1XnMEj9uk4Ezp9AUJBVAPE">Other videos</a> from the workshop—including of talks by John Preskill, Daniel Harlow, Stephen Jordan, and other names known around <em>Shtetl-Optimized</em>, and of a panel discussion in which I participated—are worth checking out as well.  Thanks so much to Stephen for organizing such a great workshop!</p>
<p>Third, thanks to everyone who’s emailed to ask whether I’m holding up OK with Hurricane Harvey, and whether I know how to swim (I do).  As it happens, I haven’t been in Texas for two months—I spent most of the summer visiting NYU and doing other travel, and this year, Dana and I are doing an early sabbatical at Tel Aviv University.  However, I understand from friends that Austin, being several hours’ drive further inland, got <em>nothing</em> compared to what Houston did, and that UT is open on schedule for the fall semester.  Hopefully our house is still standing as well!  Our thoughts go to all those affected by the disaster in Houston.  Eventually, the Earth’s rapidly destabilizing climate almost certainly means that Austin will be threatened as well by “500-year events” happening every year or two, as for that matter will a large portion of the earth’s surface.  For now, though, Austin lives to be weird another day.</p>
<hr />
<p><strong>GapP, Oracles, and Quantum Supremacy</strong></p>
<p>by Scott Aaronson</p>
<p>Stuart Kurtz 60th Birthday Conference, Columbia, South Carolina </p>
<p>August 20, 2017</p>
<p>It’s great to be here, to celebrate the life and work of Stuart Kurtz, which could never be … <em>eclipsed</em> … by anything.</p>
<p>I wanted to say something about work in structural complexity and counting complexity and oracles that Stuart was involved with “back in the day,” and how that work plays a major role in issues that concern us right now in quantum computing.  A major goal for the next few years is the unfortunately-named Quantum Supremacy.  What this means is to get a clear quantum speedup, for <em>some</em> task: not necessarily a useful task, but something that we can be as confident as possible is classically hard.  For example, consider the 49-qubit superconducting chip that Google is planning to fabricate within the next year or so.  This won’t yet be good enough for running Shor’s algorithm, to factor numbers of any interesting size, but it hopefully <em>will</em> be good enough to sample from a probability distribution over n-bit strings—in this case, 49-bit strings—that’s hard to sample from classically, taking somewhere on the order of 249 steps.</p>
<p>Furthermore, the evidence that that sort of thing is indeed classically hard, might actually be <em>stronger</em> than the evidence that factoring is classically hard.  As I like to say, a fast classical factoring algorithm would “merely” collapse the world’s electronic commerce—as far as we know, it wouldn’t collapse the polynomial hierarchy!  By contrast, a fast classical algorithm to simulate quantum sampling <em>would</em> collapse the polynomial hierarchy, assuming the simulation is exact.  Let me first go over the argument for that, and then explain some of the more recent things we’ve learned.</p>
<p>Our starting point will be two fundamental complexity classes, <a href="https://en.wikipedia.org/wiki/Sharp-P">#P</a> and <a href="https://en.wikipedia.org/wiki/GapP">GapP</a>.</p>
<p>#P is the class of all nonnegative integer functions f, for which there exists a nondeterministic polynomial-time Turing machine M such that f(x) equals the number of accepting paths of M(x).  Less formally, #P is the class of problems that boil down to summing up an exponential number of nonnegative terms, each of which is efficiently computable individually.</p>
<p>GapP—introduced by <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.42.5938">Fenner, Fortnow, and Kurtz</a> in 1992—can be defined as the set {f-g : f,g∈#P}; that is, the closure of #P under subtraction.  Equivalently, GapP is the class of problems that boil down to summing up an exponential number of terms, each of which is efficiently computable individually, but which could be either positive or negative, and which can therefore cancel each other out.  As you can see, GapP is a class that in some sense anticipates quantum computing!</p>
<p>For our purposes, the most important difference between #P and GapP is that #P functions can at least be multiplicatively <em>approximated</em> in the class BPPNP, by using Stockmeyer’s technique of approximating counting with universal hash functions.  By contrast, even if you just want to approximate a GapP function to within (say) a factor of 2—or for that matter, just decide whether a GapP function is positive or negative—it’s not hard to see that that’s already a #P-hard problem.  For, supposing we had an oracle to solve this problem, we could then shift the sum this way and that by adding positive and negative dummy terms, and use binary search, to zero in on the sum’s <em>exact</em> value in polynomial time.</p>
<p>It’s also not hard to see that a quantum computation can encode an arbitrary GapP function in one of its amplitudes.  Indeed, let s:{0,1}n→{1,-1} be any Boolean function that’s given by a polynomial-size circuit.  Then consider the quantum circuit below.</p>
<p></p>
<p>When we run this circuit, the probability that we see the all-0 string as output is</p>
<p>$$ \left( \frac{1}{\sqrt{2^n}} \sum_{z\in \{0,1\}^n} s(z) \right)^2 = \frac{1}{2^n} \sum_{z,w\in \{0,1\}^n} s(z) s(w) $$</p>
<p>which is clearly in GapP, and clearly #P-hard even to approximate to within a multiplicative factor.</p>
<p>By contrast, suppose we had a probabilistic polynomial-time classical algorithm, call it M, to sample the output distribution of the above quantum circuit.  Then we could rewrite the above probability as Prr[M(r) outputs 0…0], where r consists of the classical random bits used by M.  This is again an exponentially large sum, with one term for each possible r value—but now it’s a sum of <em>nonnegative</em> terms (probabilities), which is therefore approximable in BPPNP.</p>
<p>We can state the upshot as follows.  Let ExactSampBPP be the class of <em>sampling problems</em>—that is, families of probability distributions {Dx}x, one for each input x∈{0,1}n—for which there exists a polynomial-time randomized algorithm that outputs a sample exactly from Dx, in time polynomial in |x|.  Let ExactSampBQP be the same thing except that we allow a polynomial-time quantum algorithm.  Then we have that, if ExactSampBPP = ExactSampBQP, then squared sums of both positive and negative terms, could efficiently be rewritten as sums of nonnegative terms only—and hence P#P=BPPNP.  This, in turn, would collapse the polynomial hierarchy to the third level, by Toda’s Theorem that PH⊆P#P, together with the result BPPNP⊆∑3.  To summarize:</p>
<p><strong>Theorem 1.</strong>  Quantum computers can efficiently solve exact sampling problems that are classically hard unless the polynomial hierarchy collapses.</p>
<p>(In fact, the argument works not only if the classical algorithm exactly samples Dx, but if it samples from any distribution in which the probabilities are multiplicatively close to Dx‘s.  If we really only care about exact sampling, then we can strengthen the conclusion to get that PH collapses to the second level.)</p>
<p>This sort of reasoning was implicit in several early works, including those of <a href="https://arxiv.org/abs/quant-ph/9812056">Fenner et al.</a> and <a href="https://arxiv.org/abs/quant-ph/0205133">Terhal and DiVincenzo</a>.  It was made fully explicit in <a href="http://theoryofcomputing.org/articles/v009a004/v009a004.pdf">my paper with Alex Arkhipov</a> on BosonSampling in 2011, and in the <a href="https://arxiv.org/abs/1005.1407">independent work</a> of Bremner, Jozsa, and Shepherd on the IQP model.  These works actually showed something stronger, which is that we get a collapse of PH, not merely from a fast classical algorithm to simulate <em>arbitrary</em> quantum systems, but from fast classical algorithms to simulate various special quantum systems.  In the case of BosonSampling, that special system is a collection of identical, non-interacting photons passing through a network of beamsplitters, then being measured at the very end to count the number of photons in each mode.  In the case of IQP, the special system is a collection of qubits that are prepared, subjected to some commuting Hamiltonians acting on various subsets of the qubits, and then measured.  These special systems don’t seem to be capable of universal quantum computation (or for that matter, even universal classical computation!)—and correspondingly, many of them seem easier to realize in the lab than a full universal quantum computer.</p>
<p>From an experimental standpoint, though, <em>all</em> these results are unsatisfactory, because they all talk only about the classical hardness of <em>exact</em> (or very nearly exact) sampling—and indeed, the arguments are based around the hardness of estimating just a single, exponentially-small amplitude.  But any real experiment will have tons of noise and inaccuracy, so it seems only fair to let the classical simulation be subject to serious noise and inaccuracy as well—but as soon as we do, the previous argument collapses.</p>
<p>Thus, from the very beginning, Alex Arkhipov and I took it as our “real” goal to show, under some reasonable assumption, that there’s a distribution D that a polynomial-time quantum algorithm can sample from, but such that no polynomial-time classical algorithm can sample from any distribution that’s even <em>ε-close</em> to D in variation distance.  Indeed, this goal is what led us to BosonSampling in the first place: we knew that we needed amplitudes that were not only #P-hard but “robustly” #P-hard; we knew that the <a href="https://en.wikipedia.org/wiki/Permanent">permanent</a> of an n×n matrix (at least over finite fields) was the canonical example of a “robustly” #P-hard function; and finally, we knew that systems of identical non-interacting bosons, such as photons, gave rise to amplitudes that were permanents in an extremely natural way.  The fact that photons actually exist in the physical world, and that our friends with quantum optics labs like to do experiments with them, was just a nice bonus!</p>
<p>A bit more formally, let ApproxSampBPP be the class of sampling problems for which there exists a classical algorithm that, given an input x∈{0,1}n and a parameter ε&gt;0, samples a distribution that’s at most  away from Dx in variation distance, in time polynomial in n and 1/ε.  Let ApproxSampBQP be the same except that we allow a quantum algorithm.  Then the “dream” result that we’d love to prove—both then and now—is the following.</p>
<p><strong>Strong Quantum Supremacy Conjecture.</strong>  If ApproxSampBPP = ApproxSampBQP, then the polynomial hierarchy collapses.</p>
<p>Unfortunately, Alex and I were only able to prove this conjecture assuming a further hypothesis, about the permanents of i.i.d. Gaussian matrices.</p>
<p><strong>Theorem 2 (A.-Arkhipov).</strong>  Given an n×n matrix X of independent complex Gaussian entries, each of mean 0 and variance 1, assume it’s a #P-hard problem to approximate |Per(X)|2 to within ±ε⋅n!, with probability at least 1-δ over the choice of X, in time polynomial in n, 1/ε, and 1/δ.  Then the Strong Quantum Supremacy Conjecture holds.  Indeed, more than that: in such a case, even a fast approximate classical simulation of BosonSampling, in particular, would imply P#P=BPPNP and hence a collapse of PH.</p>
<p>Alas, after some months of effort, we were unable to prove the needed #P-hardness result for Gaussian permanents, and it remains an outstanding open problem—there’s not even a consensus as to whether it should be true or false.  Note that there <em>is</em> a famous polynomial-time classical algorithm to approximate the permanents of <em>nonnegative</em> matrices, due to <a href="https://www.cc.gatech.edu/~vigoda/Permanent.pdf">Jerrum, Sinclair, and Vigoda</a>, but that algorithm breaks down for matrices with negative or complex entries.  This is once again the power of cancellations, the difference between #P and GapP.</p>
<p>Frustratingly, if we want the exact permanents of i.i.d. Gaussian matrices, we were able to prove that that’s #P-hard; and if we want the approximate permanents of arbitrary matrices, we also know that <em>that’s</em> #P-hard—it’s only when we have approximation and random inputs in the same problem that we no longer have the tools to prove #P-hardness.</p>
<p>In the meantime, one can also ask a meta-question.  How hard should it be to prove the Strong Quantum Supremacy Conjecture?  Were we right to look at slightly exotic objects, like the permanents of Gaussian matrices?  Or could Strong Quantum Supremacy have a “pure, abstract complexity theory proof”?</p>
<p>Well, one way to formalize that question is to ask whether Strong Quantum Supremacy has a <em>relativizing</em> proof, a proof that holds in the presence of an arbitrary oracle.  Alex and I explicitly raised that as an open problem in our BosonSampling paper.</p>
<p>Note that “weak” quantum supremacy—i.e., the statement that ExactSampBPP = ExactSampBQP collapses the polynomial hierarchy—has a relativizing proof, namely the proof that I sketched earlier.  All the ingredients that we used—Toda’s Theorem, Stockmeyer approximate counting, simple manipulations of quantum circuits—were relativizing ingredients.  By contrast, all the way back in 1998, <a href="https://arxiv.org/abs/cs/9811023">Fortnow and Rogers</a> proved the following.</p>
<p><strong>Theorem 3 (Fortnow and Rogers).</strong>  There exists an oracle relative to which P=BQP and yet PH is infinite.</p>
<p>In other words, if you want to prove that P=BQP collapses the polynomial hierarchy, the proof can’t be relativizing.  This theorem was subsequently <a href="https://people.cs.uchicago.edu/~fortnow/papers/obt.ps">generalized</a> in a paper by Fenner, Fortnow, Kurtz, and Li, which used concepts like “generic oracles” that seem powerful but that I don’t understand.</p>
<p>The trouble is, Fortnow and Rogers’s construction was extremely tailored to making P=BQP.  It didn’t even make PromiseBPP=PromiseBQP (that is, it allowed that quantum computers might still be stronger than classical ones for <em>promise problems</em>), let alone did it collapse quantum with classical for sampling problems.</p>
<p>We can organize the various quantum/classical collapse possibilities as follows:</p>
<p>ExactSampBPP = ExactSampBQP<br />
⇓<br />
ApproxSampBPP = ApproxSampBQP   ⇔   FBPP = FBQP<br />
⇓<br />
PromiseBPP = PromiseBQP<br />
⇓<br />
BPP = BQP</p>
<p>Here FBPP is the class of <em>relation problems</em> solvable in randomized polynomial time—that is, problems where given an input x∈{0,1}n and a parameter ε&gt;0, the goal is to produce any output in a certain set Sx, with success probability at least 1-ε, in time polynomial in n and 1/ε.  FBQP is the same thing except for quantum polynomial time.</p>
<p>The equivalence between the two equalities ApproxSampBPP = ApproxSampBQP and FBPP=FBQP is not obvious, and was the main result in my 2011 paper <a href="http://www.scottaaronson.com/papers/samprel.pdf">The Equivalence of Sampling and Searching</a>.  While it’s easy to see that ApproxSampBPP = ApproxSampBQP implies FBPP=FBQP, the opposite direction requires us to take an arbitrary sampling problem S, and define a relation problem RS that has “essentially the same difficulty” as S (in the sense that RS has an efficient classical algorithm iff S does, RS has an efficient quantum algorithm iff S does, etc).  This, in turn, we do using Kolmogorov complexity: basically, RS asks us to output a tuple of samples that have large probabilities according to the requisite probability distribution from the sampling problem; and that also, conditioned on that, are close to algorithmically random.  The key observation is that, if a probabilistic Turing machine of fixed size can solve that relation problem for arbitrarily large inputs, then it <em>must</em> be doing so by sampling from a probability distribution close in variation distance to D—since any other approach would lead to outputs that were algorithmically compressible.</p>
<p>Be that as it may, staring at the chain of implications above, a natural question is which equalities in the chain collapse the polynomial hierarchy in a relativizing way, and which equalities collapse PH (if they do) only for deeper, non-relativizing reasons.</p>
<p>This is one of the questions that Lijie Chen and I took up, and settled, in our paper <a href="http://www.scottaaronson.com/papers/quantumsupre.pdf">Complexity-Theoretic Foundations of Quantum Supremacy Experiments</a>, which was presented at this summer’s Computational Complexity Conference (CCC) in Riga.  The “main” results in our paper—or at least, the results that the physicists care about—were about how confident we can be in the classical hardness of simulating quantum sampling experiments with random circuits, such as the experiments that the Google group will hopefully be able to do with its 49-qubit device in the near future.  This involved coming up with a new hardness assumption, which was tailored to those sorts of experiments, and giving a reduction from that new assumption, and studying how far existing algorithms come toward breaking the new assumption (tl;dr: not very far).</p>
<p>But our paper also had what I think of as a “back end,” containing results mainly of interest to complexity theorists, about what kinds of quantum supremacy theorems we can and can’t hope for in principle.  When I’m giving talks about our paper to physicists, I never have time to get to this back end—it’s always just “blah, blah, we also did some stuff involving structural complexity and oracles.”  But given that a large fraction of all the people on earth who enjoy those things are probably right here in this room, in the rest of this talk, I’d like to tell you about what was in the back end.</p>
<p>The first thing there was the following result.</p>
<p><strong>Theorem 4 (A.-Chen).</strong>  There exists an oracle relative to which ApproxSampBPP = ApproxSampBQP and yet PH is infinite. In other words, any proof of the Strong Quantum Supremacy Conjecture will require non-relativizing techniques.</p>
<p>Theorem 4 represents a substantial generalization of Fortnow and Rogers’s Theorem 3, in that it makes quantum and classical equivalent not only for promise problems, but even for approximate sampling problems.  There’s also a sense in which Theorem 4 is the best possible: as we already saw, there are no oracles relative to which ExactSampBPP = ExactSampBQP and yet PH is infinite, because the opposite conclusion relativizes.</p>
<p>So how did we prove Theorem 4?  Well, we learned at this workshop that Stuart Kurtz pioneered the development of principled ways to prove oracle results just like this one, with multiple “nearly conflicting” requirements.  But, because we didn’t know that at the time, we basically just plunged in and built the oracle we wanted by hand!</p>
<p>In more detail, you can think of our oracle construction as proceeding in three steps.</p>
<ol>
<li>We throw in an oracle for a PSPACE-complete problem.  This collapses ApproxSampBPP with ApproxSampBQP, which is what we want.  Unfortunately, it also collapses the polynomial hierarchy down to P, which is <em>not</em> what we want!</li>
<li>So then we need to add in a second part of the oracle that makes PH infinite again.  From Håstad’s seminal work in the 1980s until recently, even if we just wanted any oracle that makes PH infinite, without doing anything else at the same time, we only knew how to achieve that with quite special oracles.  But in their 2015 breakthrough, <a href="https://arxiv.org/abs/1504.03398">Rossman, Servedio, and Tan</a> have shown that even a <em>random</em> oracle makes PH infinite with probability 1.  So for simplicity, we might as well take this second part of the oracle to be random.  The “only” problem is that, along with making PH infinite, a random oracle will <em>also</em> re-separate ApproxSampBPP and ApproxSampBQP (and for that matter, even ExactSampBPP and ExactSampBQP)—for example, because of the Fourier sampling task performed by the quantum circuit I showed you earlier!  So we once again seem back where we started.<br />
(To ward off confusion: ever since Fortnow and Rogers posed the problem in 1998, it remains frustratingly open whether BPP and BQP can be separated by a random oracle—that’s a problem that I and others have worked on, making <a href="http://www.scottaaronson.com/papers/struc.pdf">partial progress</a> that makes a query complexity separation look unlikely without definitively ruling one out.  But separating the <em>sampling</em> versions of BPP and BQP by a random oracle is much, much easier.)</li>
<li>So, finally, we need to take the random oracle that makes PH infinite, and “scatter its bits around randomly” in such a way that a PH machine can still find the bits, but an ApproxSampBQP machine can’t.  In other words: given our initial random oracle A, we can make a new oracle B such that B(y,r)=(1,A(y)) if r is equal to a single randomly-chosen “password” ry, depending on the query y, and B(y,r)=(0,0) otherwise.  In that case, it takes just one more existential quantifier to guess the password ry, so PH can do it, but a quantum algorithm is stuck, basically because the linearity of quantum mechanics makes the algorithm not very sensitive to tiny random changes to the oracle string (i.e., the same reason why Grover’s algorithm can’t be arbitrarily sped up).  Incidentally, the reason why the password ry needs to depend on the query y is that otherwise the input x to the quantum algorithm could hardcode a password, and thereby reveal exponentially many bits of the random oracle A.</li>
</ol>
<p>We should now check: why does the above oracle “only” collapse ApproxSampBPP and ApproxSampBQP?  Why doesn’t it also collapse ExactSampBPP and ExactSampBQP—as we know that it can’t, by our previous argument?  The answer is: because a quantum algorithm <em>does</em> have an exponentially small probability of correctly guessing a given password ry.  And that’s enough to make the distribution sampled by the quantum algorithm differ, by 1/exp(n) in variation distance, from the distribution sampled by any efficient classical simulation of the algorithm—an error that doesn’t matter for approximate sampling, but <em>does</em> matter for exact sampling.</p>
<p>Anyway, it’s then just like seven pages of formalizing the above intuitions and you’re done!</p>
<p>OK, since there seems to be time, I’d like to tell you about <em>one more</em> result from the back end of my and Lijie’s paper.</p>
<p>If we can work relative to whatever oracle A we like, then it’s easy to get quantum supremacy, and indeed BPPA≠BQPA.  We can, for example, use Simon’s problem, or Shor’s period-finding problem, or <a href="https://www.scottaaronson.com/papers/for.pdf">Forrelation</a>, or other choices of black-box problems that admit huge, provable quantum speedups.  In the unrelativized world, by contrast, it’s clear that we have to make <em>some</em> complexity assumption for quantum supremacy—even if we just want ExactSampBPP ≠ ExactSampBQP.  For if (say) P=P#P, then ExactSampBPP and ExactSampBQP would collapse as well.</p>
<p>Lijie and I were wondering: what happens if we try to “interpolate” between the relativized and unrelativized worlds?  More specifically, what happens if our algorithms are allowed to query a black box, <em>but</em> we’re promised that whatever’s inside the black box is efficiently computable (i.e., has a small circuit)?  How hard is it to separate BPP from BQP, or ApproxSampBPP from ApproxSampBQP, relative to an oracle A that’s constrained to lie in P/poly?</p>
<p>Here, we’ll start with a beautiful observation that’s implicit in <a href="http://www.cs.columbia.edu/~rocco/Public/SG_041291_2.pdf">2004 work by Servedio and Gortler</a>, as well as <a href="https://eprint.iacr.org/2012/182.pdf">2012 work by Mark Zhandry</a>.  In our formulation, this observation is as follows:</p>
<p><strong>Theorem 5.</strong>  Suppose there exist cryptographic one-way functions (even just against classical adversaries).  Then there exists an oracle A∈P/poly such that BPPA≠BQPA.</p>
<p>While we still need to make a computational hardness assumption here, to separate quantum from classical computing, the surprise is that the assumption is so much <em>weaker</em> than what we’re used to.  We don’t need to assume the hardness of factoring or discrete log—or for that matter, of <em>any</em> “structured” problem that could be a basis for, e.g., public-key cryptography.  Just a one-way function that’s hard to invert, that’s all!</p>
<p>The intuition here is really simple.  Suppose there’s a one-way function; then it’s well-known, by the HILL and GGM Theorems of classical cryptography, that we can bootstrap it to get a cryptographic <em>pseudorandom function family</em>.  This is a family of polynomial-time computable functions fs:{0,1}n→{0,1}n, parameterized by a secret seed s, such that fs can’t be distinguished from a truly random function f by any polynomial-time algorithm that’s given oracle access to the function and that doesn’t know s.  Then, as our efficiently computable oracle A that separates quantum from classical computing, we take an ensemble of functions like</p>
<p>gs,r(x) = fs(x mod r),</p>
<p>where r is an exponentially large integer that serves as a “hidden period,” and s and r are both secrets stored by the oracle that are inaccessible to the algorithm that queries it.</p>
<p>The reasoning is now as follows: certainly there’s an efficient quantum algorithm to find r, or to solve some decision problem involving r, which we can use to define a language that’s in BQPA but not in BPPA.  That algorithm is just Shor’s period-finding algorithm!  (Technically, Shor’s algorithm needs certain assumptions on the starting function fs to work—e.g., it couldn’t be a constant function—but if those assumptions aren’t satisfied, then fs wasn’t pseudorandom anyway.)  On the other hand, suppose there were an efficient classical algorithm to find the period r.  In that case, we have a dilemma on our hands: would the classical algorithm still have worked, had we replaced fs by a <em>truly</em> random function?  If so, then the classical algorithm would violate well-known lower bounds on the classical query complexity of period-finding.  But if not, then by working on pseudorandom functions but not on truly random functions, the algorithm would be <em>distinguishing</em> the two—so fs wouldn’t have been a cryptographic pseudorandom function at all, contrary to assumption!</p>
<p>This all caused Lijie and me to wonder whether Theorem 5 could be strengthened even further, so that it wouldn’t use any complexity assumption at all.  In other words, why couldn’t we just prove <em>unconditionally</em> that there’s an oracle A∈P/poly such that BPPA≠BQPA?  By comparison, it’s not hard to see that we can unconditionally construct an oracle A∈P/poly such that PA≠NPA.</p>
<p>Alas, with the following theorem, we were able to explain why BPP vs. BQP (and even ApproxSampBPP vs. ApproxSampBQP) are different, and why <em>some</em> computational assumption is still needed to separate quantum from classical, even if we’re working relative to an efficiently computable oracle.</p>
<p><strong>Theorem 6 (A.-Chen).</strong>  Suppose that, in the real world, ApproxSampBPP = ApproxSampBQP and NP⊆BPP (granted, these are big assumptions!).  Then ApproxSampBPPA = ApproxSampBQPA for all oracles A∈P/poly.</p>
<p>Taking the contrapositive, this is saying that you can’t separate ApproxSampBPP from ApproxSampBQP relative to an efficiently computable oracle, without separating <em>some</em> complexity classes in the real world.  This contrasts not only with P vs. NP, but even with ExactSampBPP vs. ExactSampBQP, which <em>can</em> be separated unconditionally relative to efficiently computable oracles.</p>
<p>The proof of Theorem 6 is intuitive and appealing.  Not surprisingly, we’re going to heavily exploit the assumptions ApproxSampBPP = ApproxSampBQP and NP⊆BPP.  Let Q be a polynomial-time quantum algorithm that queries an oracle A∈P/poly.  Then we need to simulate Q—and in particular, sample close to the same probability distribution over outputs—using a polynomial-time <em>classical</em> algorithm that queries A.</p>
<p>Let</p>
<p>$$ \sum_{x,w} \alpha_{x,w} \left|x,w\right\rangle $$</p>
<p>be the state of Q immediately before its first query to the oracle A, where x is the input to be submitted to the oracle.  Then our first task is to get a bunch of samples from the probability distribution D={|αx,w|2}x,w, or something close to D in variation distance.  But this is easy to do, using the assumption ApproxSampBPP = ApproxSampBQP.</p>
<p>Let x1,…,xk be our samples from D, marginalized to the x part.  Then next, our classical algorithm queries A on each of x1,…,xk, getting responses A(x1),…,A(xk).  The next step is to search for a function f∈P/poly—or more specifically, a function of whatever <em>fixed</em> polynomial size is relevant—that agrees with A on the sample data, i.e. such that f(xi)=A(xi) for all i∈[k].  This is where we’ll use the assumption NP⊆BPP (together, of course, with the fact that at least one such f exists, namely A itself!), to make the task of finding f efficient.  We’ll also appeal to a fundamental fact about the sample complexity of PAC-learning.  The fact is that, if we find a polynomial-size circuit f that agrees with A on a bunch of sample points drawn independently from a distribution, then f will probably agree with A on most further points drawn from the same distribution as well.</p>
<p>So, OK, we then have a pretty good “mock oracle,” f, that we can substitute for the real oracle on the first query that Q makes.  Of course f and A won’t <em>perfectly</em> agree, but the small fraction of disagreements won’t matter much, again because of the linearity of quantum mechanics (i.e., the same thing that prevents us from speeding up Grover’s algorithm arbitrarily).  So we can basically simulate Q’s first query, and now our classical simulation is good to go until Q’s <em>second</em> query!  But now you can see where this is going: we iterate the same approach, and reuse the same assumptions ApproxSampBPP = ApproxSampBQP and NP⊆BPP, to find a new “mock oracle” that lets us simulate Q’s second query, and so on until all of Q’s queries have been simulated.</p>
<p>OK, I’ll stop there.  I don’t have a clever conclusion or anything.  Thank you.</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://www.nngroup.com/online-seminars/working-observers/">New Online Seminar: Working with Observers (Wednesday, November 8, 2017 4:00 PM ET&#x2F;1:00 PM PT)</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://www.nngroup.com/feed/rss/">Nielsen Norman Group</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">ux</span>
              <span class="tag">company</span>
            </div>
            <span class="subtitle is-7">Fri Sep 01 2017 14:45:00 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p></p><p>Working with users when doing usability testing or other user research involves some real skill. Experienced facilitators will tell you, though, that users are nothing compared to the people behind the glass or on the phone – your observers.</p>

<p>This presentation looks at tips and tricks you can use to make your relationship with your observers as productive, efficient, and positive as possible. These apply whether your observers are in the room with you, in an observation room next door, or on the phone. </p>
<p></p><br /><br /><a href="/online-seminars/working-observers/">See Full Description and Pricing</a>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://feedproxy.google.com/~r/TroyHunt/~3/v5ajZ18ZbOA/">Weekly update 50</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://feeds.feedburner.com/TroyHunt">Troy Hunt</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">security</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Fri Sep 01 2017 09:48:30 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p><a href="http://www.symantec.com/ready-secure-go/it-smb/?om_ext_cid=ws_ad_TROY_DBC-Bulldog_IT-SMB_Encrypt"><strong>Presently sponsored by:</strong> Get a security solution that will keep your website up and running—and keep you sleeping soundly: Symantec Website Security. Learn how</a></p><div><p>Yep, hit right in the face with a dodgeball. There was blood. But retribution was swiftly mine as I hunted down the kids on the other team. Oh - and I also loaded 711 million records into HIBP.</p>
<p>That's the real story this week and I wanted to speak in depth about everything from where the data came from to why you can't get your password out of it to frankly, some of the kinda disappointing comments some people left. This is a very multifaceted issue and I hope I do it justice in the audio here.</p>
<p><a href="https://itunes.apple.com/au/podcast/troy-hunts-weekly-update-podcast/id1176454699">iTunes podcast</a> | <a href="https://goo.gl/app/playmusic?ibi=com.google.PlayMusic&amp;isi=691797987&amp;ius=googleplaymusic&amp;link=https://play.google.com/music/m/If3tw7npymckucxq4q76762ncny?t%3DTroy_Hunt%27s_Weekly_Update_Podcast">Google Play Music podcast</a> | <a href="http://www.omnycontent.com/d/playlist/1439345f-6152-486d-a9c2-a6bf0067f2b7/3ba9af7f-3bfb-48fd-aae7-a6bf00689c10/fde26e49-9fb8-457d-8f16-a6bf00696676/podcast.rss">RSS podcast</a></p>

References
<ol>
<li><a href="https://appsecday.com/schedule/training-hack-yourself-first-how-to-go-on-the-cyber-offence/">Last call for my Melbourne workshop!</a> (in fact, it's the last public workshop of the year so get down there if you're local)</li>
<li><a href="https://www.troyhunt.com/have-i-been-pwned-and-spam-lists-of-personal-information/">Here's why I decided to start loading spam lists into HIBP</a> (I totally get why 85% of people wanted this)</li>
<li><a href="https://www.troyhunt.com/inside-the-massive-711-million-record-onliner-spambot-dump/">711 <em>million</em> more records are now in HIBP</a> (this is a really interesting incident on many levels)</li>
<li><a href="http://www.symantec.com/ready-secure-go/bdm-ent/?om_ext_cid=ws_ad_TROY_DBC-Bulldog_IT-SMB_Speed">Symantec's Norton Secured is sponsoring my blog this week</a> (<em>epicly</em> good timing guys, biggest week EVER!)</li>
</ol>
</div>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://ronjeffries.com/articles/017-08ff/support/">Support This Site!</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="http://ronjeffries.com/feed.xml">Ron Jeffries</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">practices</span>
              <span class="tag">agile</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Fri Sep 01 2017 05:00:00 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            If you value what I do, here are some ways you can support my work.
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://www.fourmilab.ch/fourmilog/archives/2017-08/001708.html">Reading List: Drug Lord</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://www.fourmilab.ch/fourmilog/atom_10.xml">Fourmilog - Fourmilab Change Log</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Thu Aug 31 2017 23:20:23 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            
Casey, Doug and John Hunt.
<a href="http://www.amazon.com/dp/1947449079/?tag=fourmilabwwwfour" target="Amazon_Fourmilab">Drug Lord</a>.
Charlottesville, VA: HighGround Books, 2017.
ISBN 978-1-947449-07-7.

This is the second novel in the authors' “High Ground”
series, chronicling the exploits of Charles Knight, an entrepreneur
and adventurer determined to live his life according to his own
moral code, constrained as little as possible by the rules and
regulations of coercive and corrupt governments.  The first
novel, <a href="http://www.fourmilab.ch/documents/reading_list/?book=1067" target="_top">Speculator</a> (<a href="/documents/reading_list/?month=2016-10" target="Fourmilab_readingListAux">October 2016</a>),
follows Charles's adventures in Africa as an investor in a
junior gold exploration company which just might have made
the discovery of the century, and in the financial markets as he
seeks to profit from what he's learned digging into the details.
Charles comes onto the radar of ambitious government agents seeking
to advance their careers by collecting his scalp.
<p>
Charles ends up escaping with his freedom and ethics intact, but
with much of his fortune forfeit.  He decides he's had enough of
“the land of the free” and sets out on his sailboat
to explore the world and sample the pleasures and opportunities
it holds for one who thinks for himself.
Having survived several attempts on his life and prevented a war
in Africa in the previous novel, seven years later he returns to a
<em>really dangerous</em> place, Washington DC, populated by
the Morlocks of Mordor.
</p><p>
Charles has an idea for a new business.  The crony capitalism of
the U.S. pharmaceutical-regulatory complex has inflated the price
of widely-used prescription drugs to many times that paid
outside the U.S., where these drugs, whose patents have expired
under legal regimes less easily manipulated than that of the U.S.,
are manufactured in a chemically-identical form by thoroughly
professional generic drug producers.  Charles understands, as fully
as any engineer, that wherever there is nonlinearity the possibility
for gain exists, and when that nonlinearity is the result of the
action of coercive government, the potential profits from circumventing
its grasp on the throat of the free market can be very large,
indeed.
</p><p>
When Charles's boat docked in the U.S., he had an undeclared cargo:
a large number of those little blue pills much in demand by men
of a certain age, purchased for pennies from a factory in India through
a cut-out in Africa he met on his previous adventure.  He has the
product, and a supplier able to obtain much more.  Now, all he needs
is distribution.  He must venture into the dark underside of DC to
make the connections that can get the product to the customers, and
persuade potential partners that they can make much more and far more
safely by distributing his products (which don't fall under the
purview of the Drug Enforcement Agency, and to which local cops not only
don't pay much attention, but may be potential customers).
</p><p>
Meanwhile, Charles's uncle Maurice, who has been managing what was
left of his fortune during his absence, has made an investment in
a start-up pharmaceutical company, Visioryme, whose first product,
VR-210, or Sybillene, is threading its way through the FDA regulatory
gauntlet toward approval for use as an antidepressant.  Sybillene works
through a novel neurochemical pathway, and promises to be an effective
treatment for clinical depression while avoiding the many deleterious
side effects of other drugs.  In fact, Sybillene doesn't appear to
have any side effects at all—or hardly any—there's that
one curious thing that happened in animal testing, but not wishing
to commit corporate seppuku, Visioryme hasn't mentioned it to the
regulators or even their major investor, Charles.
</p><p>
Charles pursues his two pharmaceutical ventures in parallel: one in the
DC ghetto and Africa; the other in the tidy suburban office park
where Visioryme is headquartered.  The first business begins to
prosper, and Charles must turn his ingenuity to solving the problems
attendant to any burgeoning enterprise: supply, transportation,
relations with competitors (who, in this sector of the economy,
not only are often armed but inclined to shoot first), expanding
the product offerings, growing the distribution channels, and
dealing with all of the money that's coming in, entirely in cash,
without coming onto the radar of any of the organs of the slavers
and their pervasive snooper-state.
</p><p>
Meanwhile, Sybillene finally obtains FDA approval, and Visioryme begins
to take off and ramp up production.  Charles's connections in Africa
help the company obtain the supplies of bamboo required in production
of the drug.  It seems like he now has two successful ventures, on
the dark and light sides, respectively, of the pharmaceutical
business (which is dark and which is light depending on your view
of the FDA).
</p><p>
Then, curious reports start to come in about doctors prescribing
Sybillene off-label in large doses to their well-heeled
patients.  Off-label prescription is completely legal and not
uncommon, but one wonders what's going on.  Then there's the talk
Charles is picking up from his other venture of demand for
a new drug on the street: Sybillene, which goes under names such
as Fey, Vatic, Augur, Covfefe, and most commonly, Naked Emperor.
Charles's lead distributor reports, “It helps people see
lies for what they are, and liars too.  I dunno.  I never tried
it.  Lots of people are asking though.  Society types.  Lawyers,
businessmen, doctors, even cops.”  It appears that
Sybillene, or Naked Emperor, taken in a high dose, is a powerful
nootropic which doesn't so much increase intelligence as, the
opposite of most psychoactive drugs, allows the user to think
<em>more clearly</em>, and see through the deception that
pollutes the intellectual landscape of a modern,
“developed”, society.
</p><p>
In that fœtid city by the Potomac, the threat posed by such
clear thinking dwarfs that of other “controlled
substances” which merely turn their users into zombies.
Those atop an empire built on deceit, deficits, and debt cannot
run the risk of a growing fraction of the population beginning to
see through the funny money, Ponzi financing, Potemkin
military, manipulation of public opinion, erosion of
the natural rights of citizens, and the sham which is replacing
the last vestiges of consensual government.  Perforce, Sybillene must
become Public Enemy Number One, and if a bit of lying and
even murder is required, well, that's the price of preserving
the government's ability to lie and murder.
</p><p>
Suddenly, Charles is involved in <em>two</em> illegal pharmaceutical
ventures.  As any wise entrepreneur would immediately ask himself,
“might there be synergies?”
</p><p>
Thus begins a compelling, instructive, and inspiring tale of
entrepreneurship and morality confronted with dark forces constrained
by no limits whatsoever.  We encounter friends and foes from the first
novel, as once again Charles finds himself on point position
defending those in the enterprises he has created.  As I said in my
review of Speculator, this book reminds me of
Ayn Rand's <a href="http://www.amazon.com/dp/0451191153/?tag=fourmilabwwwfour" target="Amazon_Fourmilab">The Fountainhead</a>,
but it is even more effective because Charles Knight is not a
super-hero but rather a person with a strong sense of right and
wrong who is making up his life as he goes along and learning
from the experiences he has: good and bad, success and failure.
Charles Knight, even without Naked Emperor, has that gift of seeing
things precisely as they are, unobscured by the fog, cant, spin, and
lies which are the principal products of the city in which it is set.
</p><p>
These novels are not just page-turning thrillers, they're
simultaneously an introductory course in becoming an international
man (or woman), transcending the lies of the increasingly obsolescent
nation-state, and finding the liberty that comes from seizing control
of one's own destiny.  They may be the most powerful fictional
recruiting tool for the libertarian and anarcho-capitalist world
view since the works of Ayn Rand and
<a href="http://www.amazon.com/gp/search/ref=sr_adv_b/?search-alias=stripbooks&amp;unfiltered=1&amp;field-author=l+neil+smith&amp;sort=relevanceexprank&amp;Adv-Srch-Books-Submit.x=0&amp;Adv-Srch-Books-Submit.y=0&amp;tag=fourmilabwwwfour&amp;tag=fourmilabwwwfour" target="Amazon_Fourmilab">L. Neil Smith</a>.
Speculator was my fiction
<a href="/fourmilog/archives/2016-12/001651.html" target="Fourmilab_readingListAux">book of the year</a>
for 2016, and this sequel is in the running for 2017.
</p>

          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://apenwarr.ca/log/?m=201708#10">The world in which IPv6 was a good design</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="http://apenwarr.ca/log/rss.php">apenwarr</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">networking</span>
              <span class="tag">practices</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Thu Aug 31 2017 19:33:24 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>Last November I went to an IETF meeting for the first time.  The IETF is an
interesting place; it seems to be about 1/3 maintenance grunt work, 1/3
extending existing stuff, and 1/3 blue sky insanity.  I attended mostly
because I wanted to see how people would react to <a href="https://datatracker.ietf.org/meeting/97/materials/slides-97-iccrg-bbr-congestion-control">TCP BBR, which was being
presented there for the first time</a>.  (Answer: mostly positively, but
with suspicion.  It kinda seemed too good to be true.)</p>
<p>Anyway, the IETF meetings contain lots and lots of presentations about IPv6,
the thing that was supposed to replace IPv4, which is what the Internet runs
on.  (Some would say IPv4 is already being replaced; some would say it has
already happened.) Along with those presentations about IPv6, there were
lots of people who think it's great, the greatest thing ever, and they're
pretty sure it will finally catch on Any Day Now, and IPv4 is just a giant
pile of hacks that really needs to die so that the Internet can be elegant
again.</p>
<p>I thought this would be a great chance to really try to figure out what was
going on.  Why is IPv6 such a complicated mess compared to IPv4?  Wouldn't
it be better if it had just been <a href="https://tools.ietf.org/html/rfc1710">IPv4 with more address bits</a>?  But it's not,
oh goodness, is it ever not.  So I started asking around.  Here's what I found.</p>
<p><b>Buses ruined everything</b></p>
<p>Once upon a time, there was the telephone network, which used physical
circuit switching.  Essentially, that meant moving connectors around so that
your phone connection was literally just a very long wire (&quot;<a href="https://en.wikipedia.org/wiki/OSI_model">OSI layer</a> 1&quot;).  A
&quot;leased line&quot; was a very long wire that you leased from the phone
company.  You would put bits in one end of the wire, and they'd come out the
other end, a fixed amount of time later.  You didn't need addresses because
there was exactly one machine at each end.</p>
<p>Eventually the phone company optimized that a bit.  Time-division
multiplexing (TDM) and &quot;virtual circuit switching&quot; was born.  The phone
company could transparently take the bits at a slower bit rate from multiple
lines, group them together with multiplexers and demultiplexers, and let
them pass through the middle of the phone system using fewer wires than
before.  Making that work was a little complicated, but as far as we
modem users were concerned, you still put bits in one end and they
came out the other end.  No addresses needed.</p>
<p>The Internet (not called the Internet at the time) was built on top of these
circuits.  You had a bunch of wires that you could put bits
into and have them come out the other side.  If one computer had two or
three interfaces, then it could, if given the right instructions, forward
bits from one line to another, and you could do something a lot more
efficient than a separate line between each pair of computers.  And so IP
addresses (&quot;layer 3&quot;), subnets, and routing were born.  Even then, with
these point-to-point links, you didn't need MAC addresses, because once a
packet went into the wire, there was only one place it could come out.  You
used IP addresses to decide where it should go after that.</p>
<p>Meanwhile, LANs got invented as an alternative.  If you wanted to connect
computers (or terminals and a mainframe) together at your local site, it was
pretty inconvenient to need multiple interfaces, one for each wire to each
satellite computer, arranged in a star configuration.  To save on
electronics, people wanted to have a &quot;bus&quot; network (also known as a
&quot;broadcast domain,&quot; a name that will be important later) where multiple
stations could just be plugged into a single wire, and talk to any other
station plugged into the same wire.  These were not the same people as the
ones building the Internet, so they didn't use IP addresses for this.  They
all invented their own scheme (&quot;layer 2&quot;).</p>
<p>One of the early local bus networks was arcnet, which is dear to my heart (I
wrote the first Linux arcnet driver <a href="http://apenwarr.ca/arcnet/howto/intro.html">and arcnet poetry</a> way
back in the 1990s, long after arcnet was obsolete).  Arcnet layer 2
addresses were very simplistic: just 8 bits, set by jumpers or DIP switches
on the back of the network card.  As the network owner, it was your job to
configure the addresses and make sure you didn't have any duplicates, or all
heck would ensue.  This was kind of a pain, but arcnet networks were usually
pretty small, so it was only <i>kind</i> of a pain.</p>
<p>A few years later, ethernet came along and solved that problem once and for
all, by using many more bits (48, in fact) in the layer 2 address.  That's
enough bits that you can assign a different (sharded-sequential) address to
every device that has ever been manufactured, and not have any overlaps. 
And that's exactly what they did!  Thus the ethernet MAC address was born.</p>
<p>Various LAN technologies came and went, including one of my favourites, IPX
(Internetwork Packet Exchange, though it had nothing to do with the &quot;real&quot;
Internet) and Netware, which worked great as long as all the clients and
servers were on a single bus network.  You never had to configure any
addresses, ever.  It was beautiful, and reliable, and worked.  The golden
age of networking, basically.</p>
<p>Of course, someone had to ruin it: big company/university networks.  They
wanted to have so many computers that sharing 10 Mbps of a single bus
network between them all became a huge bottleneck, so they needed a way to
have multiple buses, and then interconnect - &quot;internetwork,&quot; if you will -
those buses together.  You're probably thinking, of course! Use the Internet
Protocol for that, right?  Ha ha, no.  The Internet protocol, still not
called that, wasn't mature or popular back then, and nobody took it
seriously.  Netware-over-IPX (and the many other LAN protocols at the time)
were serious business, so as serious businesses do, they invented their own
thing(s) to extend the already-popular thing, ethernet.  Devices on
ethernet already had addresses, MAC addresses, which were about the only
thing the various LAN protocol people could agree on, so they decided to use
ethernet addresses as the keys for their routing mechanisms.  (Actually they
called it bridging and switching instead of routing.)</p>
<p>The problem with ethernet addresses is they're assigned sequentially at the
factory, so they can't be hierarchical.  That means the &quot;bridging table&quot; is
not as nice as a modern IP routing table, which can talk about the route for
a whole subnet at a time.  In order to do efficient bridging, you had to
remember which network bus each MAC address could be found on.  And humans
didn't want to configure each of those by hand, so it needed to figure
itself out automatically.  If you had a complex internetwork of bridges,
this could get a little complicated.  As I understand it, that's what led to
the <a href="http://etherealmind.com/algorhyme-radia-perlman/">spanning tree
poem</a>, and I think I'll just leave it at that.  Poetry is very important
in networking.</p>
<p>Anyway, it mostly worked, but it was a bit of a mess, and you got broadcast floods
every now and then, and the routes weren't always optimal, and it was pretty
much impossible to debug.  (You definitely couldn't write something like
traceroute for bridging, because none of the tools you need to make it work
- such as the ability for an intermediate bridge to even have an address -
exist in plain ethernet.)</p>
<p>On the other hand, all these bridges were hardware-optimized.  The whole
system was invented by hardware people, basically, as a way of fooling the
software, which had no idea about multiple buses and bridging between them,
into working better on large networks.  Hardware bridging means the bridging
could go really really fast - as fast as the ethernet could go.  Nowadays
that doesn't sound very special, but at the time, it was a big deal. 
Ethernet was 10 Mbps, because you could maybe saturate it by putting a bunch
of computers on the network all at once, not because any one computer could
saturate 10 Mbps.  That was crazy talk.</p>
<p>Anyway, the point is, bridging was a mess, and impossible to debug, but it
was fast.</p>
<p><b>Internet over buses</b></p>
<p>While all that was happening, those Internet people were getting busy, and
were of course not blind to the invention of cool cheap LAN technologies.  I
think it might have been around this time that the ARPANET got actually
renamed to the Internet, but I'm not sure.  Let's say it was, because the
story is better if I sound confident.</p>
<p>At some point, things progressed from connecting individual Internet
computers over point-to-point long distance links, to the desire to connect
whole LANs together, over point-to-point links.  Basically, you wanted a
long-distance bridge.</p>
<p>You might be thinking, hey, no big deal, why not just build a long distance
bridge and be done with it?  Sounds good, doesn't work.  I won't go into the
details right now, but basically the problem is <a href="https://en.wikipedia.org/wiki/Network_congestion">congestion
control</a>.  The deep dark secret of ethernet bridging is that it assumes
all your links are about the same speed, and/or completely uncongested,
because they have no way to slow down.  You just blast data as fast as you
can, and expect it to arrive.  But when your ethernet is 10 Mbps and your
point-to-point link is 0.128 Mbps, that's completely hopeless.  Separately,
the idea of figuring out your routes by flooding all the links to see which
one is right - this is the actual way bridging typically works - is hugely
wasteful for slow links.  And sub-optimal routing, an annoyance on local
networks with low latency and high throughput, is nasty on slow, expensive
long-distance links.  It just doesn't scale.</p>
<p>Luckily, those Internet people (if it was called the Internet yet) had been
working on that exact set of problems.  If we could just use Internet stuff
to connect ethernet buses together, we'd be in great shape.</p>
<p>And so they designed a &quot;frame format&quot; for Internet packets over ethernet
(and arcnet, for that matter, and every other kind of LAN).</p>
<p>And that's when everything started to go wrong.</p>
<p>The first problem that needed solving was that now, when you put an Internet
packet onto a wire, it was no longer clear which machine was supposed to
&quot;hear&quot; it and maybe forward it along.  If multiple Internet routers were on
the same ethernet segment, you couldn't have them all picking it up and
trying to forward it; that way lies packet storms and routing loops.  No,
you had to choose <i>which</i> router on the ethernet bus is supposed to
pick it up.  We can't just use the IP destination field for that, because
we're already using that for the <i>final</i> destination, not the router
destination.  Instead, we identify the desired router using its MAC address
in the ethernet frame.</p>
<p>So basically, to set up your local IP routing table, you want to be able to
say something like, &quot;send packets to IP address 10.1.1.1 via the router at
MAC address 11:22:33:44:55:66.&quot;  That's the actual thing you want to
express.  This is important!  Your destination is an IP address, but your
router is a MAC address.  But if you've ever configured a routing table, you
might have noticed that nobody writes it like that.  Instead, because the
writers of your operating system's TCP/IP stack are stubborn, you write
something like &quot;send packets to IP address 10.1.1.1 via the router at IP
address 192.168.1.1.&quot;</p>
<p>In truth, that really is just complicating things.  Now your operating
system has to first look up the ethernet address of 192.168.1.1, find out
it's 11:22:33:44:55:66, and finally generate a packet with destination
ethernet address 11:22:33:44:55:66 and destination IP address 10.1.1.1. 
192.168.1.1 shows up nowhere in the packet; it's just an abstraction at the
human level.</p>
<p>To do that pointless intermediate step, you need to add ARP (address
resolution protocol), a simple non-IP protocol whose job it is to convert IP
addresses to ethernet addresses.  It does this by broadcasting to everyone
on the local ethernet bus, asking them all to answer if they own that
particular IP address.  If you have bridges, they all have to forward all
the ARP packets to all their interfaces, because they're ethernet broadcast
packets, and that's what broadcasting means.  On a big, busy ethernet with
lots of interconnected LANs, excessive broadcasts start becoming one of your
biggest nightmares.  It's especially bad on wifi.  As time went on, people
started making bridges/switches with special hacks to avoid forwarding ARP
as far as it's technically supposed to go, to try to cut down on this
problem.  Some devices (especially wifi access points) just make fake ARP
answers to try to help.  But doing any of that is a hack, albeit sometimes a
necessary hack.</p>
<p><b>Death by legacy</b></p>
<p>Time passed.  Eventually (and this actually took quite a while), people
pretty much stopped using non-IP protocols on ethernet at all.  So basically
all networks became a physical wire (layer 1), with multiple stations on a
bus (layer 2), with multiple buses connected over bridges (gotcha!  still
layer 2!), and those inter-buses connected over IP routers (layer 3).</p>
<p>After a while, people got tired of manually configuring IP addresses, arcnet
style, and wanted them to auto-configure, ethernet style, except it was too
late to literally do it ethernet style, because a) the devices had already
been manufactured with ethernet addresses, not IP addresses, and b) IP
addresses were only 32 bits, which is not enough to just manufacture them
forever with no overlaps, and c) just assigning IP addresses sequentially
instead of using subnets would bring us back to square one: it would just be
ethernet over again, and we already have ethernet.</p>
<p>So that's where bootp
and DHCP came from.  Those protocols, by the way, are special kinda like ARP
is special (except they pretend not to be special, by technically being IP
packets).  They have to be special, because an IP node has to be able to
transmit them before it has an IP address, which is of course impossible, so
it just fills the IP headers with essentially nonsense (albeit nonsense
specified by an RFC), so the headers might as well have been left out.  (You
know these &quot;IP&quot; headers are nonsense because the DHCP server has to open a raw
socket and fill them in by hand; the kernel IP layer can't do it.)  But
nobody would feel nice if they were inventing a whole new protocol that
wasn't IP, so they pretended it was IP, and then they felt nice.  Well, as
nice as one can feel when one is inventing DHCP.</p>
<p>Anyway, I digress.
The salient detail here is that unlike real IP services, bootp and DHCP need to
know about ethernet addresses, because after all, it's their job to hear
your ethernet address and assign you an IP address to go with it.  They're
basically the reverse of ARP, except we can't say that, because there's a
protocol called RARP that is literally the reverse of ARP.  Actually, RARP
worked quite fine and did the same thing as bootp and DHCP while being
much simpler, but we don't talk about that.</p>
<p>The point of all this is that ethernet and IP were getting further and
further intertwined.  They're nowadays almost inseparable.  It's hard to
imagine a network interface (except ppp0) without a 48-bit MAC address, and it's hard to imagine
that network interface working without an IP address.  You write your IP
routing table using IP addresses, but of course you know you're lying when
you name the router by IP address; you're just indirectly saying that you
want to route via a MAC address.  And you have ARP, which gets bridged but
not really, and DHCP, which is an IP packet but is really an ethernet
protocol, and so on.</p>
<p>Moreover, we still have both bridging and routing, and they both get more
and more complicated as the LANs and the Internet get more and more
complicated, respectively.  Bridging is still, mostly, hardware based and
defined by IEEE, the people who control the ethernet standards.  Routing is
still, mostly, software based and defined by the IETF, the people who
control the Internet standards.  Both groups still try to pretend the other
group doesn't exist.  Network operators basically choose bridging vs routing
based on how fast they want it to go and how much they hate configuring DHCP
servers, which they really hate very much, which means they use bridging as
much as possible and routing when they have to.</p>
<p>In fact, bridging has gotten so completely out of control that people
decided to extract the layer 2 bridging decisions out completely to a higher
level (with configuration exchanged between bridges using a protocol layered
over IP, of course!) so it can be centrally managed.  That's called
software-defined networking (SDN).  It helps a lot, compared to letting your
switches and bridges just do whatever they want, but it's also fundamentally
silly, because you know what's software defined networking?  IP.  It is
literally and has always been the software-defined network you use for
interconnecting networks that have gotten too big.  But the problem is, IPv4
was initially too hard to hardware accelerate, and anyway, it didn't get
hardware accelerated, and configuring DHCP really is a huge pain, so network
operators just learned how to bridge bigger and bigger things.  And nowadays
big data centers are basically just SDNed, and you might as well not be
using IP in the data center at all, because nobody's routing the packets. 
It's all just one big virtual bus network.</p>
<p>It is, in short, a mess.</p>
<p><b>Now forget I said all that...</b></p>
<p>Great story, right?  Right.  Now pretend none of that happened, and we're
back in the early 1990s, when most of that had in fact already happened,
but people at the IETF were anyway pretending that it hadn't happened and
that the &quot;upcoming&quot; disaster could all be avoided.  This is the good part!</p>
<p>There's one thing I forgot to mention in that big long story above:
somewhere in that whole chain of events, <b>we completely stopped using bus
networks</b>.  Ethernet is not actually a bus anymore.  It just
<i>pretends</i> to be a bus.  Basically, we couldn't get ethernet's famous
<a href="https://en.wikipedia.org/wiki/Carrier-sense_multiple_access_with_collision_detection">CSMA/CD</a>
to keep working as speeds increased, so we went back to the good old star
topology.  We run bundles of cables from the switch, so that we can run one
cable from each station all the way back to the center point.  Walls and
ceilings and floors are filled with big, thick, expensive bundles of
ethernet, because we couldn't figure out how to make buses work well...  at
layer 1.  It's kinda funny actually when you think about it.  If you find
sad things funny.</p>
<p>In fact, in a bonus fit of insanity, even wifi - the ultimate bus network,
right, where literally everybody is sharing the same open-air &quot;bus&quot; - we
almost universally use wifi in a mode, called &quot;infrastructure mode,&quot; which
simulates a giant star topology.  If you have two wifi stations
connected to the same access point, they don't talk to each other directly,
even when they can hear each other just fine. 
They send a packet to the access point, but addressed to the MAC address of
the other node.  The access point then bounces it back out to the
destination node.</p>
<p>HOLD THE HORSES LET ME JUST REVIEW THAT FOR YOU.  There's a little catch
there.  When node X wants to send to Internet node Z, via IP router Y, via
wifi access point A, what does the packet look like?  Just to draw a
picture, here's what we want to happen:</p>
<pre><code>X -&gt; [wifi] -&gt; A -&gt; [wifi] -&gt; Y -&gt; [internet] -&gt; Z
</code></pre>
<p>Z is the IP destination, so obviously the IP destination field has to be Z. 
Y is the router, which we learned above that we specify by using its
ethernet MAC address in the ethernet destination field.  But in wifi, X
can't just send out a packet to Y, for various reasons (including that they
don't know each other's WPA2 encryption keys).  We have to send to A.  Where do
we put A's address, you might ask?</p>
<p>No problem!  802.11 has a thing called 3-address mode.  They add a <em>third</em>
ethernet MAC address to every frame, so they can talk about the real
ethernet destination, and the intermediate ethernet destination.  On top of
that, there are bit fields called &quot;to-AP&quot; and &quot;from-AP,&quot; which tell you if
the packet is going from a station to an AP, or from an AP to a station,
respectively.  But actually they can both be true at the same time, because
that's how you make wifi repeaters (APs send packets to APs).</p>
<p>Speaking of wifi repeaters!  If A is a repeater, it has to send back to the
base station, B, along the way, which looks like this:</p>
<pre><code>X -&gt; [wifi] -&gt; A -&gt; [wifi-repeater] -&gt; B -&gt; [wifi] -&gt; Y -&gt; [internet] -&gt; Z
</code></pre>
<p>X-&gt;A uses three-address mode, but A-&gt;B has a problem: the ethernet source
address is X, and the ethernet destination address is Y, but the packet on
the air is actually being sent from A to B; X and Y aren't involved at all. 
Suffice it to say that there's a thing called 4-address mode, and it works
pretty much like you think.</p>
<p>(In 802.11s mesh networks, there's a 6-address mode, and that's about where
I gave up trying to understand.)</p>
<p><b>Avery, I was promised IPv6, and you haven't even mentioned IPv6</b></p>
<p>Oh, oops. This post went a bit off the rails, didn't it?</p>
<p>Here's the point of the whole thing.  The IETF people, when they were
thinking about IPv6, saw this mess getting made - and maybe predicted some
of the additional mess that would happen, though I doubt they could have
predicted SDN and wifi repeater modes - and they said, hey wait a minute,
stop right there.  We don't need any of this crap!  What if instead the
world worked like this?</p>
<ul>
<li>No more physical bus networks (already done!)</li>
<li>No more layer 2 internetworks (that's what layer 3 is for)</li>
<li>No more broadcasts (layer 2 is always point-to-point, so where would you
   send the broadcast to?  replace it with multicast instead)</li>
<li>No more MAC addresses (on a point-to-point network, it's obvious who the
   sender and receiver are, and you can do multicast using IP addresses)</li>
<li>No more ARP and DHCP (no MAC addresses, no so mapping IP addresses to MAC
   addresses)</li>
<li>No more complexity in IP headers (so you can hardware accelerate IP
   routing)</li>
<li>No more IP address shortages (so we can go back to routing big subnets
   again)</li>
<li>No more manual IP address configuration except at the core (and there are
   so many IP addresses that we can recursively hand out subnets down the
   tree from there)</li>
</ul>
<p>Imagine that we lived in such a world: wifi repeaters would just be IPv6
routers.  So would wifi access points.  So would ethernet switches.  So
would SDN.  ARP storms would be gone.  &quot;IGMP snooping bridges&quot; would be
gone.  Bridging loops would be gone.  Every routing problem would be
traceroute-able.  And best of all, we could drop 12 bytes (source/dest ethernet
addresses) from every ethernet packet, and 18 bytes (source/dest/AP
addresses) from every wifi packet.  Sure, IPv6 adds an extra 24 bytes of
address (vs IPv4), but you're dropping 12 bytes of ethernet, so the added
overhead is only 12 bytes - pretty comparable to using two 64-bit IP addresses
but having to keep the ethernet header.  The idea that we could someday drop
ethernet addresses helped to justify the oversized IPv6 addresses.</p>
<p>It would have been beautiful.  Except for one problem: it never happened.</p>
<p><b>Requiem for a dream</b></p>
<p>One person at work put it best: &quot;layers are only ever added, never removed.&quot;</p>
<p>All this wonderfulness depended on the ability to start over and throw away
the legacy cruft we had built up.  And that is, unfortunately, pretty much
impossible.  Even if IPv6 hits 99% penetration, that doesn't mean we'll be
rid of IPv4.  And if we're not rid of IPv4, we won't be rid of ethernet
addresses, or wifi addresses.  And if we have to keep the IEEE 802.3 and
802.11 framing standards, we're never going to save those bytes.  So we will
always need the &quot;IPv6 neighbour discovery&quot; protocol, which is just a
more complicated ARP.  Even though we no longer have bus networks, we'll
always need some kind of simulator for broadcasts, because that's how ARP
works.  We'll need to keep running a local DHCP server at home so that our
obsolete IPv4 light bulbs keep working.  We'll keep needing NAT so that our
obsolete IPv4 light bulbs can keep reaching the Internet.</p>
<p>And that's not the worst of it. The worst of it is we still need the
infinite abomination that is layer 2 bridging, because of one more mistake
the IPv6 team <i>forgot to fix</i>.  Unfortunately, while they were
blue-skying IPv6 back in the 1990s, they neglected to solve the &quot;mobile IP&quot;
problem.  As I understand it, the idea was to get IPv6 deployed first - it
should only take a few years - and then work on it after IPv4 and MAC
addresses had been eliminated, at which time it should be much easier to
solve, and meanwhile, nobody really has a &quot;mobile IP&quot; device yet anyway.  I
mean, what would that even mean, like carrying your laptop around and
plugging into a series of one ethernet port after another while you ftp a
file?  Sounds dumb.</p>
<p><b>The killer app: mobile IP</b></p>
<p>Of course, with a couple more decades of history behind us, now we know a
few use cases for carrying around a computer - your phone - and letting it
plug into one <strike>ethernet port</strike> wireless access point
after another.  We do it all the time.  And with LTE, it even mostly works! 
With wifi, it works sometimes.  Good, right?</p>
<p>Not really, because of the Internet's secret shame: all that stuff only
works because of layer 2 bridging.  Internet routing can't handle mobility -
at all.  If you move around on an IP network, your IP address changes, and
that breaks any connections you have open.</p>
<p>Corporate wifi networks fake it for you, bridging their whole LAN together
at layer 2, so that the giant central DHCP server always hands you the same
IP address no matter which corporate wifi access point you join, and then
gets your packets to you, with at most a few seconds of confusion while the
bridge reconfigures.  Those newfangled home wifi systems with multiple
extenders/repeaters do the same trick.  But if you switch from one wifi network to
another as you walk down the street - like if there's a &quot;Public Wifi&quot;
service in a series of stores - well, too bad.  Each of those gives you a
new IP address, and each time your IP address changes, you kill all your
connections.</p>
<p>LTE tries even harder.  You keep your IP address (usually an IPv6 address in
the case of mobile networks), even if you travel miles and miles and hop
between numerous cell towers.  How?  Well... they typically just tunnel all
your traffic back to a central location, where it all gets bridged together
(albeit with lots of firewalling) into one super-gigantic virtual layer 2
LAN.  And your connections keep going.  At the expense of a ton of
complexity, and a truly embarrassing amount of extra latency, which they
would really like to fix, but it's almost impossible.</p>
<p><b>Making mobile IP actually work1</b></p>
<p>So okay, this has been a long story, but I managed to extract it from those
IETF people eventually.  When we got to this point - the problem of mobile
IP - I couldn't help but ask.  What went wrong?  Why can't we make it work?</p>
<p>The answer, it turns out, is surprisingly simple.  The great design flaw was
in how the famous &quot;4-tuple&quot; (source ip, source port, destination ip,
destination port) was defined.  We use the 4-tuple to identify a given TCP
or UDP session; if a packet has those four fields the same, then it belongs
to a given session, and we can deliver it to whatever socket is handling that
session.  But the 4-tuple crosses two layers: internetwork (layer 3) and
transport (layer 4).  If, instead, we had identified sessions using
<i>only</i> layer 4 data, then mobile IP would have worked perfectly.</p>
<p>Let's do a quick example.  X port 1111 is talking to Y port 80, so it sends
a packet with 4-tuple (X,1111,Y,80).  The response comes back with
(Y,80,X,1111), and the kernel delivers it to the socket that generated the
original packet.  When X sends more packets tagged (X,1111,Y,80), then Y
delivers them all to the same server socket, and so on.</p>
<p>Then, if X hops IP addresses, it gets a new name, say Q.  Now it'll start
sending packets with (Q,1111,Y,80).  Y has no idea what that means, and
throws it away.  Meanwhile, if Y sends packets tagged (Y,80,X,1111), they
get lost, because there is no longer an X to receive them.</p>
<p>Imagine now that we tagged sockets without reference to their IP address. 
For that to work, we'd need much bigger port numbers (which are currently 16
bits).  Let's make them, say, 128 or 256 bits, some kind of unique hash.</p>
<p>Now X sends out packets to Y with tag (uuid,80).  Note, the packets
themselves still contain the (X,Y) addressing information, down at layer 3
- that's how they get routed to the right machine in the first place. 
But the kernel doesn't <i>use</i> the layer 3 information to decide which
socket to deliver to; it just uses the uuid.  The destination port (80 in
this case) is only needed to initiate a new session, to identify what
service you want to connect to, and can be ignored or left out after that.</p>
<p>For the return direction, Y's kernel caches the fact that packets for (uuid)
go to IP address X, which is the address it most recently received (uuid)
packets from.</p>
<p>Now imagine that X changes addresses to Q.  It still sends out packets
tagged with (uuid,80), to IP address Y, but now those packets come
from address Q.  On machine
Y, it receives the packet and matches it to the socket associated with
(uuid), notes that the packets for that socket are now coming from address
Q, and updates its cache.  Its return packets can now be sent, tagged as
(uuid), back to Q instead of X.  Everything works! (Modulo some care
to prevent connection hijacking by impostors.2)</p>
<p>There's only one catch: that's not how UDP and TCP work, and it's too late
to update them.  Updating UDP and TCP would be like updating IPv4 to IPv6; a
project that sounded simple, back in the 1990s, but decades later, is less
than half accomplished (and the first half was the easy part; the long tail
is much harder).</p>
<p>The positive news is we may be able to hack around it with yet another
layering violation.  If we throw away TCP - it's getting rather old anyway -
and instead use QUIC over UDP, then we can just stop using the UDP 4-tuple
as a connection identifier at all.  Instead, if the UDP port number is
the &quot;special mobility layer&quot; port, we unwrap the content, which can be
another packet with a proper uuid tag, match it to the right session, and
deliver those packets to the right socket.</p>
<p>There's even more good news: the experimental QUIC protocol already, at
least in theory, has the right packet structure to work like this.  It turns
out you need unique session identifiers (keys) anyhow if you want to use stateless
packet encryption and authentication, which QUIC does.  So, perhaps with not
much work, QUIC could support transparent roaming.  What a world that would
be!</p>
<p>At that point, all we'd have to do is eliminate all remaining UDP and TCP
from the Internet, and then we would definitely not need layer 2
bridging anymore, for real this time, and then we could get rid of
broadcasts and MAC addresses and SDN and DHCP and all that stuff.</p>
<p>And then the Internet would be elegant again.</p>
<p><b>1 Edit 2017-08-16:</b> It turns out that nothing in this
section requires IPv6.  It would work fine with IPv4 and NAT, even roaming
across multiple NATs.</p>
<p><b>2 Edit 2017-08-15:</b> Some people asked what &quot;some care to
prevent connection hijacking&quot; might look like.  There are various ways to do
it, but the simplest would be to do something like the SYN-ACK-SYNACK
exchange TCP does at connection startup.  If Y just trusts the first packet
from the new host Q, then it's too easy for any attacker to take over the
X-&gt;Y connection by simply sending a packet to Y from anywhere on the
Internet.  (Although it's a bit hard to guess which 256-bit uuid to fill
in.) But if Y sends back a cookie that Q must receive and process and send
back to Y, that ensures that Q is at least a man-in-the-middle and not just
an outside attacker (which is all TCP would guarantee anyway).  If you're
using an encrypted protocol (like QUIC), the handshake can also be protected
by your session key.</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://bradfrost.com/blog/post/a-new-bradfrost-com/">A New bradfrost.com!</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="http://feeds.feedburner.com/brad-frosts-blog">Brad Frost</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">design</span>
              <span class="tag">web</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Thu Aug 31 2017 18:49:12 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>So after a good many years (cobbler’s children and all that), bradfrost.com has finally gotten a facelift! This makes the 8th redesign of the site (although a few redesigns have been relatively minor).</p>
<p>Here’s what I’m excited about with the new redesign:</p>
<ul>
<li>I pulled out the design into <a href="http://patternlab.io">Pattern Lab</a> where I was able to actually design the UI in a proper <a href="http://bradfrost.com/blog/post/the-workshop-and-the-storefront/">workshop,</a> rather than tripping over WordPress’s loops and garbled theme code. I’m still in the process of cleaning everything up, but if you’re interested in looking at my pattern lab, you can <a href="http://pl.bradfrost.com/?p=pages-homepage">see it here</a>.</li>
<li>I’ve cleaned up almost a decade’s worth of WordPress crap. This is still a work in progress, but I used the fantastic <a href="https://www.advancedcustomfields.com/">Advanced Custom Fields</a> to put structured content in place.</li>
<li>After years of building upon shaky ground, I burned down my entire CSS and rebuilt it using the <a href="http://bradfrost.com/blog/post/css-architecture-for-design-systems/">architecture I’ve used to build giant design systems</a>. I’m excited about this because now that I have a system in place, I’ll likely be able to tweak or redesign the site’s aesthetics more efficiently from here on out.</li>
<li>I have homes for a lot of my content that wasn’t ever there before. I have a page that describes the different kinds of <a href="http://bradfrost.com/work">work</a> I do. I have a page for all the <a href="http://bradfrost.com/projects/">side projects &amp; collaborations</a> I’ve been fortunate to work on. I have an updated <a href="/">portfolio</a> where I talk about the kind of front-end design work I do. I even have <a href="http://bradfrost.com/music/">music</a> and <a href="http://bradfrost.com/art/">art</a> pages up.</li>
<li>I’ve cleaned up my <a href="http://bradfrost.com/blog/">blog</a> so that it’s easier for me to post things and share links. I’m doubling down on owning my own content, so I’m excited to have a less-crusty home for it all.</li>
<li>It’s fun! In a world where even sites that aren’t Medium dot com are looking like Medium dot com, I’m excited to try something different. I know some people won’t like that, but whatever. I think we need a bit of fun.</li>
</ul>
<p>I was just at a conference with the amazing <a href="http://cameronmoll.com/">Cameron Moll</a>, and his advice was:</p>
<blockquote><p>Perfection never ships. Cameron Moll</p></blockquote>
<p>So it’s in that spirit that I’m launching this. I know there’s at least a hundred things that are broken or incomplete, but thankfully this is my website so I can do whatever the hell I want. I’ll be cleaning things up over the coming weeks, but in the meantime, I hope  you enjoy the new design. Thanks!</p>
<p>Old design:</p>
<p></p>
<p>New design:</p>
<p></p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://bradfrost.com/blog/link/a-cat-is-not-an-iphone/">A Cat Is Not An iPhone</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="http://feeds.feedburner.com/brad-frosts-blog">Brad Frost</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">design</span>
              <span class="tag">web</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Thu Aug 31 2017 15:48:02 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <blockquote><p>A brief note on opinion: If you look at a cat and say, “In my opinion that’s an iPhone” that’s not an opinion that’s just you being wrong.</p></blockquote>
<p>This is a great point. It seems like something this commonsensical doesn’t need to be said, but unfortunately it does.</p>
<blockquote><p>So now an objective fact is treated nationally as a subjective question because a few stubborn, ignorant assholes formed a consensus.</p></blockquote>
<p>I’d add to the list of “stubborn, ignorant assholes” the unscrupulous people who stand to retain/gain their power/fortune based on undermining objective facts.</p>
<p>Also, it feels weird linking to a Twitter thread, but I guess that’s life now.</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://bradfrost.com/blog/link/3-ways-to-stop-designing-for-addiction/">3 Ways to Stop Designing for Addiction</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="http://feeds.feedburner.com/brad-frosts-blog">Brad Frost</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">design</span>
              <span class="tag">web</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Wed Aug 30 2017 17:40:38 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p>There’s some really smart stuff in this post. Pamela recommends some actionable things we designers can do to better ensure our creations don’t harm people. Ultimately this comes down to actively thinking about this stuff, so seeing articles like this address the problem is important.</p>
<p>I’ve been struggling with a lot of the addictive aspects of technology (see <a href="http://bradfrost.com/blog/link/what-is-technology-doing-to-us/">this link</a> for more great thoughts on this), and have been taking steps to make my own social media world a bit <a href="https://calmtech.com/">calmer</a>. I uninstalled Facebook on my phone, and have been grooming Twitter in an attempt to reclaim it as a tool that helps me in my professional life, but doesn’t <a href="http://bradfrost.com/blog/post/dont-shit-where-you-eat/">drown me in everything all the time</a>.</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://sdegutis.com/blog/2017-08-30-why-im-ditching-clojure-for-javascript">Why I&#39;m ditching Clojure for JavaScript</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://sdegutis.com/blog/atom.xml">Steven Degutis</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">practices</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Wed Aug 30 2017 01:00:00 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            For the past 5 years I've worked full time on writing CleanCoders.com, a Clojure
web app I wrote from scratch. For a while I agreed with Bob that the language of
the future is probably going to be som...
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://feedproxy.google.com/~r/TroyHunt/~3/trlyP1Wux7g/">Inside the Massive 711 Million Record Onliner Spambot Dump</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://feeds.feedburner.com/TroyHunt">Troy Hunt</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">security</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Tue Aug 29 2017 20:31:34 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <p><a href="http://www.symantec.com/ready-secure-go/it-smb/?om_ext_cid=ws_ad_TROY_DBC-Bulldog_IT-SMB_Encrypt"><strong>Presently sponsored by:</strong> Get a security solution that will keep your website up and running—and keep you sleeping soundly: Symantec Website Security. Learn how</a></p><div><p>Last week I was contacted by someone alerting me to the presence of a spam list. A big one. That's a bit of a relative term though because whilst <a href="https://www.troyhunt.com/have-i-been-pwned-and-spam-lists-of-personal-information/">I've loaded &quot;big&quot; spam lists into Have I been pwned (HIBP) before</a>, the largest to date has been a mere 393m records and <a href="https://haveibeenpwned.com/PwnedWebsites#RiverCityMedia">belonged to River City Media</a>. The one I'm writing about today is 711m records which makes it the largest single set of data I've ever loaded into HIBP. Just for a sense of scale, that's almost one address for every single man, woman and child in all of Europe. This blog posts explains everything I know about it.</p>
<p>Firstly, the guy who contacted me is <a href="https://twitter.com/benkow_">Benkow moʞuƎq</a> and he's done some really interesting malware and spambot analysis in the past. During our communication over the last week, I had a read of his piece on <a href="http://benkowlab.blogspot.com.au/2017/02/spambot-safari-2-online-mail-system.html">Spambot safari #2 - Online Mail System</a> which is a good example of the sort of work he's been doing (it's also a good example of how dodgy some of this spammer code is!) He went on to explain how he'd located a machine used by the &quot;Onliner Spambot&quot; and pointed me to a path on an IP address with directory listing enabled:</p>
<p></p>
<p>I've obfuscated a bunch of info here because as of the time of writing, the server is still up and I don't want to give away any information that could result in the data being spread further. The IP address is actually based in the Netherlands and Benkow and I have been in touch with a trusted source there who's communicating with law enforcement in an attempt to get it shut down ASAP. Until that time, I'm not going to share file names in their entirety although I'll certainly describe anything of relevance in them.</p>
<p>Before I dive into the data, <a href="https://benkowlab.blogspot.com.au/2017/08/from-onliner-spambot-to-millions-of.html">Benkow has posted a dedicated piece on the mechanics of this spambot</a> that's worth a read. You can also find <a href="http://www.zdnet.com/article/onliner-spambot-largest-ever-malware-campaign-millions/">a great story on ZDNet from Zack Whittaker</a> which is a good overview of the situation. The gap I want to fill here is to explain what I can about the data because there'll be a very large number of people finding themselves on HIBP and wondering what an earth is going on. If you haven't already read Benkow's piece, there's 2 important classes of data you need to understand:</p>
<ol>
<li>Email addresses. That's it - just masses and masses of email addresses used to deliver spam to. In some cases, a single file may contain tens or even hundreds of millions of addresses.</li>
<li>Email addresses and passwords. Benkow explains that these are used in an attempt to abuse the owners' SMTP server in order to deliver spam. I also believe that many of these may simply be aggregations from various other breach sources I'll talk about a little later on.</li>
</ol>
<p>Getting on to the data itself, the first place to start is with an uncomfortable truth: my email address is in there. Twice:</p>
<p></p>
<p>That first file is the 14GB one from the earlier directly listing whilst the second is 131MB. In many cases, I found the same data in both the former larger file and a subsequent smaller one. Interestingly, as you can see from the suffix above, both refer to &quot;UK&quot; (I'm certainly not from the United Kingdom) whilst others refer to &quot;AU&quot; (although I'm not in there). There are no other 2 letter country codes represented in the file names but clearly when we're talking many hundreds of millions of addresses here, a heap of them are from other locations so take those suffixes with a grain of salt.</p>
<p>One of the files with the &quot;NewFile_&quot; prefix contained over 43k rows associated with the <a href="http://www.rms.nsw.gov.au/">Roads and Maritime Services</a> of my neighbouring state here in Australia:</p>
<p></p>
<p>Every row contains <a href="mailto:RMSETollDontReply@rms.nsw.gov.au">RMSETollDontReply@rms.nsw.gov.au</a> in quotes followed by &quot;support@&quot; and then predominantly .com.au domains, albeit with over 13k .ru domains. This email address is used to send notifications relating to the &quot;E-Tag&quot; device installed on your car windscreen so that you can pay tolls. I know this because I've received a bunch of them in the past:</p>
<p></p>
<p>I'll take a stab at it and say that there's not many legitimate drivers using the New South Wales toll road system with Russian email addresses! Clearly, the constant alias on every one of these accounts is auto-generated. Interestingly, I saw a similar pattern with <a href="https://haveibeenpwned.com/PwnedWebsites#B2BUSABusinesses">the B2B USA Businesses</a> spam list I loaded last month with many comments like this:</p>
<blockquote><p>I received a domain alert on this one. Went through the process, turned out to be an invented address (sales@domain). Address doesn't exist.</p>— Peter Bance (@peterbance) <a href="https://twitter.com/peterbance/status/887795931143430144">July 19, 2017</a></blockquote>

<p>There's also some pretty poorly parsed data in there which I suspect may have been scraped off the web. For example, <a href="mailto:Employees-bringing-in-their-own-electrical-appliances.htmlmark.cornish@bowelcanceruk.org.uk">Employees-bringing-in-their-own-electrical-appliances.htmlmark.cornish@bowelcanceruk.org.uk</a> appears twice:</p>
<p></p>
<p>The first file is the same one my own email address was in and the second is the same file name structure albeit with a different number in it. And if you're wondering why I've publicly listed someone else's address, it's because <a href="https://www.yumpu.com/en/document/view/45572519/job-description-bowel-cancer-uk-database-officer-about-bowel-/2">it's already publicly listed</a>:</p>
<p></p>
<p>But of course, the data in the dump has a bunch of junk prefixed to the address, junk which appears to be an HTML file name and may indicate the &quot;address&quot; was scraped off the web and the parsing simply wasn't done very well. The point here is that there's going to be a bunch of addresses here that simply aren't very well-formed so whilst the &quot;711 million&quot; headline is technically accurate, the number of real humans in the data is going to be somewhat less.</p>
<p>And then we get into passwords. One file is named numerically and contains 1.2m rows like this:</p>
<p></p>
<p>A random selection of a dozen different email addresses checked against HIBP showed that <em>every single one of them</em> was in the LinkedIn data breach. Now this is interesting because assuming that's the source, all those passwords were exposed as SHA1 hashes (no salt) so it's quite possible these are just a small sample of the 164m addresses that were in there and had readily crackable passwords.</p>
<p>A similar file (with a similar naming structure) contains 4.2m email address and password pairs, this time with every single account having a hit on <a href="https://www.troyhunt.com/password-reuse-credential-stuffing-and-another-1-billion-records-in-have-i-been-pwned/">the massive Exploit.In combo list</a>. This should give you an appreciation of how our data is redistributed over and over again once it's out there in the public domain.</p>
<p>Other files have equal or even greater numbers; one has 29m rows of email address and password pairs, the former of which consistently shows up in HIBP, albeit without an exclusive data breach pattern per the previous two examples. Another is named in a fashion that suggests it's a large Aussie set of addresses and true to its name, there's 12.5m rows in there which would mean roughly one per every 2 people in the country. A large portion of those don't show up in HIBP at all, including the 379k .gov.au ones which appear to be a mixture of legitimate, fake and malformed addresses.</p>
<p>Yet another file contains over 3k records with email, password, SMTP server and port (<a href="http://docs.mailpoet.com/article/59-default-ports-numbers-smtp-pop-imap">both 25 and 587 are common SMTP ports</a>):</p>
<p></p>
<p>This immediately illustrates the value of the data: thousands of valid SMTP accounts give the spammer a nice range of mail servers to send their messages from. There are many files like this too; another one contained 142k email addresses, passwords, SMTP servers and ports.</p>
<p>Some of the data was quite a jumble of info, for example the file with 20m rows of Russian email addresses surrounded by what appears to be file names containing SQL:</p>
<p></p>
<p>And it goes on and on. Email addresses, passwords and SMTP servers and ports spread across tens of gigabytes of files. It took HIBP 110 data breaches over a period of 2 and a half years to accumulate 711m addresses and here we go, in one fell swoop, with that many concentrated in a single location. It's a mind-boggling amount of data.</p>
<p>The above examples are by no means exhaustive, rather they're intended to illustrate just how diverse the data is. It helps explain both the massive number of records and the inevitable responses I'll get of &quot;there are addresses in there which aren't real&quot;. It also illustrates how broad the sources of data inevitably are; finding yourself in this data set unfortunately doesn't give you much insight into <em>where</em> your email address was obtained from nor what you can actually do about it. I have no idea how this service got mine, but even for me with all the data I see doing what I do, there was still a moment where I went &quot;ah, this helps explain all the spam I get&quot;. And that's the unfortunate reality for all of us: our email addresses are a simple commodity that's shared and traded with reckless abandon, used by unscrupulous parties to bombard us with everything from Viagra offers to promises of Nigerian prince wealth. That, unfortunately, is life on the web today.</p>
<p>All 711 million records <a href="https://haveibeenpwned.com/">are now searchable in HIBP</a>.</p>
<p><strong>Edit 1:</strong> For the folks asking for passwords (or if one was present for the account), please read <a href="https://www.troyhunt.com/here-are-all-the-reasons-i-dont-make-passwords-available-via-have-i-been-pwned/">Here are all the reasons I don't make passwords available via Have I been pwned</a>.</p>
<p><strong>Edit 2:</strong> For those asking for an indication of whether a password exists for an email address or not, there are several major challenges with this:</p>
<ol>
<li>The source data is in such a mess the parsing would be a nightmare. At present, I just point a regex at it and suck the email addresses out.</li>
<li>If I <em>could</em> readily pull all the passwords and indicate whether it existed or not, I'd still be bombarded with requests for &quot;can I please have the password&quot; which brings us back to the previous edit.</li>
<li>Looking at this more generally, data breaches frequently have a broad range of attributes (the &quot;Compromised data&quot; section on each breach listed on the <a href="https://haveibeenpwned.com/PwnedWebsites">Who's been pwned page</a>. Indicating which attribute was present on each account would be extremely time consuming.</li>
<li>I simply don't have a construct for this in HIBP at present. This would be a ground-up new feature.</li>
</ol>
<p>For this particular incident, if you're creating strong, unique passwords on each service (get a <a href="https://www.troyhunt.com/only-secure-password-is-one-you-cant/">password manager</a> if you don't have one already) and using multi-step verification wherever possible, I wouldn't be at all worried. If you're not, now's a great time to start!</p>
<p><strong>Edit 3:</strong> Regarding those asking &quot;what do I do now&quot;, you'll fall into one of two camps:</p>
<ol>
<li>If you're using a password manager and creating genuinely strong, unique passwords everywhere, I wouldn't be in the least bit worried. The sources of this list are from various legacy incidents, for example LinkedIn which hashed passwords. Yes, it was only SHA1 without a salt, your 20+ char random password still isn't getting cracked. Every password I saw in the data set was weak; I didn't see any evidence to suggest there were plain text data sources.</li>
<li>If you're not already using a password manager, start now. <em>Now!</em> You've already had weak passwords exposed (which they almost certainly are if you're not using a password manager), and they've been out there for some time. Go and grab <a href="https://1password.com/">1Password</a> and spend a couple of hours learning to use it and changing the passwords of your most important accounts now and everything else as time permits.</li>
</ol>
<p>Further to that, regardless of which camp you're in, enable multi-step verification on <em>everything</em>. Even if your password is exposed (either because it's weak or was stored by the site in plain text), this renders the credentials alone absolutely useless.</p>
<p>None of this is hard, it requires just a little bit of effort and makes a fundamental difference to your security profile.</p>
</div>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://technology.condenast.com/story/the-why-and-how-of-google-amp-at-conde-nast">The Why and How of Google AMP at Condé Nast</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://technology.condenast.com/feed/rss">Condé Nast Technology</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">web</span>
              <span class="tag">company</span>
            </div>
            <span class="subtitle is-7">Tue Aug 29 2017 15:49:48 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            Google AMP aims to provide a better experience for the mobile web. Let’s explore the motivations, technology, and outcomes of the AMP implementation at Condé Nast.
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://www.fourmilab.ch/fourmilog/archives/2017-08/001707.html">Flash, Bang!</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://www.fourmilab.ch/fourmilog/atom_10.xml">Fourmilog - Fourmilab Change Log</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Tue Aug 29 2017 14:53:51 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            Last Thursday, 2017-08-24, was interesting. I was programming away when, as often occurs in the mid-afternoon here in the summer, thunderheads boiled up above the Jura, the sky darkened, and before long a full-on thunderboomer complete with high winds, torrential rain, and plenty of flashes and bangs was underway.

<p>

Then, <em><b>flash, bang!</b></em>  When you perceive them at exactly the same time, it's never a good sign. Instantaneously, the <a href="http://www.dell.com/ed/business/p/dell-up3017-monitor/pd" target="Fourmilog_Aux">auxiliary monitor</a> on my development machine emitted a crack and went black. The UPS units all went on battery, but quickly came back on line. I suspect the event they detected was not a power outage but a transient due to the lightning strike. After about five seconds, the monitor lit back up as if nothing had happened.

</p><p>

So far, so good. Further along, not so good. The phone next to the computer, which is connected to Fourmilab's Alcatel OmniPCX phone central was completely dead: even the LCD display was blank. All of the other phones connected to the central were equally <em>hors de combat</em>. The main <a href="/fourmilog/archives/2008-07/001024.html" target="Fourmilog_Aux">Fourmilab fibre optic connection</a> to the Internet wasn't perturbed at all, but the backup Swisscom ADSL connection was dead, and its router responded by butting in to new connections and diverting them to its &quot;landing page&quot; until I pulled the plug.

</p><p>

This isn't the first time something like this has happened. When you live on a plateau 806 metres above sea level just downwind of the first serious mountain range moist air encounters after crossing France and being heated, summer thunderstorms are part of the deal. (Although I don't keep detailed records, I think this is about the eighth time Fourmilab has been struck by lightning resulting in damage to electronics. Once is chance; twice is coincidence; three times is enemy action; —eight—
you must've really made old thunderhammer quite irate.)

</p><p>

All of the power and telephone lines are buried, and protected with state of the art lightning arrestors, and all non-resistive electrical loads are connected to UPS units. The problem is nearby strikes which are conducted to ground by the protection, but which, in doing so, induce currents in data cables parallel to them. You can do everything right, but when you're talking about hundreds of thousands of amperes, all of your remediation does about as much good as a <a href="https://en.wikipedia.org/wiki/Tin_foil_hat" target="Fourmlog_Aux">tinfoil hat</a>.

</p><p>

<a href="https://en.wikipedia.org/wiki/Lightning_rod" target="Fourmlog_Aux">Lightning rods</a> on buildings don't help. They may protect the building, but the cone of protection doesn't extend sufficiently far to guard against ground strikes which get into telephone wires. You could put up lightning protection masts like they use around rocket launch pads, but it would be difficult to get the neighbours to approve.

</p><p>

Here is what happens when lightning gets into a data cable between two buildings. First. here's the connector into which the cable was plugged.

</p><p>

<a href="http://www.fourmilab.ch/fourmilog/archives/2017/08/29/foudre_2017-08-28a.jpg"></a>

</p><p>

It was port 2 that was hit. Note how the metal at the top of the connector has been melted and the conductors blackened by smoke. The smoke above the connector was the tip-off, and more visible to the eye than in this picture.

</p><p>

Here's the cable:

</p><p>



</p><p>

You can see the smoke staining the plastic of the connector. There are also smoke stains on the metal sides of the connector, but they don't show up well since it's a specular reflector.

</p><p>

I'd like to say that everything has been resolved and all is well, but it's not. There's a lot more time to be consumed in remediation of this event. What can you do to keep this from happening to you? <em>Don't get struck by lightning!</em>  Other than moving to somewhere the risk is lower, there's little more that can be done.</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="https://8thlight.com/blog/kofi-gumbs/2017/08/29/elm-json-decoding-custom.html">How JSON decoding works in Elm—Part 3</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="https://8thlight.com/blog/feed/atom.xml">8th Light Blog</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">practices</span>
              <span class="tag">principles</span>
              <span class="tag">company</span>
            </div>
            <span class="subtitle is-7">Tue Aug 29 2017 06:00:00 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            <blockquote>
<p>Elm approaches JSON much differently than languages like JavaScript and Ruby.
Elm's built-in functions don't offer the immediate convenience of <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/JSON/parse"><code>JSON.parse</code></a>.
You'll find built-in <code>Decoder</code>s for the basic types, like <code>String</code> and <code>Int</code>,
but there is no single function that says &quot;just grab whatever is in this HTTP response.&quot;
Instead, the standard library takes a modular approach,
providing a few building blocks that can be arbitrarily composed.
This design makes Elm's JSON decoding (or parsing) flexible and precise.
Over the next few posts, we will discover this ourselves
by implementing a simplified version of the <a href="http://package.elm-lang.org/packages/elm-lang/core/5.1.1/Json-Decode"><code>Json.Decode</code></a> module.</p>
</blockquote>

<p>In the <a href="/blog/kofi-gumbs/2017/07/17/elm-json-decoding-data-structures.html">last post</a>, we learned how to compose <code>Decoder</code>s
in order to model JSON values that contain other JSON values.
So far, however, we've only been able to extract built-in types,
like <code>String</code>, <code>Int</code>, and <code>List</code>.
Now we'll make our module a bit friendlier by letting users build custom types
and providing great error messages as they do so!</p>

<h3>A map from A to B</h3>

<p>Let's begin by looking at the type signature for <code>Json.Decode.map</code>:</p>
<div><pre><code>map : (a -&gt; b) -&gt; Decoder a -&gt; Decoder b
map _ _ =
    Debug.crash &quot;TODO&quot;

-- TRY IT OUT: https://ellie-app.com/43yWY6hWbVfa1/1
</code></pre></div>
<p>The <code>map</code> function allows us to transform the type of our decoder
by providing a function <em>from</em> the current type, <em>to</em> the target type.
Suppose we have a <code>UserId</code> type with a constructor that takes an <code>Int</code>.
We can create a <code>Decoder UserId</code> like so:</p>
<div><pre><code>type UserId =
    UserId Int

userIdDecoder : Decoder UserId
userIdDecoder =
    map UserId int

-- TRY IT OUT: https://ellie-app.com/43yWY6hWbVfa1/2
</code></pre></div>
<p>Hey, that looks like a good opportunity for a test case!</p>
<div><pre><code>test &quot;decodes a user id&quot; &lt;|
    \_ -&gt;
        decodeValue userIdDecoder (Jnumber 123)
            |&gt; Expect.equal (Ok (UserId 123))

map : (a -&gt; b) -&gt; Decoder a -&gt; Decoder b
map transform (Decoder parameterAttempt) =
    let
        attemptToDecode value =
            Debug.crash &quot;TODO&quot;
    in
        Decoder attemptToDecode

-- TRY IT OUT: https://ellie-app.com/43yWY6hWbVfa1/4
</code></pre></div>
<p>Let's pause and observe the variables to which we have access.</p>

<ul>
<li><code>value</code> has type <code>Value</code></li>
<li><code>parameterAttempt</code> has type <code>Value -&gt; Result String a</code></li>
<li><code>transform</code> has type <code>a -&gt; b</code></li>
<li><code>attemptToDecode</code> <em>must</em> have type <code>Value -&gt; Result String b</code></li>
</ul>

<p>The breakdown above should make it clear how we can use the
available functions to fill in the body of <code>attemptToDecode</code>.
Our <code>value</code> must first be decoded with <code>parameterAttempt</code>,
so that we can have access to the <code>Result String a</code>.
Once we have it, we can just use our <code>transform</code> function to create the correct type.
As it happens, the <code>Result</code> type has its own <code>map</code> function,
which is perfect for this situation.</p>
<div><pre><code>map : (a -&gt; b) -&gt; Decoder a -&gt; Decoder b
map transform (Decoder parameterAttempt) =
    let
        attemptToDecode value =
            parameterAttempt value
                |&gt; Result.map transform
    in
        Decoder attemptToDecode

-- TRY IT OUT: https://ellie-app.com/43yWY6hWbVfa1/5
</code></pre></div>
<h3>Decode this <code>andThen</code> decode that</h3>

<p>Imagine that we want to validate the <code>UserId</code> type introduced above
since negative id's do not make much sense in our app.
It would be nice if we could model this invariant in our <code>Decoder</code>,
so that it is not even possible to create a bad <code>UserId</code>.
Well, that's exactly what <code>succeed</code>, <code>fail</code>, and <code>andThen</code> let us do.</p>
<div><pre><code>succeed : a -&gt; Decoder a
succeed _ =
    Debug.crash &quot;TODO&quot;

fail : String -&gt; Decoder a
fail _ =
    Debug.crash &quot;TODO&quot;

andThen : (a -&gt; Decoder b) -&gt; Decoder a -&gt; Decoder b
andThen _ _ =
    Debug.crash &quot;TODO&quot;

userIdDecoder : Decoder UserId
userIdDecoder =
    let
        ensurePositive n =
            if n &gt; 0 then
                succeed (UserId n)
            else
                fail &quot;Need a positive number&quot;
    in
        int |&gt; andThen ensurePositive

-- TRY IT OUT: https://ellie-app.com/43yWY6hWbVfa1/6
</code></pre></div>
<p>These functions let us decode with much more precision
because we can now filter according to our own rules.
Quite exciting!</p>
<div><pre><code>test &quot;fails to decode a negative number&quot; &lt;|
    \_ -&gt;
        decodeValue userIdDecoder (Jnumber -5)
            |&gt; Expect.equal (Err &quot;Need a positive number&quot;)

succeed : a -&gt; Decoder a
succeed a =
    Decoder (\_ -&gt; Ok a)

fail : String -&gt; Decoder a
fail reason =
    Decoder (\_ -&gt; Err reason)

andThen : (a -&gt; Decoder b) -&gt; Decoder a -&gt; Decoder b
andThen transform (Decoder parameterAttempt) =
    let
        attemptToDecode value =
            parameterAttempt value
                |&gt; Result.map (\param -&gt; Debug.crash &quot;What here?!&quot;)
    in
        Decoder attemptToDecode

-- TRY IT OUT: https://ellie-app.com/43yWY6hWbVfa1/7
</code></pre></div>
<p><code>succeed</code> and <code>fail</code> allow us to create <code>Decoder</code>s with predefined behavior,
regardless of the <code>Value</code> used as input.
<code>andThen</code> is a little trickier though.
The type signature <em>almost</em> looks like that of <code>map</code>,
and we can get pretty far copying that general structure.
Let's pause again to reflect on the variables in scope:</p>

<ul>
<li><code>param</code> has type <code>a</code></li>
<li><code>transform</code> has type <code>a -&gt; Decoder b</code></li>
<li><code>attemptToDecode</code> <em>must</em> have type <code>Value -&gt; Result String b</code></li>
</ul>

<p>The types tell us that we can use <code>transform param</code> to get a <code>Decoder b</code>,
but we actually need to return a <code>Result String b</code>.
That means we need a function that takes a <code>Decoder b</code> and returns a <code>Result String b</code>...
We have that function already: <a href="http://package.elm-lang.org/packages/elm-lang/core/5.1.1/Json-Decode#decodeValue"><code>decodeValue</code></a>!
We'll also take advantage of <a href="http://package.elm-lang.org/packages/elm-lang/core/5.1.1/Result#andThen"><code>Result.andThen</code></a>, to flatten out our <code>Result</code>.</p>
<div><pre><code>andThen : (a -&gt; Decoder b) -&gt; Decoder a -&gt; Decoder b
andThen transform (Decoder parameterAttempt) =
    let
        attemptToDecode value =
            parameterAttempt value
                |&gt; Result.andThen (\param -&gt; decodeValue (transform param) value)
    in
        Decoder attemptToDecode

-- TRY IT OUT: https://ellie-app.com/43yWY6hWbVfa1/8
</code></pre></div>
<h3>Helpful errors: Elm's secret sauce</h3>

<p>Thus far we've glossed over one major feature of Elm's <code>Json.Decode</code> library:
great error messaging.
For instance <code>decodeString (field &quot;a&quot; (list string)) &quot;&quot;&quot;{&quot;a&quot;: [&quot;x&quot;, 123]}&quot;&quot;&quot;</code>
will fail with a helpful message: <code>&quot;Expecting a String at _.a[1] but instead got: 123&quot;</code>.
This error is quite specific
because Elm has pinpointed exactly where the decoding went wrong.
Our module, in contrast, would return <code>&quot;not a string&quot;</code>.
Let's take a stab at better error messages by keeping track of an error context.</p>
<div><pre><code>type Decoder =
    Decoder (String -&gt; Value -&gt; Result String a)

decodeValue : Decoder a -&gt; Value -&gt; Result String a
decodeValue =
    runDecoder &quot;_&quot;

runDecoder : String -&gt; Decoder a -&gt; Value -&gt; Result String a
runDecoder context (Decoder attemptToDecode) value =
    attemptToDecode context value

-- TRY IT OUT: https://ellie-app.com/43yWY6hWbVfa1/9
</code></pre></div>
<p>This change to the <code>Decoder</code> type forces a number of changes inside of our module.
All of our <code>attemptToDecode</code> helpers must now take an additional <code>String</code> parameter,
which I am calling <code>context</code>.
With those mechanical changes out of the way,
we can write a test for the error messaging we want to see.</p>
<div><pre><code>test &quot;better error messaging&quot; &lt;|
    \_ -&gt;
        Jobject (Dict.singleton &quot;a&quot; (Jarray [Jstring &quot;x&quot;, Jnumber 123]))
            |&gt; decodeValue (field &quot;a&quot; (list string))
            |&gt; Expect.equal (Err &quot;Expecting a String at _.a[1] but instead got: 123&quot;)

-- TRY IT OUT: https://ellie-app.com/43yWY6hWbVfa1/10
</code></pre></div>
<p>We'll start by adjusting the implementation of <code>string</code>
to take advantage of the new <code>context</code> parameter.</p>
<div><pre><code>string : Decoder String
string =
    let
        attemptToDecode context value =
            case value of
                Jstring foundIt -&gt;
                    Ok foundIt
                _ -&gt;
                    Err &lt;|
                        &quot;Expecting a String at &quot;
                            ++ context
                            ++ &quot; but instead got: &quot;
                            ++ givenString value
    in
        Decoder attemptToDecode

givenString : Value -&gt; String
givenString value =
    case value of
        Jnumber foundIt -&gt;
            toString foundIt
        _ -&gt;
            Debug.crash &quot;TODO&quot;

-- TRY IT OUT: https://ellie-app.com/43yWY6hWbVfa1/11
</code></pre></div>
<p>Running the tests at this point reveals that we are very close:</p>
<div><pre><code>Err &quot;Expecting a String at _.a[1] but instead got: 123&quot;
╷
│ Expect.equal
╵
Err &quot;Expecting a String at TODO but instead got: 123&quot;
</code></pre></div>
<p>Now we need to build up the <code>context</code> inside the decoders that we wrote in the
<a href="/blog/kofi-gumbs/2017/07/17/elm-json-decoding-data-structures.html">last post</a>.</p>
<div><pre><code>field : String -&gt; Decoder a -&gt; Decoder a
field key (Decoder parameterAttempt) =
    let
        -- The new part: building error message context
        withDot context value =
            parameterAttempt (context ++ &quot;.&quot; ++ key) value

        decodeKey context object =
            Dict.get key object
                |&gt; Result.fromMaybe &quot;TODO&quot;
                |&gt; Result.andThen (withDot context)

        attemptToDecode context value =
            case value of
                Jobject foundIt -&gt;
                    decodeKey context foundIt
                _ -&gt;
                    Err &quot;TODO&quot; -- we can use `givenString` here
    in
        Decoder attemptToDecode

list : Decoder a -&gt; Decoder (List a)
list (Decoder parameterAttempt) =
    let
        -- The new part: building error message context
        withBrackets context i value =
            parameterAttempt (context ++ &quot;[&quot; ++ toString i ++ &quot;]&quot;) value

        collectResults next all =
            Result.map2 (::) next all

        attemptToDecode context value =
            case value of
                Jarray foundIt -&gt;
                    List.indexedMap (withBrackets context) foundIt
                        |&gt; List.foldr collectResults (Ok [])
                _ -&gt;
                    Err &quot;TODO&quot; -- we can use `givenString` here
    in
        Decoder attemptToDecode

-- TRY IT OUT: https://ellie-app.com/43yWY6hWbVfa1/12
</code></pre></div>
<p>And we're passing!
Of course, there is much left <code>TODO</code>,
but we've now developed a structure to easily build up these error contexts.
The next step is to determine the error message that would be most helpful
in each of those cases.
This sort of work is required for a good API,
but it does not fit within the scope of this blog series.</p>

<hr />

<p>I've mentioned &quot;the immediate convenience of <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/JSON/parse"><code>JSON.parse</code></a>&quot; in all three posts,
but we have not taken the opportunity to reflect on that claim.
In JavaScript and Ruby, a successful call to <code>JSON.parse</code>
only means that the input was valid JSON.
Once you have the data, <strong>you still need code to ensure it is well-formed for <em>your</em> app.</strong></p>

<p>Unfortunately, this step is often skipped—we assume that valid JSON means that the data is in the shape our app's needs.
This has some unfortunate consequences:</p>

<ul>
<li>When our assumption is incorrect, the fail site is quite far from the <code>JSON.parse</code></li>
<li>We force our internal structure to fit that of the API</li>
</ul>

<p>Elm's approach alleviates both of these concerns.
The assumptions in JavaScript and Ruby are now codified as <em>rules</em> in our Elm decoders.
And as we saw above, you will even get a nice error message!
Second, our application data models need not depend on the shape of our JSON.
If a remote field name changes, we just update the string passed into <code>field</code>,
so there's no need to search our entire app.</p>

<p>JSON decoding is just an example of what I like about Elm:
its thoughtful design encourages clean code.
Hopefully these posts helped you develop both
an intuition for the <em>how</em> behind <code>Json.Decode</code> and an appreciation for the <em>why</em>.</p>
          </div>
        </div>
      </article>
      <article class="feed-item section">
        <div class="container">
          <div class="feed-item-meta">
            <h1 class="title"><a href="http://ronjeffries.com/articles/017-08ff/ipad-k-db/">iPad - Moving to Dropbox for Some Reason</a></h1>
            <h2 class="subtitle feed-item-feed-name"><a href="http://ronjeffries.com/feed.xml">Ron Jeffries</a></h2>
            <div class="tags feed-item-feed-tags">
              <span class="tag">practices</span>
              <span class="tag">agile</span>
              <span class="tag">personal</span>
            </div>
            <span class="subtitle is-7">Tue Aug 29 2017 05:00:00 GMT+0100 (BST)</span>
          </div>
          <div class="feed-item-summary content">
            Chet has me nearly convinced to move my site base to Dropbox. Today we'll talk further about that and about moving the product forward.
          </div>
        </div>
      </article>
    </main>
    <footer class="footer">
      <div class="container">
        <div class="content has-text-centered">
          <span class="generator">Generated with <a href="https://github.com/wcerfgba/syzygys">Syzygys</a></span>
        </div>
      </div>
    </footer>
    <style>
      a.return-to-top {
        position: fixed;
        bottom: 1rem;
        right: 1rem;
        width: 2rem;
        height: 2rem;
        border-radius: 100%;
        background: #ddd;
        opacity: 0.3;
        text-align: center;
        text-decoration: none;
        line-height: 2rem;
      }
      .feed-item-meta {
        margin-bottom: 1.5rem;
      }
      .feed-item-feed-name {
        margin-bottom: 0.5rem !important;
      }
      .feed-item-feed-tags {
        margin-bottom: 0 !important;
      }
    </style>
  </body>
</html>
